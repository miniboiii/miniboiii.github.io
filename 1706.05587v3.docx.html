<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=fpjTOVmNbO4Lz34iLyptLSGpxL_7AiMlXksihSwXLVVq_Mfl6Qa6938s66etLwSb527w0KN8IuYKnSxUwZQM6w);.lst-kix_list_4-1>li{counter-increment:lst-ctn-kix_list_4-1}ol.lst-kix_list_3-1{list-style-type:none}.lst-kix_list_3-1>li{counter-increment:lst-ctn-kix_list_3-1}.lst-kix_list_2-1>li{counter-increment:lst-ctn-kix_list_2-1}ol.lst-kix_list_3-0{list-style-type:none}.lst-kix_list_1-1>li{counter-increment:lst-ctn-kix_list_1-1}.lst-kix_list_3-0>li:before{content:"[" counter(lst-ctn-kix_list_3-0,decimal) "] "}ol.lst-kix_list_3-1.start{counter-reset:lst-ctn-kix_list_3-1 0}.lst-kix_list_3-1>li:before{content:"(" counter(lst-ctn-kix_list_3-1,lower-latin) ") "}.lst-kix_list_3-2>li:before{content:"\002022   "}.lst-kix_list_4-0>li{counter-increment:lst-ctn-kix_list_4-0}ul.lst-kix_list_1-3{list-style-type:none}.lst-kix_list_3-5>li:before{content:"\002022   "}ul.lst-kix_list_1-4{list-style-type:none}.lst-kix_list_3-4>li:before{content:"\002022   "}ul.lst-kix_list_1-2{list-style-type:none}ul.lst-kix_list_1-7{list-style-type:none}.lst-kix_list_3-3>li:before{content:"\002022   "}ul.lst-kix_list_1-8{list-style-type:none}ul.lst-kix_list_1-5{list-style-type:none}ul.lst-kix_list_1-6{list-style-type:none}li.li-bullet-44:before{margin-left:-19.9pt;white-space:nowrap;display:inline-block;min-width:19.9pt}li.li-bullet-35:before{margin-left:-19.9pt;white-space:nowrap;display:inline-block;min-width:19.9pt}li.li-bullet-37:before{margin-left:-19.9pt;white-space:nowrap;display:inline-block;min-width:19.9pt}.lst-kix_list_3-8>li:before{content:"\002022   "}li.li-bullet-46:before{margin-left:-28.1pt;white-space:nowrap;display:inline-block;min-width:28.1pt}li.li-bullet-42:before{margin-left:-19.9pt;white-space:nowrap;display:inline-block;min-width:19.9pt}.lst-kix_list_2-0>li{counter-increment:lst-ctn-kix_list_2-0}li.li-bullet-39:before{margin-left:-19.6pt;white-space:nowrap;display:inline-block;min-width:19.6pt}li.li-bullet-33:before{margin-left:-19.6pt;white-space:nowrap;display:inline-block;min-width:19.6pt}.lst-kix_list_3-6>li:before{content:"\002022   "}li.li-bullet-31:before{margin-left:-19.9pt;white-space:nowrap;display:inline-block;min-width:19.9pt}.lst-kix_list_3-7>li:before{content:"\002022   "}li.li-bullet-48:before{margin-left:-19.8pt;white-space:nowrap;display:inline-block;min-width:19.8pt}li.li-bullet-40:before{margin-left:-19.8pt;white-space:nowrap;display:inline-block;min-width:19.8pt}ol.lst-kix_list_4-2.start{counter-reset:lst-ctn-kix_list_4-2 0}ol.lst-kix_list_2-0{list-style-type:none}ol.lst-kix_list_2-1{list-style-type:none}.lst-kix_list_4-8>li:before{content:"\002022   "}.lst-kix_list_4-7>li:before{content:"\002022   "}li.li-bullet-6:before{margin-left:-19.2pt;white-space:nowrap;display:inline-block;min-width:19.2pt}li.li-bullet-7:before{margin-left:-19.2pt;white-space:nowrap;display:inline-block;min-width:19.2pt}ul.lst-kix_list_4-8{list-style-type:none}ul.lst-kix_list_4-6{list-style-type:none}ol.lst-kix_list_4-1.start{counter-reset:lst-ctn-kix_list_4-1 0}ul.lst-kix_list_4-7{list-style-type:none}li.li-bullet-10:before{margin-left:-14pt;white-space:nowrap;display:inline-block;min-width:14pt}li.li-bullet-13:before{margin-left:-15.4pt;white-space:nowrap;display:inline-block;min-width:15.4pt}li.li-bullet-14:before{margin-left:-14.8pt;white-space:nowrap;display:inline-block;min-width:14.8pt}ul.lst-kix_list_4-4{list-style-type:none}ul.lst-kix_list_4-5{list-style-type:none}ul.lst-kix_list_4-3{list-style-type:none}li.li-bullet-36:before{margin-left:-19.6pt;white-space:nowrap;display:inline-block;min-width:19.6pt}li.li-bullet-18:before{margin-left:-15.4pt;white-space:nowrap;display:inline-block;min-width:15.4pt}ol.lst-kix_list_1-0.start{counter-reset:lst-ctn-kix_list_1-0 0}li.li-bullet-43:before{margin-left:-19.6pt;white-space:nowrap;display:inline-block;min-width:19.6pt}.lst-kix_list_3-0>li{counter-increment:lst-ctn-kix_list_3-0}li.li-bullet-32:before{margin-left:-19.9pt;white-space:nowrap;display:inline-block;min-width:19.9pt}ol.lst-kix_list_4-0.start{counter-reset:lst-ctn-kix_list_4-0 0}li.li-bullet-2:before{margin-left:-24.7pt;white-space:nowrap;display:inline-block;min-width:24.7pt}li.li-bullet-21:before{margin-left:-19.9pt;white-space:nowrap;display:inline-block;min-width:19.9pt}li.li-bullet-25:before{margin-left:-19.9pt;white-space:nowrap;display:inline-block;min-width:19.9pt}li.li-bullet-29:before{margin-left:-19.9pt;white-space:nowrap;display:inline-block;min-width:19.9pt}li.li-bullet-47:before{margin-left:-19.7pt;white-space:nowrap;display:inline-block;min-width:19.7pt}li.li-bullet-50:before{margin-left:-19.8pt;white-space:nowrap;display:inline-block;min-width:19.8pt}li.li-bullet-54:before{margin-left:-19.8pt;white-space:nowrap;display:inline-block;min-width:19.8pt}.lst-kix_list_2-6>li:before{content:"\002022   "}.lst-kix_list_2-7>li:before{content:"\002022   "}ol.lst-kix_list_1-0{list-style-type:none}.lst-kix_list_2-4>li:before{content:"\002022   "}.lst-kix_list_2-5>li:before{content:"\002022   "}.lst-kix_list_2-8>li:before{content:"\002022   "}ol.lst-kix_list_1-1{list-style-type:none}li.li-bullet-20:before{margin-left:-19.4pt;white-space:nowrap;display:inline-block;min-width:19.4pt}ul.lst-kix_list_3-7{list-style-type:none}ul.lst-kix_list_3-8{list-style-type:none}ol.lst-kix_list_3-0.start{counter-reset:lst-ctn-kix_list_3-0 44}ul.lst-kix_list_3-2{list-style-type:none}ul.lst-kix_list_3-5{list-style-type:none}ul.lst-kix_list_3-6{list-style-type:none}ul.lst-kix_list_3-3{list-style-type:none}ul.lst-kix_list_3-4{list-style-type:none}li.li-bullet-19:before{margin-left:-14.9pt;white-space:nowrap;display:inline-block;min-width:14.9pt}li.li-bullet-53:before{margin-left:-19.6pt;white-space:nowrap;display:inline-block;min-width:19.6pt}li.li-bullet-17:before{margin-left:-15.4pt;white-space:nowrap;display:inline-block;min-width:15.4pt}li.li-bullet-1:before{margin-left:-109.6pt;white-space:nowrap;display:inline-block;min-width:109.6pt}li.li-bullet-24:before{margin-left:-19.9pt;white-space:nowrap;display:inline-block;min-width:19.9pt}.lst-kix_list_4-0>li:before{content:"" counter(lst-ctn-kix_list_4-0,decimal) ". "}li.li-bullet-15:before{margin-left:-15.4pt;white-space:nowrap;display:inline-block;min-width:15.4pt}.lst-kix_list_4-1>li:before{content:"" counter(lst-ctn-kix_list_4-0,decimal) "." counter(lst-ctn-kix_list_4-1,decimal) ". "}li.li-bullet-3:before{margin-left:-19.2pt;white-space:nowrap;display:inline-block;min-width:19.2pt}li.li-bullet-22:before{margin-left:-19.9pt;white-space:nowrap;display:inline-block;min-width:19.9pt}li.li-bullet-26:before{margin-left:-19.9pt;white-space:nowrap;display:inline-block;min-width:19.9pt}li.li-bullet-28:before{margin-left:-19.9pt;white-space:nowrap;display:inline-block;min-width:19.9pt}.lst-kix_list_4-4>li:before{content:"\002022   "}.lst-kix_list_4-3>li:before{content:"\002022   "}.lst-kix_list_4-5>li:before{content:"\002022   "}.lst-kix_list_4-2>li:before{content:"(" counter(lst-ctn-kix_list_4-2,lower-latin) ") "}.lst-kix_list_4-6>li:before{content:"\002022   "}ol.lst-kix_list_1-1.start{counter-reset:lst-ctn-kix_list_1-1 0}li.li-bullet-51:before{margin-left:-19.9pt;white-space:nowrap;display:inline-block;min-width:19.9pt}ol.lst-kix_list_4-0{list-style-type:none}ol.lst-kix_list_4-1{list-style-type:none}ol.lst-kix_list_4-2{list-style-type:none}li.li-bullet-8:before{margin-left:-14.7pt;white-space:nowrap;display:inline-block;min-width:14.7pt}li.li-bullet-9:before{margin-left:-6.8pt;white-space:nowrap;display:inline-block;min-width:6.8pt}li.li-bullet-5:before{margin-left:-108.6pt;white-space:nowrap;display:inline-block;min-width:108.6pt}li.li-bullet-12:before{margin-left:-15.4pt;white-space:nowrap;display:inline-block;min-width:15.4pt}ul.lst-kix_list_2-8{list-style-type:none}li.li-bullet-11:before{margin-left:-14.7pt;white-space:nowrap;display:inline-block;min-width:14.7pt}ul.lst-kix_list_2-2{list-style-type:none}.lst-kix_list_1-0>li:before{content:"[" counter(lst-ctn-kix_list_1-0,decimal) "] "}ul.lst-kix_list_2-3{list-style-type:none}ul.lst-kix_list_2-6{list-style-type:none}.lst-kix_list_1-1>li:before{content:"" counter(lst-ctn-kix_list_1-1,upper-roman) ". "}.lst-kix_list_1-2>li:before{content:"\002022   "}ol.lst-kix_list_2-0.start{counter-reset:lst-ctn-kix_list_2-0 0}ul.lst-kix_list_2-7{list-style-type:none}ul.lst-kix_list_2-4{list-style-type:none}ul.lst-kix_list_2-5{list-style-type:none}li.li-bullet-45:before{margin-left:-19.5pt;white-space:nowrap;display:inline-block;min-width:19.5pt}.lst-kix_list_1-3>li:before{content:"\002022   "}.lst-kix_list_1-4>li:before{content:"\002022   "}li.li-bullet-34:before{margin-left:-19.9pt;white-space:nowrap;display:inline-block;min-width:19.9pt}li.li-bullet-38:before{margin-left:-19.6pt;white-space:nowrap;display:inline-block;min-width:19.6pt}.lst-kix_list_1-0>li{counter-increment:lst-ctn-kix_list_1-0}li.li-bullet-16:before{margin-left:-15.4pt;white-space:nowrap;display:inline-block;min-width:15.4pt}.lst-kix_list_1-7>li:before{content:"\002022   "}.lst-kix_list_1-5>li:before{content:"\002022   "}.lst-kix_list_1-6>li:before{content:"\002022   "}li.li-bullet-0:before{margin-left:-12pt;white-space:nowrap;display:inline-block;min-width:12pt}li.li-bullet-4:before{margin-left:-12.2pt;white-space:nowrap;display:inline-block;min-width:12.2pt}li.li-bullet-23:before{margin-left:-19.9pt;white-space:nowrap;display:inline-block;min-width:19.9pt}li.li-bullet-27:before{margin-left:-19.5pt;white-space:nowrap;display:inline-block;min-width:19.5pt}li.li-bullet-30:before{margin-left:-19.9pt;white-space:nowrap;display:inline-block;min-width:19.9pt}.lst-kix_list_2-0>li:before{content:"" counter(lst-ctn-kix_list_2-0,upper-latin) ". "}.lst-kix_list_2-1>li:before{content:"(" counter(lst-ctn-kix_list_2-1,lower-latin) ") "}ol.lst-kix_list_2-1.start{counter-reset:lst-ctn-kix_list_2-1 0}li.li-bullet-41:before{margin-left:-28.1pt;white-space:nowrap;display:inline-block;min-width:28.1pt}.lst-kix_list_1-8>li:before{content:"\002022   "}.lst-kix_list_2-2>li:before{content:"\002022   "}.lst-kix_list_2-3>li:before{content:"\002022   "}li.li-bullet-49:before{margin-left:-19.7pt;white-space:nowrap;display:inline-block;min-width:19.7pt}li.li-bullet-52:before{margin-left:-19.6pt;white-space:nowrap;display:inline-block;min-width:19.6pt}.lst-kix_list_4-2>li{counter-increment:lst-ctn-kix_list_4-2}ol{margin:0;padding:0}table td,table th{padding:0}.c213{border-right-style:solid;border-bottom-color:#000000;border-top-width:1.5pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1.5pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1.5pt;width:45.5pt;border-top-color:#000000;border-bottom-style:solid}.c286{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:31.9pt;border-top-color:#000000;border-bottom-style:solid}.c104{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:57pt;border-top-color:#000000;border-bottom-style:solid}.c628{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:63.4pt;border-top-color:#000000;border-bottom-style:solid}.c29{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:45.9pt;border-top-color:#000000;border-bottom-style:solid}.c528{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:26.8pt;border-top-color:#000000;border-bottom-style:solid}.c145{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:35.8pt;border-top-color:#000000;border-bottom-style:solid}.c574{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:44.2pt;border-top-color:#000000;border-bottom-style:solid}.c320{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:30.1pt;border-top-color:#000000;border-bottom-style:solid}.c395{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:35.8pt;border-top-color:#000000;border-bottom-style:solid}.c513{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:35.8pt;border-top-color:#000000;border-bottom-style:solid}.c323{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:33.5pt;border-top-color:#000000;border-bottom-style:solid}.c386{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:43.4pt;border-top-color:#000000;border-bottom-style:solid}.c380{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:59.2pt;border-top-color:#000000;border-bottom-style:solid}.c364{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:44.2pt;border-top-color:#000000;border-bottom-style:solid}.c480{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:45.9pt;border-top-color:#000000;border-bottom-style:solid}.c94{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:26.8pt;border-top-color:#000000;border-bottom-style:solid}.c373{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:21.1pt;border-top-color:#000000;border-bottom-style:solid}.c78{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:25.1pt;border-top-color:#000000;border-bottom-style:solid}.c125{border-right-style:solid;border-bottom-color:#000000;border-top-width:1.5pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1.5pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:45.5pt;border-top-color:#000000;border-bottom-style:solid}.c60{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:57pt;border-top-color:#000000;border-bottom-style:solid}.c533{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:68.5pt;border-top-color:#000000;border-bottom-style:solid}.c460{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:45.9pt;border-top-color:#000000;border-bottom-style:solid}.c310{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:21.1pt;border-top-color:#000000;border-bottom-style:solid}.c138{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:73.5pt;border-top-color:#000000;border-bottom-style:solid}.c47{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:57pt;border-top-color:#000000;border-bottom-style:solid}.c184{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:73.5pt;border-top-color:#000000;border-bottom-style:solid}.c469{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:39.1pt;border-top-color:#000000;border-bottom-style:solid}.c219{border-right-style:solid;border-bottom-color:#000000;border-top-width:1.5pt;border-right-width:1.5pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1.5pt;width:26.4pt;border-top-color:#000000;border-bottom-style:solid}.c165{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:21.1pt;border-top-color:#000000;border-bottom-style:solid}.c220{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:38.5pt;border-top-color:#000000;border-bottom-style:solid}.c172{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:26.8pt;border-top-color:#000000;border-bottom-style:solid}.c590{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:128.4pt;border-top-color:#000000;border-bottom-style:solid}.c413{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:73.5pt;border-top-color:#000000;border-bottom-style:solid}.c356{border-right-style:solid;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:59.2pt;border-top-color:#000000;border-bottom-style:solid}.c210{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:68.5pt;border-top-color:#000000;border-bottom-style:solid}.c586{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:68.5pt;border-top-color:#000000;border-bottom-style:solid}.c90{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:39.1pt;border-top-color:#000000;border-bottom-style:solid}.c538{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:32.4pt;border-top-color:#000000;border-bottom-style:solid}.c247{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:63.4pt;border-top-color:#000000;border-bottom-style:solid}.c214{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:39.3pt;border-top-color:#000000;border-bottom-style:solid}.c295{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:33.5pt;border-top-color:#000000;border-bottom-style:solid}.c195{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:30.1pt;border-top-color:#000000;border-bottom-style:solid}.c622{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:45.9pt;border-top-color:#000000;border-bottom-style:solid}.c594{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:63.4pt;border-top-color:#000000;border-bottom-style:solid}.c95{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:38.5pt;border-top-color:#000000;border-bottom-style:solid}.c129{border-right-style:solid;border-bottom-color:#000000;border-top-width:1pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:45.9pt;border-top-color:#000000;border-bottom-style:solid}.c270{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:28.2pt;border-top-color:#000000;border-bottom-style:solid}.c516{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:31.9pt;border-top-color:#000000;border-bottom-style:solid}.c259{border-right-style:solid;border-bottom-color:#000000;border-top-width:1.5pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1.5pt;width:30.4pt;border-top-color:#000000;border-bottom-style:solid}.c404{border-right-style:solid;border-bottom-color:#000000;border-top-width:1.5pt;border-right-width:1.5pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:26.4pt;border-top-color:#000000;border-bottom-style:solid}.c239{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:33.5pt;border-top-color:#000000;border-bottom-style:solid}.c630{border-right-style:solid;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:56.8pt;border-top-color:#000000;border-bottom-style:solid}.c417{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:55.9pt;border-top-color:#000000;border-bottom-style:solid}.c199{border-right-style:solid;border-bottom-color:#000000;border-top-width:1.5pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1.5pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1.5pt;width:30.6pt;border-top-color:#000000;border-bottom-style:solid}.c621{border-right-style:solid;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:136.8pt;border-top-color:#000000;border-bottom-style:solid}.c455{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:39.1pt;border-top-color:#000000;border-bottom-style:solid}.c459{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:22.4pt;border-top-color:#000000;border-bottom-style:solid}.c449{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:55.9pt;border-top-color:#000000;border-bottom-style:solid}.c218{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:56.8pt;border-top-color:#000000;border-bottom-style:solid}.c66{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:59.4pt;border-top-color:#000000;border-bottom-style:solid}.c612{border-right-style:solid;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:73.5pt;border-top-color:#000000;border-bottom-style:solid}.c79{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:22.4pt;border-top-color:#000000;border-bottom-style:solid}.c520{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:59.2pt;border-top-color:#000000;border-bottom-style:solid}.c87{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:39.1pt;border-top-color:#000000;border-bottom-style:solid}.c350{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:44.2pt;border-top-color:#000000;border-bottom-style:solid}.c143{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:28.2pt;border-top-color:#000000;border-bottom-style:solid}.c453{border-right-style:solid;border-bottom-color:#000000;border-top-width:1pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:73.5pt;border-top-color:#000000;border-bottom-style:solid}.c616{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:128.4pt;border-top-color:#000000;border-bottom-style:solid}.c182{border-right-style:solid;border-bottom-color:#000000;border-top-width:1.5pt;border-right-width:1.5pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:40.2pt;border-top-color:#000000;border-bottom-style:solid}.c112{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:59.2pt;border-top-color:#000000;border-bottom-style:solid}.c131{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:56.8pt;border-top-color:#000000;border-bottom-style:solid}.c546{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:28.2pt;border-top-color:#000000;border-bottom-style:solid}.c284{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:59.4pt;border-top-color:#000000;border-bottom-style:solid}.c498{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:73.5pt;border-top-color:#000000;border-bottom-style:solid}.c450{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:25.1pt;border-top-color:#000000;border-bottom-style:solid}.c141{border-right-style:solid;border-bottom-color:#000000;border-top-width:1.5pt;border-right-width:1.5pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1.5pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1.5pt;width:30.4pt;border-top-color:#000000;border-bottom-style:solid}.c209{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:38.5pt;border-top-color:#000000;border-bottom-style:solid}.c321{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:73.5pt;border-top-color:#000000;border-bottom-style:solid}.c575{border-right-style:solid;border-bottom-color:#000000;border-top-width:1pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:45.9pt;border-top-color:#000000;border-bottom-style:solid}.c387{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:33.5pt;border-top-color:#000000;border-bottom-style:solid}.c588{border-right-style:solid;border-bottom-color:#000000;border-top-width:1pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:136.8pt;border-top-color:#000000;border-bottom-style:solid}.c504{border-right-style:solid;border-bottom-color:#000000;border-top-width:1.5pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1.5pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:30.6pt;border-top-color:#000000;border-bottom-style:solid}.c301{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:37.6pt;border-top-color:#000000;border-bottom-style:solid}.c337{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:56.8pt;border-top-color:#000000;border-bottom-style:solid}.c299{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:59.2pt;border-top-color:#000000;border-bottom-style:solid}.c92{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:73.5pt;border-top-color:#000000;border-bottom-style:solid}.c433{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:156.6pt;border-top-color:#000000;border-bottom-style:solid}.c322{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:68.5pt;border-top-color:#000000;border-bottom-style:solid}.c397{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:22.4pt;border-top-color:#000000;border-bottom-style:solid}.c243{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:30.1pt;border-top-color:#000000;border-bottom-style:solid}.c249{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:73.5pt;border-top-color:#000000;border-bottom-style:solid}.c555{border-right-style:solid;border-bottom-color:#000000;border-top-width:1pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:59.2pt;border-top-color:#000000;border-bottom-style:solid}.c407{border-right-style:solid;border-bottom-color:#000000;border-top-width:1pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:59.2pt;border-top-color:#000000;border-bottom-style:solid}.c300{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:32.4pt;border-top-color:#000000;border-bottom-style:solid}.c232{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:43.4pt;border-top-color:#000000;border-bottom-style:solid}.c20{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:63.4pt;border-top-color:#000000;border-bottom-style:solid}.c432{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:37.6pt;border-top-color:#000000;border-bottom-style:solid}.c389{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:55.9pt;border-top-color:#000000;border-bottom-style:solid}.c341{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:28.2pt;border-top-color:#000000;border-bottom-style:solid}.c280{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:56.8pt;border-top-color:#000000;border-bottom-style:solid}.c256{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:39.1pt;border-top-color:#000000;border-bottom-style:solid}.c170{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:63.4pt;border-top-color:#000000;border-bottom-style:solid}.c589{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:55.9pt;border-top-color:#000000;border-bottom-style:solid}.c262{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:32.4pt;border-top-color:#000000;border-bottom-style:solid}.c518{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:63.4pt;border-top-color:#000000;border-bottom-style:solid}.c475{border-right-style:solid;border-bottom-color:#000000;border-top-width:1.5pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:30.4pt;border-top-color:#000000;border-bottom-style:solid}.c445{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:39.3pt;border-top-color:#000000;border-bottom-style:solid}.c244{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:31.9pt;border-top-color:#000000;border-bottom-style:solid}.c82{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:21.1pt;border-top-color:#000000;border-bottom-style:solid}.c587{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:26.8pt;border-top-color:#000000;border-bottom-style:solid}.c552{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:56.8pt;border-top-color:#000000;border-bottom-style:solid}.c534{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:25.1pt;border-top-color:#000000;border-bottom-style:solid}.c607{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:22.4pt;border-top-color:#000000;border-bottom-style:solid}.c405{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:63.4pt;border-top-color:#000000;border-bottom-style:solid}.c227{border-right-style:solid;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:26.8pt;border-top-color:#000000;border-bottom-style:solid}.c331{border-right-style:solid;border-bottom-color:#000000;border-top-width:1pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:25.1pt;border-top-color:#000000;border-bottom-style:solid}.c464{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:38.5pt;border-top-color:#000000;border-bottom-style:solid}.c185{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:59.2pt;border-top-color:#000000;border-bottom-style:solid}.c367{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:59.2pt;border-top-color:#000000;border-bottom-style:solid}.c508{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:31.9pt;border-top-color:#000000;border-bottom-style:solid}.c577{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:26.8pt;border-top-color:#000000;border-bottom-style:solid}.c160{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:128.4pt;border-top-color:#000000;border-bottom-style:solid}.c107{border-right-style:solid;border-bottom-color:#000000;border-top-width:1.5pt;border-right-width:1.5pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1.5pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:30.4pt;border-top-color:#000000;border-bottom-style:solid}.c201{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:25.1pt;border-top-color:#000000;border-bottom-style:solid}.c293{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:25.1pt;border-top-color:#000000;border-bottom-style:solid}.c349{border-right-style:solid;border-bottom-color:#000000;border-top-width:1pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:38.5pt;border-top-color:#000000;border-bottom-style:solid}.c368{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:26.8pt;border-top-color:#000000;border-bottom-style:solid}.c205{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:33.5pt;border-top-color:#000000;border-bottom-style:solid}.c398{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:39.3pt;border-top-color:#000000;border-bottom-style:solid}.c376{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1.5pt;width:30.6pt;border-top-color:#000000;border-bottom-style:solid}.c485{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:30.1pt;border-top-color:#000000;border-bottom-style:solid}.c105{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1.5pt;width:30.4pt;border-top-color:#000000;border-bottom-style:solid}.c553{border-right-style:solid;border-bottom-color:#000000;border-top-width:2.2pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:2.2pt;width:123.8pt;border-top-color:#000000;border-bottom-style:solid}.c360{border-right-style:solid;border-bottom-color:#000000;border-top-width:1pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:33.5pt;border-top-color:#000000;border-bottom-style:solid}.c576{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1.5pt;width:30.4pt;border-top-color:#000000;border-bottom-style:solid}.c288{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:32.4pt;border-top-color:#000000;border-bottom-style:solid}.c215{border-right-style:solid;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1.5pt;width:40.2pt;border-top-color:#000000;border-bottom-style:solid}.c86{border-right-style:solid;border-bottom-color:#000000;border-top-width:1.5pt;border-right-width:1.5pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1.5pt;width:40.2pt;border-top-color:#000000;border-bottom-style:solid}.c335{margin-left:28pt;padding-top:0.6pt;list-style-position:inside;text-indent:45pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:11.2pt}.c89{margin-left:7.8pt;padding-top:0pt;list-style-position:inside;text-indent:45pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:11.1pt}.c142{margin-left:8.1pt;padding-top:6.7pt;text-indent:11.9pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:2.3pt}.c637{margin-left:26.1pt;padding-top:1.4pt;padding-left:1.8pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:12.8pt}.c7{margin-left:26.1pt;padding-top:1.3pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:1.9pt}.c486{margin-left:26.1pt;padding-top:0.9pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:left;margin-right:269.9pt}.c494{margin-left:8.1pt;padding-top:0pt;text-indent:-0.3pt;padding-bottom:0pt;line-height:1.0291666666666666;text-align:justify;margin-right:2pt}.c46{margin-left:26pt;padding-top:0.8pt;padding-left:1.4pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:11.2pt}.c224{margin-left:7.5pt;padding-top:1.6pt;text-indent:0.3pt;padding-bottom:0pt;line-height:1.0166666666666666;text-align:justify;margin-right:11.2pt}.c446{margin-left:30.4pt;padding-top:0.8pt;padding-left:-2.6pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:11.2pt}.c561{margin-left:8.1pt;padding-top:0pt;text-indent:-1.3pt;padding-bottom:0pt;line-height:1.0;text-align:justify;margin-right:3.6pt}.c544{margin-left:26.1pt;padding-top:0.8pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:3.1pt}.c474{margin-left:7.6pt;padding-top:2pt;text-indent:12.4pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:1.9pt}.c206{-webkit-text-decoration-skip:none;color:#000000;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-family:"Times New Roman";font-style:normal}.c106{margin-left:26.1pt;padding-top:3.5pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:1.9pt}.c363{margin-left:25.9pt;padding-top:0.8pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:11.2pt}.c483{margin-left:17.9pt;padding-top:0.7pt;padding-left:10.1pt;padding-bottom:0pt;line-height:1.0;text-align:right;margin-right:3.5pt}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Times New Roman";font-style:normal}.c302{margin-left:6.6pt;padding-top:7.8pt;text-indent:13.1pt;padding-bottom:0pt;line-height:1.0;text-align:justify;margin-right:11.1pt}.c316{margin-left:26.1pt;padding-top:0.7pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:1.9pt}.c34{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c187{margin-left:26.1pt;padding-top:0.8pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:12.8pt}.c436{margin-left:30.4pt;padding-top:0pt;padding-left:-2.6pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:12.4pt}.c61{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c0{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Calibri";font-style:italic}.c18{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:10pt;font-family:"Roboto";font-style:normal}.c8{color:#00ff00;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c150{margin-left:26.1pt;padding-top:1.4pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:left;margin-right:11.2pt}.c117{margin-left:7.8pt;padding-top:1.2pt;text-indent:12.3pt;padding-bottom:0pt;line-height:1.0291666666666666;text-align:justify;margin-right:1.9pt}.c578{margin-left:26.1pt;padding-top:1.3pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:3.5pt}.c67{margin-left:7.3pt;padding-top:0.3pt;text-indent:11.9pt;padding-bottom:0pt;line-height:1.0291666666666666;text-align:justify;margin-right:11.5pt}.c638{margin-left:26.1pt;padding-top:0.8pt;padding-left:1.5pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:1.9pt}.c50{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Helvetica Neue";font-style:normal}.c392{margin-left:30.6pt;padding-top:1.5pt;padding-left:-2.6pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:2.5pt}.c548{margin-left:47.2pt;padding-top:2.4pt;padding-left:90.6pt;padding-bottom:0pt;line-height:0.9874999999999999;text-align:left;margin-right:33.9pt}.c530{margin-left:8pt;padding-top:6.8pt;text-indent:-0.3pt;padding-bottom:0pt;line-height:1.05;text-align:justify;margin-right:11.2pt}.c451{margin-left:6.9pt;padding-top:0.9pt;text-indent:13.1pt;padding-bottom:0pt;line-height:1.0166666666666666;text-align:justify;margin-right:1.9pt}.c592{margin-left:26.1pt;padding-top:0.8pt;padding-left:1.6pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:1.9pt}.c382{margin-left:7.8pt;padding-top:7.4pt;text-indent:-0.3pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:11.5pt}.c15{color:#ff0000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c600{margin-left:7.5pt;padding-top:12.4pt;text-indent:12.6pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:1.9pt}.c564{margin-left:26.1pt;padding-top:1.4pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:left;margin-right:12.6pt}.c595{margin-left:26.1pt;padding-top:1.7pt;padding-left:1.6pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:12.8pt}.c639{margin-left:26.1pt;padding-top:0.7pt;padding-left:1.6pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:1.9pt}.c396{margin-left:26.1pt;padding-top:1.3pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:left;margin-right:3.5pt}.c472{margin-left:7.6pt;padding-top:3.1pt;text-indent:12.3pt;padding-bottom:0pt;line-height:1.0291666666666666;text-align:justify;margin-right:11pt}.c171{margin-left:26.1pt;padding-top:1.3pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:2.4pt}.c176{margin-left:7.8pt;padding-top:1.4pt;text-indent:12.3pt;padding-bottom:0pt;line-height:1.0166666666666666;text-align:justify;margin-right:1.9pt}.c260{margin-left:7.8pt;padding-top:0pt;text-indent:12.3pt;padding-bottom:0pt;line-height:1.0291666666666666;text-align:justify;margin-right:1.9pt}.c613{margin-left:26.1pt;padding-top:1.2pt;padding-left:1.6pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:3.5pt}.c596{margin-left:26.1pt;padding-top:0.7pt;padding-left:1.8pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:12.8pt}.c379{margin-left:26.1pt;padding-top:1.4pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:12.8pt}.c584{margin-left:30.4pt;padding-top:0.8pt;padding-left:-3.2pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:11.7pt}.c471{margin-left:26.1pt;padding-top:0.7pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:11.2pt}.c370{margin-left:26.1pt;padding-top:1.4pt;padding-left:1.6pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:12.8pt}.c550{margin-left:7.8pt;padding-top:4.8pt;text-indent:0.3pt;padding-bottom:0pt;line-height:1.0291666666666666;text-align:justify;margin-right:1.9pt}.c371{margin-left:26.1pt;padding-top:3.5pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:left;margin-right:271.5pt}.c93{margin-left:7.8pt;padding-top:7.6pt;text-indent:11.9pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:11.1pt}.c456{margin-left:47.3pt;padding-top:2.4pt;padding-left:91.6pt;padding-bottom:0pt;line-height:1.0291666666666666;text-align:left;margin-right:5.6pt}.c108{margin-left:26.1pt;padding-top:0.8pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:1.9pt}.c273{margin-left:25.9pt;padding-top:0.7pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:12.8pt}.c137{margin-left:7.8pt;padding-top:0.5pt;text-indent:12.3pt;padding-bottom:0pt;line-height:1.0166666666666666;text-align:justify;margin-right:1.9pt}.c558{margin-left:7.6pt;padding-top:7pt;text-indent:12.4pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:1.9pt}.c19{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Calibri";font-style:normal}.c120{margin-left:7.8pt;padding-top:7.3pt;text-indent:12.2pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:11.1pt}.c32{margin-left:7.8pt;padding-top:0pt;text-indent:12.2pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:1.9pt}.c35{margin-left:7pt;padding-top:1.1pt;text-indent:12.7pt;padding-bottom:0pt;line-height:1.0166666666666666;text-align:justify;margin-right:11pt}.c135{margin-left:8.1pt;padding-top:3.5pt;text-indent:-0.8pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:1.9pt}.c22{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Times New Roman";font-style:normal}.c353{margin-left:30.4pt;padding-top:0.8pt;padding-left:-2.6pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:12.8pt}.c162{margin-left:7.8pt;padding-top:0.4pt;text-indent:12.3pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:1.9pt}.c283{margin-left:7.1pt;padding-top:8.4pt;text-indent:-0.3pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:12.8pt}.c497{margin-left:7.8pt;padding-top:0.3pt;text-indent:12.3pt;padding-bottom:0pt;line-height:1.0;text-align:justify;margin-right:1.9pt}.c40{margin-left:8pt;padding-top:3.5pt;text-indent:11.9pt;padding-bottom:0pt;line-height:1.0166666666666666;text-align:justify;margin-right:11.5pt}.c608{margin-left:8.1pt;padding-top:4.8pt;text-indent:-0.4pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:3.6pt}.c252{margin-left:7.8pt;padding-top:3.6pt;text-indent:0.3pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:3pt}.c326{margin-left:26.1pt;padding-top:0.7pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:3.5pt}.c443{margin-left:7.6pt;padding-top:0.3pt;text-indent:12.3pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:11.5pt}.c388{margin-left:26.1pt;padding-top:1.3pt;padding-left:1.5pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:1.9pt}.c39{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Times New Roman";font-style:normal}.c358{margin-left:27.4pt;padding-top:0.6pt;text-indent:0.5pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:11.3pt}.c345{margin-left:7.8pt;padding-top:0.5pt;text-indent:12.3pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:1.9pt}.c309{margin-left:7.6pt;padding-top:0.3pt;text-indent:12.4pt;padding-bottom:0pt;line-height:1.0166666666666666;text-align:justify;margin-right:1.9pt}.c77{margin-left:26.1pt;padding-top:1.2pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:3.5pt}.c501{margin-left:7.8pt;padding-top:4.7pt;text-indent:0.3pt;padding-bottom:0pt;line-height:1.0;text-align:justify;margin-right:3.4pt}.c481{margin-left:6.8pt;padding-top:0.5pt;text-indent:12.3pt;padding-bottom:0pt;line-height:1.0291666666666666;text-align:justify;margin-right:11.1pt}.c291{-webkit-text-decoration-skip:none;color:#000000;text-decoration:underline;vertical-align:super;text-decoration-skip-ink:none;font-size:18.3pt;font-family:"Verdana"}.c372{margin-left:7.7pt;padding-top:9.4pt;text-indent:12.2pt;padding-bottom:0pt;line-height:1.0291666666666666;text-align:justify;margin-right:11pt}.c343{margin-left:26.1pt;padding-top:1.1pt;padding-left:1.6pt;padding-bottom:0pt;line-height:0.9166666666666666;text-align:justify;margin-right:12.5pt}.c521{margin-left:28pt;padding-top:0.6pt;text-indent:-0.2pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:3.5pt}.c467{margin-left:26.1pt;padding-top:0.8pt;padding-left:1.6pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:3.5pt}.c133{margin-left:7.8pt;padding-top:4.8pt;text-indent:0.2pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:11.1pt}.c369{margin-left:27.7pt;padding-top:0.6pt;text-indent:0.2pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:3.5pt}.c626{margin-left:30.4pt;padding-top:0.7pt;padding-left:-2.6pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:12.8pt}.c441{margin-left:26.1pt;padding-top:1.3pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:3.3pt}.c535{margin-left:7.8pt;padding-top:0.5pt;text-indent:-0.3pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:12.8pt}.c627{margin-left:26.1pt;padding-top:0.8pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:2.4pt}.c359{margin-left:7.8pt;padding-top:4.6pt;text-indent:12.2pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:11pt}.c28{margin-left:26.1pt;padding-top:1.4pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:11.2pt}.c248{margin-left:7.6pt;padding-top:8.3pt;text-indent:12.4pt;padding-bottom:0pt;line-height:1.0166666666666666;text-align:justify;margin-right:1.9pt}.c122{margin-left:26.1pt;padding-top:0.8pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:3.5pt}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Times New Roman";font-style:normal}.c384{margin-left:26.1pt;padding-top:1.2pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:1.9pt}.c110{margin-left:26.1pt;padding-top:0.8pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:11.2pt}.c461{margin-left:26.1pt;padding-top:0.7pt;padding-left:1.8pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:11.2pt}.c75{margin-left:26.1pt;padding-top:0.8pt;padding-left:1.7pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:11.2pt}.c462{margin-left:7.5pt;padding-top:0.9pt;text-indent:12.3pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:11.1pt}.c26{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:italic}.c130{margin-left:30.6pt;padding-top:7.5pt;padding-left:-2.6pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:2pt}.c585{margin-left:7.7pt;padding-top:5.8pt;text-indent:0.4pt;padding-bottom:0pt;line-height:1.0;text-align:justify;margin-right:1.9pt}.c186{margin-left:6.6pt;padding-top:0pt;text-indent:1.2pt;padding-bottom:0pt;line-height:1.05;text-align:justify;margin-right:2pt}.c62{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Helvetica Neue";font-style:normal}.c351{margin-left:26.1pt;padding-top:1.3pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:left;margin-right:1.9pt}.c366{margin-left:30.4pt;padding-top:0.7pt;padding-left:-3.1pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:11.2pt}.c465{margin-left:26.1pt;padding-top:0.8pt;padding-left:1.6pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:11.2pt}.c354{margin-left:26pt;padding-top:0.7pt;padding-left:1.4pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:11.2pt}.c269{margin-left:7.2pt;padding-top:7pt;text-indent:-0.3pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:11.2pt}.c81{margin-left:28pt;padding-top:0.6pt;text-indent:-0.2pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:12.8pt}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c568{margin-left:26.1pt;padding-top:0.8pt;padding-left:1.6pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:11.3pt}.c427{margin-left:26.1pt;padding-top:1.4pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:1.9pt}.c96{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Arial";font-style:normal}.c234{margin-left:26.1pt;padding-top:0.8pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:12.4pt}.c614{margin-left:26.1pt;padding-top:0.8pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:12.6pt}.c31{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7.5pt;font-family:"Arial";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:13pt;font-family:"Times New Roman";font-style:normal}.c377{margin-left:26.1pt;padding-top:1.4pt;padding-left:1.8pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:11.2pt}.c540{margin-left:26.1pt;padding-top:6.8pt;padding-left:6.7pt;padding-bottom:0pt;line-height:0.9500000000000001;text-align:left;margin-right:3.6pt}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9.5pt;font-family:"Times New Roman";font-style:normal}.c492{margin-left:7.8pt;padding-top:7.4pt;text-indent:12.3pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:1.9pt}.c597{margin-left:5.2pt;padding-top:0pt;padding-bottom:0pt;line-height:0.8041666666666667;text-align:center;margin-right:4.2pt}.c157{margin-left:8.1pt;padding-top:4.7pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:11.1pt}.c411{margin-left:25.9pt;padding-top:0.1pt;padding-left:-6pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c509{margin-left:8.1pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:11.4pt}.c306{margin-left:12.6pt;padding-top:1.5pt;padding-bottom:0pt;line-height:0.7541666666666668;text-align:center;margin-right:12.1pt}.c545{margin-left:7.2pt;padding-top:1.4pt;padding-bottom:0pt;line-height:0.7958333333333334;text-align:center;margin-right:7.2pt}.c502{margin-left:28pt;padding-top:0.6pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:2.4pt}.c591{margin-left:7.8pt;padding-top:1pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:11.1pt}.c303{color:#000000;text-decoration:none;vertical-align:super;font-size:18.3pt;font-family:"Times New Roman";font-style:normal}.c173{margin-left:3.8pt;padding-top:0.3pt;padding-bottom:0pt;line-height:0.6666666666666666;text-align:center;margin-right:2.5pt}.c228{margin-left:7.2pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:7.3pt}.c394{margin-left:7.2pt;padding-top:3.2pt;padding-bottom:0pt;line-height:0.7791666666666667;text-align:center;margin-right:7.3pt}.c330{margin-left:28pt;padding-top:0.6pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:12.8pt}.c333{-webkit-text-decoration-skip:none;color:#000000;text-decoration:underline;vertical-align:super;text-decoration-skip-ink:none;font-size:11.7pt}.c526{margin-left:26pt;padding-top:0.1pt;padding-left:-3.4pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c635{margin-left:7.2pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:7.2pt}.c237{margin-left:7.8pt;padding-top:0.1pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:left;margin-right:271.1pt}.c529{margin-left:4.8pt;padding-top:1.1pt;padding-bottom:0pt;line-height:0.7958333333333334;text-align:center;margin-right:4.8pt}.c100{color:#000000;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Helvetica Neue";font-style:normal}.c390{margin-left:4.8pt;padding-top:3.4pt;padding-bottom:0pt;line-height:0.8416666666666667;text-align:center;margin-right:4.8pt}.c338{margin-left:7.2pt;padding-top:1.1pt;padding-bottom:0pt;line-height:0.7958333333333334;text-align:center;margin-right:7.3pt}.c484{margin-left:4.2pt;padding-top:0.8pt;padding-bottom:0pt;line-height:0.6416666666666667;text-align:center;margin-right:3.3pt}.c496{margin-left:26.1pt;padding-top:9.9pt;padding-left:1.2pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c636{margin-left:28pt;padding-top:0.6pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:11.2pt}.c285{margin-left:5.8pt;padding-top:2.5pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:5.8pt}.c457{margin-left:26.1pt;padding-top:0.8pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0;text-align:justify}.c355{margin-left:5.6pt;padding-top:0.3pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:5.7pt}.c391{margin-left:8.1pt;padding-top:9.2pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:1.9pt}.c429{margin-left:8.1pt;padding-top:3.6pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:3.6pt}.c102{margin-left:7.2pt;padding-top:1.5pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:7.3pt}.c374{margin-left:8.1pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0291666666666666;text-align:justify;margin-right:1.9pt}.c532{margin-left:31.9pt;padding-top:2.5pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:31.8pt}.c307{margin-left:102.9pt;padding-top:5.3pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:98.5pt}.c603{margin-left:7.2pt;padding-top:1.6pt;padding-bottom:0pt;line-height:0.8208333333333333;text-align:center;margin-right:7.2pt}.c463{margin-left:8.1pt;padding-top:7.4pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:1.9pt}.c514{margin-left:25.3pt;padding-top:8.3pt;padding-left:1.2pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c312{margin-left:26.1pt;padding-top:6.8pt;padding-left:1.2pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c604{margin-left:7.2pt;padding-top:3.2pt;padding-bottom:0pt;line-height:0.7791666666666667;text-align:center;margin-right:7.2pt}.c222{margin-left:4.2pt;padding-top:0.8pt;padding-bottom:0pt;line-height:0.6583333333333333;text-align:center;margin-right:3.3pt}.c98{margin-left:7.5pt;padding-top:3.4pt;padding-bottom:0pt;line-height:0.8458333333333333;text-align:center;margin-right:7.2pt}.c128{margin-left:6.9pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:11.2pt}.c126{margin-left:4.8pt;padding-top:1.4pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:4.8pt}.c263{margin-left:12.6pt;padding-top:3.4pt;padding-bottom:0pt;line-height:0.7875;text-align:center;margin-right:12.1pt}.c495{margin-left:8pt;padding-top:7.5pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:11.1pt}.c340{margin-left:3.8pt;padding-top:0pt;padding-bottom:0pt;line-height:0.6124999999999999;text-align:center;margin-right:2.5pt}.c71{margin-left:28pt;padding-top:3.5pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:11.7pt}.c634{margin-left:5.6pt;padding-top:1.6pt;padding-bottom:0pt;line-height:0.7333333333333334;text-align:center;margin-right:5.7pt}.c314{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11.5pt;font-family:"Arial";font-style:normal}.c414{margin-left:5.5pt;padding-top:0.8pt;padding-bottom:0pt;line-height:0.6583333333333333;text-align:center;margin-right:4.3pt}.c258{margin-left:7.2pt;padding-top:1.4pt;padding-bottom:0pt;line-height:0.7958333333333334;text-align:center;margin-right:7.3pt}.c438{margin-left:6.7pt;padding-top:1pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:6.4pt}.c410{margin-left:12.6pt;padding-top:1.9pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:12.1pt}.c24{margin-left:5.5pt;padding-top:0.2pt;padding-bottom:0pt;line-height:0.6458333333333334;text-align:center;margin-right:4.3pt}.c203{margin-left:26.1pt;padding-top:3.5pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0;text-align:justify}.c72{margin-left:12.6pt;padding-top:1.5pt;padding-bottom:0pt;line-height:0.7875;text-align:center;margin-right:12.1pt}.c146{margin-left:6.8pt;padding-top:0pt;text-indent:13.2pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c632{margin-left:25.9pt;padding-top:0pt;padding-left:1.2pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c415{margin-left:7.8pt;padding-top:9.8pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:12.8pt}.c582{margin-left:26.1pt;padding-top:2.1pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0;text-align:justify}.c217{margin-left:28pt;padding-top:0.7pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:left;margin-right:271.1pt}.c189{margin-left:5.5pt;padding-top:0.7pt;padding-bottom:0pt;line-height:0.6583333333333333;text-align:center;margin-right:4.3pt}.c631{margin-left:53.8pt;padding-top:5.7pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:58.4pt}.c602{margin-left:4.8pt;padding-top:3.2pt;padding-bottom:0pt;line-height:0.85;text-align:center;margin-right:4.8pt}.c272{margin-left:8.1pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:11.1pt}.c424{margin-left:8.1pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:2.4pt}.c17{margin-left:7.2pt;padding-top:1.5pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:7.2pt}.c409{margin-left:26.1pt;padding-top:0.7pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0;text-align:justify}.c166{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Times New Roman";font-style:normal}.c348{margin-left:6.7pt;padding-top:2.3pt;padding-bottom:0pt;line-height:0.8291666666666666;text-align:center;margin-right:6.4pt}.c56{margin-left:7.8pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:justify;margin-right:11pt}.c365{margin-left:6.6pt;padding-top:0pt;padding-bottom:0pt;line-height:0.8791666666666668;text-align:center;margin-right:5.3pt}.c174{margin-left:48.4pt;padding-top:0pt;padding-left:-11.2pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c44{margin-left:53.8pt;padding-top:6.2pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:58.4pt}.c539{margin-left:28pt;padding-top:0.6pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:2.3pt}.c148{color:#000000;text-decoration:none;vertical-align:baseline;font-size:9.5pt;font-family:"Helvetica Neue";font-style:normal}.c624{margin-left:4.8pt;padding-top:3.2pt;padding-bottom:0pt;line-height:0.7791666666666667;text-align:center;margin-right:4.8pt}.c305{margin-left:8.1pt;padding-top:0.5pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:271.5pt}.c289{margin-left:53.8pt;padding-top:0.7pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:58.4pt}.c59{margin-left:4.9pt;padding-top:1.9pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:4.9pt}.c282{margin-left:26.1pt;padding-top:0.9pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c477{margin-left:4.8pt;padding-top:1.6pt;padding-bottom:0pt;line-height:0.8208333333333333;text-align:center;margin-right:4.8pt}.c440{margin-left:192.8pt;padding-top:2.6pt;padding-left:-5.8pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c109{margin-left:8.1pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:1.9pt}.c598{margin-left:7.5pt;padding-top:0.9pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:11.2pt}.c488{margin-left:6.6pt;padding-top:0pt;padding-bottom:0pt;line-height:0.8624999999999999;text-align:center;margin-right:5.3pt}.c69{margin-left:6.6pt;padding-top:0pt;padding-bottom:0pt;line-height:0.875;text-align:center;margin-right:5.3pt}.c267{margin-left:7.2pt;padding-top:1.6pt;padding-bottom:0pt;line-height:0.8208333333333333;text-align:center;margin-right:7.3pt}.c226{margin-left:4pt;padding-top:0pt;padding-bottom:0pt;line-height:0.6124999999999999;text-align:center;margin-right:2.5pt}.c164{margin-left:4pt;padding-top:0.3pt;padding-bottom:0pt;line-height:0.6666666666666666;text-align:center;margin-right:2.5pt}.c161{margin-left:6.6pt;padding-top:0pt;padding-bottom:0pt;line-height:0.8041666666666667;text-align:center;margin-right:5.3pt}.c615{margin-left:7.6pt;padding-top:0.1pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:12.8pt}.c435{margin-left:26.1pt;padding-top:8.6pt;padding-left:1.2pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c149{color:#000000;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Times New Roman";font-style:normal}.c294{margin-left:4.8pt;padding-top:1.4pt;padding-bottom:0pt;line-height:0.7958333333333334;text-align:center;margin-right:4.8pt}.c169{margin-left:4.9pt;padding-top:3.4pt;padding-bottom:0pt;line-height:0.8458333333333333;text-align:center;margin-right:4.9pt}.c297{margin-left:26pt;padding-top:0pt;padding-left:-3.4pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c183{margin-left:8pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:11.1pt}.c416{margin-left:4.8pt;padding-top:1.5pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:4.8pt}.c571{margin-left:4.9pt;padding-top:1.5pt;padding-bottom:0pt;line-height:0.7541666666666668;text-align:center;margin-right:4.9pt}.c311{margin-left:7.2pt;padding-top:3.4pt;padding-bottom:0pt;line-height:0.7875;text-align:center;margin-right:7.3pt}.c565{margin-left:5.2pt;padding-top:0pt;padding-bottom:0pt;line-height:0.8791666666666668;text-align:center;margin-right:4.2pt}.c238{margin-left:8.1pt;padding-top:6.2pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:1.9pt}.c204{margin-left:8.1pt;padding-top:7.2pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:3.5pt}.c208{margin-left:7.3pt;padding-top:6.5pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:12.8pt}.c136{margin-left:5.6pt;padding-top:0.9pt;padding-bottom:0pt;line-height:0.7833333333333333;text-align:center;margin-right:5.7pt}.c583{margin-left:7.2pt;padding-top:1.1pt;padding-bottom:0pt;line-height:0.7958333333333334;text-align:center;margin-right:7.2pt}.c362{margin-left:26.1pt;padding-top:0pt;padding-left:-6pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c401{margin-left:25.7pt;padding-top:0pt;padding-left:-6pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c88{margin-left:4.3pt;padding-top:0.4pt;padding-bottom:0pt;line-height:0.5541666666666667;text-align:center;margin-right:4.1pt}.c599{margin-left:5.5pt;padding-top:0.8pt;padding-bottom:0pt;line-height:0.6458333333333334;text-align:center;margin-right:4.3pt}.c265{color:#000000;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Roboto";font-style:normal}.c557{margin-left:4.3pt;padding-top:0.7pt;padding-bottom:0pt;line-height:0.5541666666666667;text-align:center;margin-right:4.1pt}.c536{margin-left:5.5pt;padding-top:0.7pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:4.3pt}.c554{margin-left:8pt;padding-top:7.3pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:12.8pt}.c425{margin-left:8.1pt;padding-top:0.4pt;padding-bottom:0pt;line-height:1.0166666666666666;text-align:justify;margin-right:1.9pt}.c524{margin-left:12.7pt;padding-top:0.7pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:11.8pt}.c527{margin-left:4.2pt;padding-top:0.2pt;padding-bottom:0pt;line-height:0.6416666666666667;text-align:center;margin-right:3.3pt}.c84{margin-left:26.1pt;padding-top:1.3pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0;text-align:justify}.c499{margin-left:7.6pt;padding-top:5pt;padding-bottom:0pt;line-height:1.0166666666666666;text-align:justify;margin-right:11.1pt}.c212{margin-left:26.1pt;padding-top:1.4pt;padding-left:1.9pt;padding-bottom:0pt;line-height:1.0;text-align:justify}.c399{margin-left:8.1pt;padding-top:0.1pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:2.3pt}.c361{margin-left:26.1pt;padding-top:9.4pt;padding-left:-6pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c36{margin-left:25.9pt;padding-top:0.1pt;padding-left:-4pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c507{margin-left:7.8pt;padding-top:2.7pt;padding-bottom:0pt;line-height:0.9874999999999999;text-align:justify;margin-right:11.2pt}.c235{margin-left:53.8pt;padding-top:0pt;padding-bottom:0pt;line-height:1.1500000000000001;text-align:center;margin-right:58.4pt}.c551{margin-left:17.4pt;padding-top:3.4pt;padding-bottom:0pt;line-height:0.8458333333333333;text-align:center;margin-right:7.2pt}.c80{margin-left:8.1pt;padding-top:6.2pt;text-indent:11.9pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:left}.c113{margin-left:7.2pt;padding-top:3.4pt;padding-bottom:0pt;line-height:0.7875;text-align:center;margin-right:7.2pt}.c581{margin-left:26.1pt;padding-top:9.2pt;padding-left:-6pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c116{margin-left:4.8pt;padding-top:3.4pt;padding-bottom:0pt;line-height:0.7875;text-align:center;margin-right:4.8pt}.c132{margin-left:8.1pt;padding-top:1.2pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:justify;margin-right:13.2pt}.c192{margin-left:7.3pt;padding-top:0.7pt;padding-bottom:0pt;line-height:1.0374999999999999;text-align:justify;margin-right:11.1pt}.c559{margin-left:8.1pt;padding-top:0pt;padding-bottom:0pt;line-height:0.9791666666666666;text-align:justify;margin-right:3.6pt}.c278{margin-left:74pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0083333333333333;text-align:center;margin-right:78.7pt}.c121{margin-left:7.1pt;padding-top:8pt;padding-bottom:0pt;line-height:1.0083333333333333;text-align:justify;margin-right:11.1pt}.c304{padding-top:2.5pt;padding-bottom:0pt;line-height:0.8208333333333333;text-align:right;margin-right:8.8pt}.c197{margin-left:16pt;padding-top:0.5pt;padding-bottom:0pt;line-height:0.5499999999999999;text-align:left}.c101{margin-left:70.2pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c242{margin-left:0.4pt;padding-top:1.5pt;padding-bottom:0pt;line-height:0.7875;text-align:left}.c336{margin-left:21pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c439{margin-left:2pt;padding-top:3.4pt;padding-bottom:0pt;line-height:0.8458333333333333;text-align:left}.c510{margin-left:0.8pt;padding-top:0.8pt;padding-bottom:0pt;line-height:0.5499999999999999;text-align:center}.c10{margin-left:0.7pt;padding-top:0.8pt;padding-bottom:0pt;line-height:0.5499999999999999;text-align:center}.c115{margin-left:22.1pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c33{padding-top:0.3pt;padding-bottom:0pt;line-height:1.0;text-align:left;height:11pt}.c253{margin-left:4.7pt;padding-top:0pt;padding-bottom:0pt;line-height:0.6666666666666666;text-align:left}.c70{padding-top:1.4pt;padding-bottom:0pt;line-height:1.0;text-align:right;margin-right:8.2pt}.c418{margin-left:29.6pt;padding-top:0pt;padding-bottom:0pt;line-height:0.08333333333333333;text-align:left}.c73{margin-left:8.4pt;padding-top:0pt;padding-bottom:0pt;line-height:0.9125;text-align:left}.c65{padding-top:1.1pt;padding-bottom:0pt;line-height:0.7958333333333334;text-align:right;margin-right:8.2pt}.c560{padding-top:0pt;padding-bottom:0pt;line-height:0.9166666666666666;text-align:right;margin-right:13.2pt}.c153{margin-left:3.6pt;padding-top:0.8pt;padding-bottom:0pt;line-height:1.0;text-align:center}.c503{margin-left:196.4pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c618{margin-left:5.9pt;padding-top:3.4pt;padding-bottom:0pt;line-height:0.8458333333333333;text-align:left}.c625{padding-top:4.8pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:4.7pt}.c617{margin-left:2.5pt;padding-top:0.8pt;padding-bottom:0pt;line-height:0.65;text-align:center}.c193{margin-left:20.1pt;padding-top:0.3pt;padding-bottom:0pt;line-height:1.0;text-align:justify}.c623{margin-left:0.7pt;padding-top:0.5pt;padding-bottom:0pt;line-height:0.5499999999999999;text-align:center}.c611{padding-top:2.2pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:44.1pt}.c254{margin-left:13.8pt;padding-top:0.3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c547{margin-left:19.1pt;padding-top:0.2pt;padding-bottom:0pt;line-height:0.9333333333333332;text-align:justify}.c339{margin-left:8.2pt;padding-top:1.5pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c257{margin-left:41.1pt;padding-top:7.7pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c412{padding-top:0.8pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:0.9pt}.c620{padding-top:6.5pt;padding-bottom:0pt;line-height:1.0;text-align:right;margin-right:191.7pt}.c198{margin-left:19.2pt;padding-top:0pt;padding-bottom:0pt;line-height:0.6;text-align:left}.c123{margin-left:8pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:justify}.c207{margin-left:18.2pt;padding-top:2.3pt;padding-bottom:0pt;line-height:0.8291666666666666;text-align:left}.c151{margin-left:8.1pt;padding-top:5.2pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c421{padding-top:0pt;padding-bottom:0pt;line-height:0.8624999999999999;text-align:right;margin-right:8.2pt}.c191{margin-left:29.2pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c154{margin-left:18pt;padding-top:0pt;padding-bottom:0pt;line-height:0.5458333333333333;text-align:left}.c292{margin-left:3.6pt;padding-top:0.9pt;padding-bottom:0pt;line-height:0.6375000000000001;text-align:center}.c45{margin-left:3.6pt;padding-top:0.8pt;padding-bottom:0pt;line-height:0.65;text-align:center}.c629{padding-top:1.1pt;padding-bottom:0pt;line-height:1.0;text-align:right;margin-right:8.8pt}.c74{padding-top:0.1pt;padding-bottom:0pt;line-height:1.0;text-align:left;height:11pt}.c567{padding-top:0pt;padding-bottom:0pt;line-height:0.8791666666666668;text-align:right;margin-right:10.1pt}.c225{padding-top:0.2pt;padding-bottom:0pt;line-height:1.0;text-align:right;margin-right:13.2pt}.c266{margin-left:0.4pt;padding-top:3.4pt;padding-bottom:0pt;line-height:0.7875;text-align:left}.c579{margin-left:6.8pt;padding-top:0pt;padding-bottom:0pt;line-height:0.6124999999999999;text-align:left}.c119{margin-left:16pt;padding-top:0.8pt;padding-bottom:0pt;line-height:0.5499999999999999;text-align:left}.c385{margin-left:5.9pt;padding-top:3.2pt;padding-bottom:0pt;line-height:0.85;text-align:left}.c49{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Times New Roman"}.c347{margin-left:8.5pt;padding-top:3.4pt;padding-bottom:0pt;line-height:0.7875;text-align:left}.c245{margin-left:33.5pt;padding-top:2.5pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c329{margin-left:18.2pt;padding-top:1pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c264{margin-left:5.6pt;padding-top:0pt;padding-bottom:0pt;line-height:0.6875;text-align:left}.c346{margin-left:7.4pt;padding-top:1.9pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c296{color:#000000;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Verdana"}.c41{margin-left:8.1pt;padding-top:0pt;padding-bottom:0pt;line-height:0.9541666666666666;text-align:justify}.c332{margin-left:0.4pt;padding-top:0.3pt;padding-bottom:0pt;line-height:1.0;text-align:center}.c610{margin-left:7.3pt;padding-top:0pt;padding-bottom:0pt;line-height:0.9458333333333333;text-align:left}.c188{margin-left:28pt;padding-top:0.6pt;padding-bottom:0pt;line-height:1.0;text-align:justify}.c352{margin-left:8.1pt;padding-top:0.6pt;padding-bottom:0pt;line-height:1.0583333333333333;text-align:left}.c606{margin-left:0.4pt;padding-top:1.6pt;padding-bottom:0pt;line-height:0.7333333333333334;text-align:center}.c491{margin-left:0.4pt;padding-top:1.9pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c268{margin-left:5.3pt;padding-top:0.3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c393{margin-left:2pt;padding-top:0pt;padding-bottom:0pt;line-height:0.8083333333333332;text-align:center}.c426{margin-left:19.7pt;padding-top:0.4pt;padding-bottom:0pt;line-height:1.0;text-align:justify}.c543{margin-left:14.5pt;padding-top:0pt;padding-bottom:0pt;line-height:0.8333333333333334;text-align:left}.c147{margin-left:28pt;padding-top:0.7pt;padding-bottom:0pt;line-height:1.0;text-align:justify}.c408{margin-left:1.1pt;padding-top:0pt;padding-bottom:0pt;line-height:0.5916666666666667;text-align:left}.c357{margin-left:2.5pt;padding-top:0.9pt;padding-bottom:0pt;line-height:0.65;text-align:center}.c570{margin-left:20.4pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c406{margin-left:8.5pt;padding-top:1.9pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c448{margin-left:6.6pt;padding-top:0pt;padding-bottom:0pt;line-height:0.8333333333333334;text-align:left}.c466{margin-left:9.9pt;padding-top:2pt;padding-bottom:0pt;line-height:1.0;text-align:center}.c605{margin-left:7.4pt;padding-top:1.5pt;padding-bottom:0pt;line-height:0.7541666666666668;text-align:left}.c342{margin-left:8.2pt;padding-top:3.4pt;padding-bottom:0pt;line-height:0.7875;text-align:left}.c175{margin-left:5.5pt;padding-top:0pt;padding-bottom:0pt;line-height:0.6875;text-align:left}.c478{padding-top:0pt;padding-bottom:0pt;line-height:0.8624999999999999;text-align:right;margin-right:10.1pt}.c21{margin-left:33.5pt;padding-top:0.3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c277{margin-left:8.1pt;padding-top:0pt;padding-bottom:0pt;line-height:0.9874999999999999;text-align:left}.c442{padding-top:0.8pt;padding-bottom:0pt;line-height:0.65;text-align:center;margin-right:1.4pt}.c152{margin-left:8.5pt;padding-top:3.4pt;padding-bottom:0pt;line-height:0.8458333333333333;text-align:left}.c236{margin-left:2.5pt;padding-top:0pt;padding-bottom:0pt;line-height:0.9458333333333333;text-align:left}.c181{padding-top:0pt;padding-bottom:0pt;line-height:0.9041666666666667;text-align:right;margin-right:13.2pt}.c178{margin-left:14.4pt;padding-top:0pt;padding-bottom:0pt;line-height:0.9;text-align:left}.c287{margin-left:102.3pt;padding-top:0pt;padding-bottom:0pt;line-height:0.08333333333333333;text-align:left}.c114{margin-left:14.2pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c402{padding-top:0.3pt;padding-bottom:0.1pt;line-height:1.0;text-align:left;height:11pt}.c51{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left;height:11pt}.c601{margin-left:73.5pt;padding-top:0pt;padding-bottom:0pt;line-height:0.09999999999999999;text-align:left}.c38{padding-top:0.2pt;padding-bottom:0pt;line-height:1.0;text-align:left;height:11pt}.c250{margin-left:5.7pt;padding-top:0pt;padding-bottom:0pt;line-height:0.6875;text-align:left}.c200{margin-left:22.6pt;padding-top:0pt;padding-bottom:0pt;line-height:0.6875;text-align:left}.c525{padding-top:0.7pt;padding-bottom:0pt;line-height:1.0;text-align:right;margin-right:3.5pt}.c12{margin-left:1.8pt;padding-top:0.3pt;padding-bottom:0pt;line-height:0.6375000000000001;text-align:center}.c275{margin-left:2.5pt;padding-top:0pt;padding-bottom:0pt;line-height:0.8541666666666666;text-align:left}.c549{padding-top:1.6pt;padding-bottom:0pt;line-height:0.8208333333333333;text-align:right;margin-right:8.2pt}.c490{margin-left:14pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c556{margin-left:3.6pt;padding-top:0.9pt;padding-bottom:0pt;line-height:0.65;text-align:center}.c523{margin-left:5.3pt;padding-top:0.7pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c580{margin-left:8pt;padding-top:7.5pt;padding-bottom:0pt;line-height:1.0;text-align:justify}.c23{margin-left:8.1pt;padding-top:5.3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c470{margin-left:27.7pt;padding-top:0.6pt;padding-bottom:0pt;line-height:1.0;text-align:justify}.c118{margin-left:0.5pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:center}.c383{margin-left:13.8pt;padding-top:0pt;padding-bottom:0pt;line-height:0.8958333333333334;text-align:left}.c230{padding-top:0.5pt;padding-bottom:0pt;line-height:1.0;text-align:left;height:11pt}.c85{margin-left:33.5pt;padding-top:0.9pt;padding-bottom:0pt;line-height:0.7833333333333333;text-align:left}.c223{margin-left:69.7pt;padding-top:0pt;padding-bottom:0pt;line-height:0.6625;text-align:left}.c168{margin-left:7.4pt;padding-top:3.4pt;padding-bottom:0pt;line-height:0.7875;text-align:left}.c11{margin-left:4.7pt;padding-top:0.3pt;padding-bottom:0pt;line-height:0.6666666666666666;text-align:left}.c458{margin-left:7.8pt;padding-top:0pt;padding-bottom:0pt;line-height:0.775;text-align:left}.c83{margin-left:8.5pt;padding-top:1.5pt;padding-bottom:0pt;line-height:0.7875;text-align:left}.c261{margin-left:8.1pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c325{margin-left:6.5pt;padding-top:0pt;padding-bottom:0pt;line-height:0.8333333333333334;text-align:left}.c196{color:#000000;text-decoration:none;vertical-align:baseline;font-family:"Times New Roman";font-style:normal}.c246{margin-left:0.4pt;padding-top:0.9pt;padding-bottom:0pt;line-height:0.7833333333333333;text-align:center}.c431{padding-top:1.8pt;padding-bottom:0pt;line-height:1.0;text-align:right;margin-right:13.2pt}.c324{padding-top:0pt;padding-bottom:0pt;line-height:0.8708333333333332;text-align:right;margin-right:10.1pt}.c512{padding-top:3.2pt;padding-bottom:0pt;line-height:0.7791666666666667;text-align:right;margin-right:8.2pt}.c57{margin-left:7.4pt;padding-top:1.5pt;padding-bottom:0pt;line-height:0.7875;text-align:left}.c216{margin-left:13.5pt;padding-top:0.5pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c619{padding-top:1.4pt;padding-bottom:0pt;line-height:0.7958333333333334;text-align:right;margin-right:8.2pt}.c140{margin-left:20.9pt;padding-top:0pt;padding-bottom:0pt;line-height:0.06;text-align:left}.c43{padding-top:0.8pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:1.4pt}.c37{margin-left:13.5pt;padding-top:0.5pt;padding-bottom:0pt;line-height:0.9249999999999999;text-align:left}.c25{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left;height:11pt}.c103{margin-left:14.7pt;padding-top:0pt;padding-bottom:0pt;line-height:0.5875;text-align:left}.c159{margin-left:2.5pt;padding-top:0.8pt;padding-bottom:0pt;line-height:1.0;text-align:center}.c517{margin-left:20.1pt;padding-top:0pt;padding-bottom:0pt;line-height:0.65;text-align:left}.c344{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:center;height:11pt}.c476{padding-top:0pt;padding-bottom:0pt;line-height:0.9125;text-align:right;margin-right:13.2pt}.c68{margin-left:2.5pt;padding-top:0pt;padding-bottom:0pt;line-height:0.9041666666666667;text-align:left}.c400{padding-top:0.6pt;padding-bottom:0pt;line-height:1.0;text-align:right;margin-right:1.9pt}.c313{margin-left:8.5pt;padding-top:1.5pt;padding-bottom:0pt;line-height:0.7541666666666668;text-align:left}.c271{margin-left:16pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c139{margin-left:0.4pt;padding-top:1.5pt;padding-bottom:0pt;line-height:0.7541666666666668;text-align:left}.c308{margin-left:9.5pt;padding-top:0pt;padding-bottom:0pt;line-height:0.8708333333333332;text-align:left}.c434{padding-top:0pt;padding-bottom:0pt;line-height:0.8791666666666668;text-align:right;margin-right:13.2pt}.c180{margin-left:35.7pt;padding-top:1.6pt;padding-bottom:0pt;line-height:0.7333333333333334;text-align:left}.c279{margin-left:42.4pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c167{margin-left:22.5pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c231{padding-top:0.8pt;padding-bottom:0pt;line-height:1.0;text-align:center;margin-right:1.8pt}.c454{margin-left:5.9pt;padding-top:3.4pt;padding-bottom:0pt;line-height:0.8416666666666667;text-align:left}.c111{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:right;margin-right:191.7pt}.c505{padding-top:0pt;padding-bottom:0pt;line-height:0.8708333333333332;text-align:right;margin-right:8.2pt}.c519{margin-left:14.1pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c327{padding-top:0pt;padding-bottom:0pt;line-height:0.8791666666666668;text-align:left}.c190{margin-left:0.3pt;border-spacing:0;border-collapse:collapse;margin-right:auto}.c211{color:#000000;text-decoration:none;vertical-align:super;font-size:11.7pt}.c541{padding-top:0.5pt;padding-bottom:2pt;line-height:1.0;text-align:left}.c537{padding-top:0.4pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c493{margin-left:315.1pt;border-spacing:0;border-collapse:collapse;margin-right:auto}.c479{padding-top:0pt;padding-bottom:0pt;line-height:0.9166666666666666;text-align:left}.c437{margin-left:9.3pt;border-spacing:0;border-collapse:collapse;margin-right:auto}.c381{padding-top:0.5pt;padding-bottom:0pt;line-height:0.9249999999999999;text-align:left}.c281{padding-top:0pt;padding-bottom:0pt;line-height:0.7916666666666666;text-align:left}.c328{padding-top:3.4pt;padding-bottom:0pt;line-height:0.7875;text-align:center}.c403{margin-left:9.2pt;border-spacing:0;border-collapse:collapse;margin-right:auto}.c562{padding-top:0.3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c419{padding-top:0.5pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c444{margin-left:20.4pt;border-spacing:0;border-collapse:collapse;margin-right:auto}.c542{padding-top:1.8pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c202{padding-top:0pt;padding-bottom:0pt;line-height:0.9041666666666667;text-align:left}.c127{padding-top:0.2pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c179{padding-top:1.5pt;padding-bottom:0pt;line-height:0.7875;text-align:center}.c91{color:#000000;text-decoration:none;vertical-align:baseline;font-family:"Calibri"}.c54{font-size:12pt;font-family:"Verdana";font-weight:400}.c48{font-size:10pt;font-family:"Calibri";font-weight:400}.c515{color:#000000;text-decoration:none;vertical-align:baseline}.c633{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c473{color:#000000;text-decoration:none;vertical-align:super}.c155{font-size:5pt;font-style:normal}.c5{color:inherit;text-decoration:inherit}.c298{font-size:1pt;font-style:normal}.c64{font-family:"Calibri";font-weight:400}.c318{font-size:15pt;font-style:normal}.c221{font-size:2pt;font-style:normal}.c30{font-size:10pt;font-weight:700}.c124{color:#00ff00;font-size:10pt}.c194{font-size:10.5pt;font-style:normal}.c163{font-size:21pt;font-style:normal}.c422{font-size:5.5pt;font-style:normal}.c276{font-size:8.5pt;font-style:normal}.c423{font-size:14.5pt;font-style:normal}.c99{font-size:6pt;font-style:normal}.c52{padding:0;margin:0}.c468{font-size:2.5pt}.c573{font-size:18.5pt}.c42{font-style:italic}.c53{height:13.9pt}.c452{height:14.8pt}.c511{font-size:15.5pt}.c375{height:16.6pt}.c255{height:8.9pt}.c241{height:9.2pt}.c229{height:10.6pt}.c144{font-size:10pt}.c447{height:11.7pt}.c134{font-family:"Calibri"}.c420{height:16.4pt}.c482{height:18.2pt}.c315{height:13.4pt}.c430{height:12.5pt}.c240{margin-left:22.5pt}.c593{font-size:7pt}.c158{height:14.4pt}.c55{font-weight:700}.c572{font-size:13.5pt}.c500{font-size:7.5pt}.c428{font-style:normal}.c97{font-weight:400}.c58{height:12.8pt}.c487{margin-left:13.5pt}.c251{height:11.5pt}.c76{font-size:12pt}.c2{height:12.4pt}.c317{height:11.3pt}.c63{height:11.9pt}.c156{text-indent:11.9pt}.c290{height:9.7pt}.c274{height:13.5pt}.c177{margin-left:8.4pt}.c569{font-size:20.5pt}.c531{font-size:6.5pt}.c27{font-size:9pt}.c319{height:11.2pt}.c563{height:8.1pt}.c522{height:8.3pt}.c489{height:11pt}.c16{height:12.9pt}.c378{font-family:"Verdana"}.c506{height:13.6pt}.c334{height:13.8pt}.c609{height:34pt}.c566{height:13.2pt}.c233{height:11.6pt}.c14{height:9.6pt}.title{padding-top:0.1pt;color:#000000;font-size:20pt;padding-bottom:0pt;font-family:"Times New Roman";line-height:1.0;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.0;page-break-after:avoid;font-style:italic;text-align:left}li{color:#000000;font-size:11pt;font-family:"Times New Roman"}p{margin:0;color:#000000;font-size:11pt;font-family:"Times New Roman"}h1{padding-top:0pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:0pt;font-family:"Times New Roman";line-height:1.0;text-align:left}h2{padding-top:0pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:0pt;font-family:"Times New Roman";line-height:1.0;text-align:left}h3{padding-top:0pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:0pt;font-family:"Times New Roman";line-height:1.0;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;text-align:left}</style></head><body class="c633 doc-content"><p class="c25"><span class="c3"></span></p><p class="c74"><span class="c3"></span></p><p class="c631"><span class="c55 c149">Rethinking Atrous Convolution for Semantic Image Segmentation</span></p><p class="c74"><span class="c196 c55 c569"></span></p><p class="c278"><span class="c34">Liang-Chieh Chen&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;George Papandreou&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Florian Schroff&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hartwig Adam Google Inc.</span></p><p class="c235"><span class="c42 c54">{</span><span class="c76">lcchen, gpapan, fschroff, hadam</span><span class="c54 c42">}</span><span class="c34">@google.com</span></p><p class="c25"><span class="c3"></span></p><p class="c74"><span class="c9"></span></p><h1 class="c307"><span class="c196 c55 c76">Abstract</span></h1><p class="c600"><span class="c26">In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter&rsquo;s field-of-view as well as control the resolution of feature responses computed by Deep Convolu- tional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional fea- tures at multiple scales, with image-level features encoding global context and further boost performance. We also elab- orate on implementation details and share our experience on training our system. The proposed &lsquo;DeepLabv3&rsquo; system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.</span></p><p class="c25"><span class="c49 c42 c76"></span></p><ol class="c52 lst-kix_list_4-0 start" start="1"><li class="c361 li-bullet-0"><h1 style="display:inline"><span class="c196 c55 c76">Introduction</span></h1></li></ol><p class="c492"><span class="c3">For the task of semantic segmentation [</span><span class="c8"><a class="c5" href="#h.2grqrue">20</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.2afmg28">63</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.3o7alnk">14</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.3x8tuzt">97</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.3whwml4">7</a></span><span class="c3">], we consider two challenges in applying Deep Convolutional Neural Networks (DCNNs) [</span><span class="c8"><a class="c5" href="#h.kgcv8k">50</a></span><span class="c3">]. The first one is the reduced feature resolution caused by consecutive pooling operations or convolution striding, which allows DCNNs to learn in- creasingly abstract feature representations. However, this invariance to local image transformation may impede dense prediction tasks, where detailed spatial information is de- sired. To overcome this problem, we advocate the use of atrous convolution [</span><span class="c8"><a class="c5" href="#h.206ipza">36</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.19c6y18">26</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1gf8i83">74</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1opuj5n">66</a></span><span class="c3">], which has been shown to be effective for semantic image segmentation [</span><span class="c8"><a class="c5" href="#h.3as4poj">10</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.zu0gcz">90</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1pxezwc">11</a></span><span class="c3">]. Atrous convolution, also known as dilated convolution, al- lows us to repurpose ImageNet [</span><span class="c8"><a class="c5" href="#h.haapch">72</a></span><span class="c3">] pretrained networks to extract denser feature maps by removing the downsam- pling operations from the last few layers and upsampling the corresponding filter kernels, equivalent to inserting holes (&lsquo;trous&rsquo; in French) between filter weights. With atrous convo- lution, one is able to control the resolution at which feature</span></p><hr style="page-break-before:always;display:none;"><p class="c230"><span class="c49 c422"></span></p><p class="c490"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 299.07px; height: 141.87px;"><img alt="" src="images/image17.png" style="width: 299.07px; height: 141.87px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 18.53px; height: 18.53px;"><img alt="" src="images/image31.png" style="width: 18.53px; height: 18.53px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c224" id="h.gjdgxs"><span class="c27">Figure 1. Atrous convolution with kernel size </span><span class="c64 c27">3 3 </span><span class="c27">and different rates. Standard convolution corresponds to atrous convolution with </span><span class="c64 c42 c27">rate </span><span class="c64 c27">= 1</span><span class="c9">. Employing large value of atrous rate enlarges the model&rsquo;s field-of-view, enabling object encoding at multiple scales.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 10.60px; height: 21.80px;"><img alt="" src="images/image4.png" style="width: 10.60px; height: 21.80px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c25"><span class="c13"></span></p><p class="c415"><span class="c3">responses are computed within DCNNs without requiring learning extra parameters.</span></p><p class="c156 c591"><span class="c3">Another difficulty comes from the existence of objects at multiple scales. Several methods have been proposed to handle the problem and we mainly consider four categories in this work, as illustrated in Fig. </span><span class="c15"><a class="c5" href="#h.30j0zll">2</a></span><span class="c3">. First, the DCNN is applied to an image pyramid to extract features for each scale input [</span><span class="c8"><a class="c5" href="#h.3fwokq0">22</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.41mghml">19</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1302m92">69</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.2iq8gzs">55</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.49x2ik5">12</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1pxezwc">11</a></span><span class="c3">] where objects at different scales become prominent at different feature maps. Sec- ond, the encoder-decoder structure [</span><span class="c8"><a class="c5" href="#h.4i7ojhp">3</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.2250f4o">71</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.2u6wntf">25</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.2iq8gzs">54</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.3mzq4wv">70</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.2nusc19">68</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1egqt2p">39</a></span><span class="c3">] exploits multi-scale features from the encoder part and re- covers the spatial resolution from the decoder part. Third, extra modules are cascaded on top of the original network for capturing long range information. In particular, DenseCRF</span></p><ol class="c52 lst-kix_list_3-0 start" start="45"><li class="c89"><span class="c3">is employed to encode pixel-level pairwise similarities [</span><span class="c8"><a class="c5" href="#h.3as4poj">10</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1d96cc0">96</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.2iq8gzs">55</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.319y80a">73</a></span><span class="c3">], while [</span><span class="c8"><a class="c5" href="#h.4h042r0">59</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.zu0gcz">90</a></span><span class="c3">] develop several extra convo- lutional layers in cascade to gradually capture long range context. Fourth, spatial pyramid pooling [</span><span class="c8"><a class="c5" href="#h.1pxezwc">11</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1d96cc0">95</a></span><span class="c3">] probes an incoming feature map with filters or pooling operations at multiple rates and multiple effective field-of-views, thus capturing objects at multiple scales.</span></li></ol><p class="c462"><span class="c3">In this work, we revisit applying atrous convolution, which allows us to effectively enlarge the field of view of filters to incorporate multi-scale context, in the framework of both cascaded modules and spatial pyramid pooling. In par- ticular, our proposed module consists of atrous convolution with various rates and batch normalization layers which we</span></p><p class="c562"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 33.20px; height: 458.47px;"><img alt="" src="images/image50.png" style="width: 33.20px; height: 458.47px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c625"><span class="c3">1</span></p><p class="c114"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 130.00px; height: 144.73px;"><img alt="" src="images/image18.png" style="width: 130.00px; height: 144.73px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 174.27px; height: 144.73px;"><img alt="" src="images/image15.png" style="width: 174.27px; height: 144.73px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 174.27px; height: 144.73px;"><img alt="" src="images/image14.png" style="width: 174.27px; height: 144.73px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 125.53px; height: 144.73px;"><img alt="" src="images/image16.png" style="width: 125.53px; height: 144.73px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 7.13px; height: 7.20px;"><img alt="" src="images/image33.png" style="width: 7.13px; height: 7.20px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 18.60px; height: 9.87px;"><img alt="" src="images/image32.png" style="width: 18.60px; height: 9.87px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ol class="c52 lst-kix_list_3-1 start" start="1"><li class="c456 li-bullet-1" id="h.30j0zll"><span class="c9">Image Pyramid&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(b) Encoder-Decoder&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(c) Deeper w. Atrous Convolution&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(d) Spatial Pyramid Pooling Figure 2. Alternative architectures to capture multi-scale context.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 120.87px; height: 7.73px;"><img alt="" src="images/image37.png" style="width: 120.87px; height: 7.73px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 71.53px; height: 82.73px;"><img alt="" src="images/image46.png" style="width: 71.53px; height: 82.73px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ol><p class="c230"><span class="c6"></span></p><p class="c550"><span class="c3">found important to be trained as well. We experiment with laying out the modules in cascade or in parallel (specifically, Atrous Spatial Pyramid Pooling (ASPP) method [</span><span class="c8"><a class="c5" href="#h.1pxezwc">11</a></span><span class="c3">]). We discuss an important practical issue when applying a </span><span class="c19">3 3 </span><span class="c3">atrous convolution with an extremely large rate, which fails to capture long range information due to image boundary effects, effectively simply degenerating to </span><span class="c19">1 1 </span><span class="c3">convolu- tion, and propose to incorporate image-level features into the ASPP module. Furthermore, we elaborate on imple- mentation details and share experience on training the pro- posed models, including a simple yet effective bootstrapping method for handling rare and finely annotated objects. In the end, our proposed model, &lsquo;DeepLabv3&rsquo; improves over our previous works [</span><span class="c8"><a class="c5" href="#h.3as4poj">10</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1pxezwc">11</a></span><span class="c3">] and attains performance of 85.7% on the PASCAL VOC 2012 </span><span class="c26">test </span><span class="c3">set without DenseCRF post- processing.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 11.33px; height: 24.07px;"><img alt="" src="images/image11.png" style="width: 11.33px; height: 24.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 11.33px; height: 24.07px;"><img alt="" src="images/image11.png" style="width: 11.33px; height: 24.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c74"><span class="c3"></span></p><ol class="c52 lst-kix_list_4-0" start="2"><li class="c362 li-bullet-0"><h1 style="display:inline"><span class="c196 c55 c76">Related Work</span></h1></li></ol><p class="c156 c463"><span class="c3">It has been shown that global features or contextual in- teractions [</span><span class="c8"><a class="c5" href="#h.2lwamvv">33</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.2fk6b3p">76</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.3cqmetx">43</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.3q5sasy">48</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.3tbugp1">27</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.2koq656">89</a></span><span class="c3">] are beneficial in cor- rectly classifying pixels for semantic segmentation. In this work, we discuss four types of Fully Convolutional Networks (FCNs) [</span><span class="c8"><a class="c5" href="#h.1gf8i83">74</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.2w5ecyt">60</a></span><span class="c3">] (see Fig. </span><span class="c15"><a class="c5" href="#h.30j0zll">2</a></span><span class="c15">&nbsp;</span><span class="c3">for illustration) that exploit context information for semantic segmentation [</span><span class="c8"><a class="c5" href="#h.37m2jsg">30</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.23ckvvd">15</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.3vac5uf">62</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.qsh70q">9</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1d96cc0">96</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.2iq8gzs">55</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.39kk8xu">65</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.319y80a">73</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1ljsd9k">87</a></span><span class="c3">].</span></p><p class="c162"><span class="c0">Image pyramid: </span><span class="c3">The same model, typically with shared weights, is applied to multi-scale inputs. Feature responses from the small scale inputs encode the long-range context, while the large scale inputs preserve the small object details. Typical examples include Farabet </span><span class="c26">et al</span><span class="c3">. [</span><span class="c8"><a class="c5" href="#h.3fwokq0">22</a></span><span class="c3">] who transform the input image through a Laplacian pyramid, feed each scale input to a DCNN and merge the feature maps from all the scales. [</span><span class="c8"><a class="c5" href="#h.41mghml">19</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1302m92">69</a></span><span class="c3">] apply multi-scale inputs sequentially from coarse-to-fine, while [</span><span class="c8"><a class="c5" href="#h.2iq8gzs">55</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.49x2ik5">12</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1pxezwc">11</a></span><span class="c3">] directly resize the input for several scales and fuse the features from all the scales. The main drawback of this type of models is that it does not scale well for larger/deeper DCNNs (</span><span class="c26">e.g</span><span class="c3">., networks like [</span><span class="c8"><a class="c5" href="#h.46r0co2">32</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.3jtnz0s">91</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.36ei31r">86</a></span><span class="c3">]) due to limited GPU memory and thus it is usually applied during the inference stage [</span><span class="c8"><a class="c5" href="#h.ihv636">16</a></span><span class="c3">].</span></p><p class="c193"><span class="c30">Encoder-decoder: </span><span class="c3">This model consists of two parts: (a)</span></p><hr style="page-break-before:always;display:none;"><p class="c133"><span class="c3">the encoder where the spatial dimension of feature maps is gradually reduced and thus longer range information is more easily captured in the deeper encoder output, and (b) the decoder where object details and spatial dimension are gradually recovered. For example, [</span><span class="c8"><a class="c5" href="#h.2w5ecyt">60</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.pkwqa1">64</a></span><span class="c3">] employ deconvo- lution [</span><span class="c8"><a class="c5" href="#h.1yyy98l">92</a></span><span class="c3">] to learn the upsampling of low resolution feature responses. SegNet [</span><span class="c8"><a class="c5" href="#h.4i7ojhp">3</a></span><span class="c3">] reuses the pooling indices from the encoder and learn extra convolutional layers to densify the feature responses, while U-Net [</span><span class="c8"><a class="c5" href="#h.2250f4o">71</a></span><span class="c3">] adds skip connections from the encoder features to the corresponding decoder acti- vations, and [</span><span class="c8"><a class="c5" href="#h.2u6wntf">25</a></span><span class="c3">] employs a Laplacian pyramid reconstruc- tion network. More recently, RefineNet [</span><span class="c8"><a class="c5" href="#h.2iq8gzs">54</a></span><span class="c3">] and [</span><span class="c8"><a class="c5" href="#h.3mzq4wv">70</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.2nusc19">68</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1egqt2p">39</a></span><span class="c3">] have demonstrated the effectiveness of models based on encoder-decoder structure on several semantic segmentation benchmarks. This type of model is also explored in the context of object detection [</span><span class="c8"><a class="c5" href="#h.xvir7l">56</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.upglbi">77</a></span><span class="c3">].</span></p><p class="c156 c157"><span class="c0">Context module: </span><span class="c3">This model contains extra modules laid out in cascade to encode long-range context. One ef- fective method is to incorporate DenseCRF [</span><span class="c8"><a class="c5" href="#h.4bvk7pj">45</a></span><span class="c3">] (with effi- cient high-dimensional filtering algorithms [</span><span class="c8"><a class="c5" href="#h.3j2qqm3">2</a></span><span class="c3">]) to DCNNs [</span><span class="c8"><a class="c5" href="#h.3as4poj">10</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1pxezwc">11</a></span><span class="c3">]. Furthermore, [</span><span class="c8"><a class="c5" href="#h.1d96cc0">96</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.2iq8gzs">55</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.319y80a">73</a></span><span class="c3">] propose to jointly train both the CRF and DCNN components, while [</span><span class="c8"><a class="c5" href="#h.4h042r0">59</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.zu0gcz">90</a></span><span class="c3">] em- ploy several extra convolutional layers on top of the belief maps of DCNNs (belief maps are the final DCNN feature maps that contain output channels equal to the number of predicted classes) to capture context information. Recently,</span></p><p class="c272"><span class="c3">[</span><span class="c8"><a class="c5" href="#h.2dlolyb">41</a></span><span class="c3">] proposes to learn a general and sparse high-dimensional convolution (bilateral convolution), and [</span><span class="c8"><a class="c5" href="#h.184mhaj">82</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.2bn6wsx">8</a></span><span class="c3">] combine Gaussian Conditional Random Fields and DCNNs for se- mantic segmentation.</span></p><p class="c359"><span class="c0">Spatial pyramid pooling: </span><span class="c3">This model employs spatial pyramid pooling [</span><span class="c8"><a class="c5" href="#h.28h4qwu">28</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.25b2l0r">49</a></span><span class="c3">] to capture context at several ranges. The image-level features are exploited in ParseNet [</span><span class="c8"><a class="c5" href="#h.1x0gk37">58</a></span><span class="c3">] for global context information. DeepLabv2 [</span><span class="c8"><a class="c5" href="#h.1pxezwc">11</a></span><span class="c3">] proposes atrous spatial pyramid pooling (ASPP), where parallel atrous con- volution layers with different rates capture multi-scale infor- mation. Recently, Pyramid Scene Parsing Net (PSP) [</span><span class="c8"><a class="c5" href="#h.1d96cc0">95</a></span><span class="c3">] performs spatial pooling at several grid scales and demon- strates outstanding performance on several semantic segmen- tation benchmarks. There are other methods based on LSTM</span></p><p class="c429"><span class="c3">[</span><span class="c8"><a class="c5" href="#h.3l18frh">35</a></span><span class="c3">] to aggregate global context [</span><span class="c8"><a class="c5" href="#h.43ky6rz">53</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1ci93xb">6</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.45jfvxd">88</a></span><span class="c3">]. Spatial pyramid pooling has also been applied in object detection [</span><span class="c8"><a class="c5" href="#h.1mrcu09">31</a></span><span class="c3">].</span></p><p class="c32"><span class="c3">In this work, we mainly explore atrous convolution [</span><span class="c8"><a class="c5" href="#h.206ipza">36</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.19c6y18">26</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1gf8i83">74</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1opuj5n">66</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.3as4poj">10</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.zu0gcz">90</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1pxezwc">11</a></span><span class="c3">] as a </span><span class="c0">context module </span><span class="c3">and tool for </span><span class="c0">spatial pyramid pooling</span><span class="c3">. Our proposed framework is general in the sense that it could be applied to any network. To be concrete, we duplicate several copies of the original last block in ResNet [</span><span class="c8"><a class="c5" href="#h.46r0co2">32</a></span><span class="c3">] and arrange them in cascade, and also revisit the ASPP module [</span><span class="c8"><a class="c5" href="#h.1pxezwc">11</a></span><span class="c3">] which contains several atrous convolutions in parallel. Note that our cascaded mod- ules are applied directly on the feature maps instead of belief maps. For the proposed modules, we experimentally find it important to train with batch normalization [</span><span class="c8"><a class="c5" href="#h.2zbgiuw">38</a></span><span class="c3">]. To further capture global context, we propose to augment ASPP with image-level features, similar to [</span><span class="c8"><a class="c5" href="#h.1x0gk37">58</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1d96cc0">95</a></span><span class="c3">].</span></p><p class="c109 c156"><span class="c0">Atrous convolution: </span><span class="c3">Models based on atrous convolu- tion have been actively explored for semantic segmentation. For example, [</span><span class="c8"><a class="c5" href="#h.meukdy">85</a></span><span class="c3">] experiments with the effect of modify- ing atrous rates for capturing long-range information, [</span><span class="c8"><a class="c5" href="#h.279ka65">84</a></span><span class="c3">] adopts hybrid atrous rates within the last two blocks of ResNet, while [</span><span class="c8"><a class="c5" href="#h.1hmsyys">18</a></span><span class="c3">] further proposes to learn the deformable convolution which samples the input features with learned offset, generalizing atrous convolution. To further improve the segmentation model accuracy, [</span><span class="c8"><a class="c5" href="#h.3s49zyc">83</a></span><span class="c3">] exploits image cap- tions, [</span><span class="c8"><a class="c5" href="#h.3ygebqi">40</a></span><span class="c3">] utilizes video motion, and [</span><span class="c8"><a class="c5" href="#h.1rvwp1q">44</a></span><span class="c3">] incorporates depth information. Besides, atrous convolution has been applied to object detection by [</span><span class="c8"><a class="c5" href="#h.1opuj5n">66</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.32hioqz">17</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.4k668n3">37</a></span><span class="c3">].</span></p><ol class="c52 lst-kix_list_4-0" start="3"><li class="c581 li-bullet-0"><h1 style="display:inline"><span class="c196 c55 c76">Methods</span></h1></li></ol><p class="c558"><span class="c3">In this section, we review how atrous convolution is ap- plied to extract dense features for semantic segmentation. We then discuss the proposed modules with atrous convolu- tion modules employed in cascade or in parallel.</span></p><ol class="c52 lst-kix_list_4-1 start" start="1"><li class="c540 li-bullet-2"><h2 style="display:inline"><span class="c55 c166">Atrous Convolution for Dense Feature Extrac- tion</span></h2></li></ol><p class="c156 c238"><span class="c3">Deep Convolutional Neural Networks (DCNNs) [</span><span class="c8"><a class="c5" href="#h.kgcv8k">50</a></span><span class="c3">] de- ployed in fully convolutional fashion [</span><span class="c8"><a class="c5" href="#h.1gf8i83">74</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.2w5ecyt">60</a></span><span class="c3">] have shown to be effective for the task of semantic segmentation. However, the repeated combination of max-pooling and striding at consecutive layers of these networks significantly reduces the spatial resolution of the resulting feature maps, typically by a factor of 32 across each direction in recent DCNNs [</span><span class="c8"><a class="c5" href="#h.1664s55">47</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.3ep43zb">78</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.46r0co2">32</a></span><span class="c3">]. Deconvolutional layers (or transposed convolu-</span></p><p class="c109"><span class="c3">tion) [</span><span class="c8"><a class="c5" href="#h.1yyy98l">92</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.2w5ecyt">60</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.pkwqa1">64</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.4i7ojhp">3</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.2250f4o">71</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.2nusc19">68</a></span><span class="c3">] have been employed to recover the spatial resolution. Instead, we advocate the use of &lsquo;atrous convolution&rsquo;, originally developed for the efficient computa- tion of the undecimated wavelet transform in the &ldquo;algorithme a` trous&rdquo; scheme of [</span><span class="c8"><a class="c5" href="#h.206ipza">36</a></span><span class="c3">] and used before in the DCNN context by [</span><span class="c8"><a class="c5" href="#h.19c6y18">26</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1gf8i83">74</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1opuj5n">66</a></span><span class="c3">].</span></p><p class="c156 c559"><span class="c3">Consider two-dimensional signals, for each location </span><span class="c91 c30 c42">i </span><span class="c3">on the output </span><span class="c91 c30 c42">y </span><span class="c3">and a filter </span><span class="c91 c30 c42">w</span><span class="c3">, atrous convolution is applied over the input feature map </span><span class="c30 c42 c91">x</span><span class="c3">:</span></p><hr style="page-break-before:always;display:none;"><p class="c33"><span class="c49 c163"></span></p><p class="c101"><span class="c30 c42 c134">y</span><span class="c48">[</span><span class="c30 c42 c134">i</span><span class="c48">] =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c30 c42 c134">x</span><span class="c48">[</span><span class="c30 c42 c134">i </span><span class="c48">+ </span><span class="c48 c42">r </span><span class="c42 c144 c97 c378">&middot; </span><span class="c30 c42 c134">k</span><span class="c48">]</span><span class="c30 c42 c134">w</span><span class="c48">[</span><span class="c30 c42 c134">k</span><span class="c48">]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c3">(1)</span></p><p class="c611"><span class="c42 c55 c296">k</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 20.20px; height: 50.60px;"><img alt="" src="images/image19.png" style="width: 20.20px; height: 50.60px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c25"><span class="c296 c42 c55"></span></p><p class="c499"><span class="c3">where the atrous rate </span><span class="c26">r </span><span class="c3">corresponds to the stride with which we sample the input signal, which is equivalent to convolving the input </span><span class="c91 c30 c42">x </span><span class="c3">with upsampled filters produced by inserting </span><span class="c1">r </span><span class="c19">1 </span><span class="c3">zeros between two consecutive filter values along each spatial dimension (hence the name atrous convolution where the French word trous means holes in English). Standard convolution is a special case for rate </span><span class="c1">r </span><span class="c19">= 1</span><span class="c3">, and atrous convolution allows us to adaptively modify filter&rsquo;s field-of- view by changing the rate value. See Fig. </span><span class="c15"><a class="c5" href="#h.gjdgxs">1 </a></span><span class="c3">for illustration.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 11.33px; height: 24.07px;"><img alt="" src="images/image27.png" style="width: 11.33px; height: 24.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c472"><span class="c3">Atrous convolution also allows us to explicitly control how densely to compute feature responses in fully convolu- tional networks. Here, we denote by </span><span class="c26">output stride </span><span class="c3">the ratio of input image spatial resolution to final output resolution. For the DCNNs [</span><span class="c8"><a class="c5" href="#h.1664s55">47</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.3ep43zb">78</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.46r0co2">32</a></span><span class="c3">] deployed for the task of image classification, the final feature responses (before fully con- nected layers or global pooling) is 32 times smaller than the input image dimension, and thus </span><span class="c26">output stride </span><span class="c19">= 32</span><span class="c3">. If one would like to double the spatial density of computed fea- ture responses in the DCNNs (</span><span class="c26">i.e</span><span class="c3">., </span><span class="c26">output stride </span><span class="c19">= 16</span><span class="c3">), the stride of last pooling or convolutional layer that decreases resolution is set to 1 to avoid signal decimation. Then, all subsequent convolutional layers are replaced with atrous convolutional layers having rate </span><span class="c1">r </span><span class="c19">= 2</span><span class="c3">. This allows us to extract denser feature responses without requiring learning any extra parameters. Please refer to [</span><span class="c8"><a class="c5" href="#h.1pxezwc">11</a></span><span class="c3">] for more details.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c74"><span class="c149 c97"></span></p><ol class="c52 lst-kix_list_4-1" start="2"><li class="c632 li-bullet-3"><h2 style="display:inline"><span class="c166 c55">Going Deeper with Atrous Convolution</span></h2></li></ol><p class="c372"><span class="c3">We first explore designing modules with atrous convolu- tion laid out in cascade. To be concrete, we duplicate several copies of the last ResNet block, denoted as block4 in Fig. </span><span class="c15"><a class="c5" href="#h.1fob9te">3</a></span><span class="c3">, and arrange them in cascade. There are three </span><span class="c19">3 3 </span><span class="c3">convolu- tions in those blocks, and the last convolution contains stride 2 except the one in last block, similar to original ResNet. The motivation behind this model is that the introduced strid- ing makes it easy to capture long range information in the deeper blocks. For example, the whole image feature could be summarized in the last small resolution feature map, as illustrated in Fig. </span><span class="c15"><a class="c5" href="#h.1fob9te">3 </a></span><span class="c3">(a). However, we discover that the con- secutive striding is harmful for semantic segmentation (see Tab. </span><span class="c15"><a class="c5" href="#h.1t3h5sf">1 </a></span><span class="c3">in Sec. </span><span class="c15"><a class="c5" href="#h.2et92p0">4</a></span><span class="c3">) since detail information is decimated, and thus we apply atrous convolution with rates determined by the desired </span><span class="c26">output stride </span><span class="c3">value, as shown in Fig. </span><span class="c15"><a class="c5" href="#h.1fob9te">3 </a></span><span class="c3">(b) where </span><span class="c26">output stride </span><span class="c19">= 16</span><span class="c3">.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 11.33px; height: 24.07px;"><img alt="" src="images/image45.png" style="width: 11.33px; height: 24.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c40"><span class="c3">In this proposed model, we experiment with cascaded ResNet blocks up to block7 (</span><span class="c26">i.e</span><span class="c3">., extra block5, block6, block7 as replicas of block4), which has </span><span class="c26">output stride </span><span class="c19">= 256 </span><span class="c3">if no atrous convolution is applied.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c570"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 626.67px; height: 91.33px;"><img alt="" src="images/image8.png" style="width: 626.67px; height: 91.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 50.07px; height: 66.13px;"><img alt="" src="images/image3.jpg" style="width: 50.07px; height: 66.13px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ol class="c52 lst-kix_list_4-2 start" start="1"><li class="c440 li-bullet-4"><span class="c9">Going deeper without atrous convolution.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 626.67px; height: 91.33px;"><img alt="" src="images/image21.png" style="width: 626.67px; height: 91.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 50.07px; height: 66.13px;"><img alt="" src="images/image3.jpg" style="width: 50.07px; height: 66.13px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 18.60px; height: 18.33px;"><img alt="" src="images/image30.png" style="width: 18.60px; height: 18.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 24.20px; height: 24.00px;"><img alt="" src="images/image29.png" style="width: 24.20px; height: 24.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 13.80px; height: 13.87px;"><img alt="" src="images/image5.png" style="width: 13.80px; height: 13.87px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c548 li-bullet-5" id="h.1fob9te"><span class="c9">Going deeper with atrous convolution. Atrous convolution with </span><span class="c91 c42 c27 c97">rate &gt; </span><span class="c91 c97 c27 c428">1 </span><span class="c9">is applied after block3 when </span><span class="c49 c42 c27">output stride </span><span class="c91 c428 c97 c27">= 16</span><span class="c9">. Figure 3. Cascaded modules without and with atrous convolution.</span></li></ol><p class="c489 c537"><span class="c6"></span></p><h3 class="c151"><span class="c0">3.2.1 Multi-grid Method</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h3><p class="c585"><span class="c3">Motivated by multi-grid methods which employ a hierar- chy of grids of different sizes [</span><span class="c8"><a class="c5" href="#h.4i7ojhp">4</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.2szc72q">81</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.2xcytpi">5</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.48pi1tg">67</a></span><span class="c3">] and following [</span><span class="c8"><a class="c5" href="#h.279ka65">84</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1hmsyys">18</a></span><span class="c3">], we adopt different atrous rates within block4 to block7 in the proposed model. In particular, we define as </span><span class="c26">Multi Grid </span><span class="c19">= (</span><span class="c1">r , r , r </span><span class="c19">) </span><span class="c3">the unit rates for the three convo-</span></p><p class="c418"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 4.00px; height: 0.53px;"><img alt="" src="images/image7.png" style="width: 4.00px; height: 0.53px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><hr style="page-break-before:always;display:none;"><p class="c620"><span class="c31">1</span></p><p class="c38"><span class="c96"></span></p><p class="c111"><span class="c31">0.8</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 182.40px; height: 141.53px;"><img alt="" src="images/image10.png" style="width: 182.40px; height: 141.53px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 14.20px; height: 85.40px;"><img alt="" src="images/image34.png" style="width: 14.20px; height: 85.40px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c38"><span class="c96"></span></p><p class="c111"><span class="c31">0.6</span></p><p class="c38"><span class="c96"></span></p><p class="c111"><span class="c31">0.4</span></p><p class="c601"><span class="c97 c265">1 &nbsp; &nbsp; 2 &nbsp; &nbsp; 3</span></p><p class="c374"><span class="c3">lutional layers within block4 to block7. The final atrous rate for the convolutional layer is equal to the multiplication of the unit rate and the corresponding rate. For example, when </span><span class="c26">output stride </span><span class="c19">= 16 </span><span class="c3">and </span><span class="c26">Multi Grid </span><span class="c19">= (1</span><span class="c1">, </span><span class="c19">2</span><span class="c1">, </span><span class="c19">4)</span><span class="c3">, the three</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><hr style="page-break-before:always;display:none;"><p class="c23"><span class="c31">0.2</span></p><p class="c38"><span class="c96"></span></p><p class="c103"><span class="c31">0</span></p><p class="c154"><span class="c31">0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;20&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;40</span></p><p class="c223"><span class="c31">atrous rate</span></p><hr style="page-break-before:always;display:none;"><p class="c25"><span class="c61"></span></p><p class="c25"><span class="c61"></span></p><p class="c74"><span class="c97 c314"></span></p><p class="c261"><span class="c31">60&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;80</span></p><p class="c277"><span class="c3">convolutions will have </span><span class="c1">rates </span><span class="c19">= 2 (1</span><span class="c1">, </span><span class="c19">2</span><span class="c1">, </span><span class="c19">4) = (2</span><span class="c1">, </span><span class="c19">4</span><span class="c1">, </span><span class="c19">8) </span><span class="c3">in the block4, respectively.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 4.73px; height: 24.07px;"><img alt="" src="images/image40.png" style="width: 4.73px; height: 24.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ol class="c52 lst-kix_list_4-1" start="3"><li class="c435 li-bullet-6"><h2 style="display:inline"><span class="c166 c55">Atrous Spatial Pyramid Pooling</span></h2></li></ol><p class="c142"><span class="c3">We revisit the Atrous Spatial Pyramid Pooling proposed in [</span><span class="c8"><a class="c5" href="#h.1pxezwc">11</a></span><span class="c3">], where four parallel atrous convolutions with different atrous rates are applied on top of the feature map. ASPP is inspired by the success of spatial pyramid pooling [</span><span class="c8"><a class="c5" href="#h.28h4qwu">28</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.25b2l0r">49</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1mrcu09">31</a></span><span class="c3">] which showed that it is effective to resample features at different scales for accurately and efficiently classifying regions of an arbitrary scale. Different from [</span><span class="c8"><a class="c5" href="#h.1pxezwc">11</a></span><span class="c3">], we include batch normalization within ASPP.</span></p><p class="c137"><span class="c3">ASPP with different atrous rates effectively captures multi-scale information. However, we discover that as the sampling rate becomes larger, the number of valid filter weights (</span><span class="c26">i.e</span><span class="c3">., the weights that are applied to the valid fea- ture region, instead of padded zeros) becomes smaller. This effect is illustrated in Fig. </span><span class="c15"><a class="c5" href="#h.3znysh7">4 </a></span><span class="c3">when applying a </span><span class="c19">3 &nbsp; &nbsp;3 </span><span class="c3">filter to a </span><span class="c19">65 65 </span><span class="c3">feature map with different atrous rates. In the extreme case where the rate value is close to the feature map size, the </span><span class="c19">3 3 </span><span class="c3">filter, instead of capturing the whole image context, degenerates to a simple </span><span class="c19">1 1 </span><span class="c3">filter since only the center filter weight is effective.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 11.33px; height: 24.07px;"><img alt="" src="images/image11.png" style="width: 11.33px; height: 24.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 11.33px; height: 24.07px;"><img alt="" src="images/image11.png" style="width: 11.33px; height: 24.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 11.33px; height: 24.07px;"><img alt="" src="images/image11.png" style="width: 11.33px; height: 24.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 11.33px; height: 24.07px;"><img alt="" src="images/image45.png" style="width: 11.33px; height: 24.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c156 c399"><span class="c3">To overcome this problem and incorporate global context information to the model, we adopt image-level features, similar to [</span><span class="c8"><a class="c5" href="#h.1x0gk37">58</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1d96cc0">95</a></span><span class="c3">]. Specifically, we apply global average</span></p><hr style="page-break-before:always;display:none;"><p class="c507" id="h.3znysh7"><span class="c27">Figure 4. Normalized counts of valid weights with a </span><span class="c64 c27">3 &nbsp; &nbsp; &nbsp;3 </span><span class="c27">filter on a </span><span class="c64 c27">65 &nbsp; &nbsp; &nbsp;65 </span><span class="c27">feature map as atrous rate varies. When atrous rate is small, all the 9 filter weights are applied to most of the valid region on feature map, while atrous rate gets larger, the </span><span class="c64 c27">3 </span><span class="c42 c97 c27 c378">&times; </span><span class="c64 c27">3 </span><span class="c27">filter degenerates to a </span><span class="c64 c27">1 </span><span class="c42 c97 c27 c378">&times; </span><span class="c64 c27">1 </span><span class="c9">filter since only the center weight is effective.</span></p><p class="c74"><span class="c49 c428 c573"></span></p><p class="c56"><span class="c3">pooling on the last feature map of the model, feed the re- sulting image-level features to a </span><span class="c19">1 1 </span><span class="c3">convolution with 256 filters (and batch normalization [</span><span class="c8"><a class="c5" href="#h.2zbgiuw">38</a></span><span class="c3">]), and then bilinearly upsample the feature to the desired spatial dimension. In the end, our improved ASPP consists of (a) one </span><span class="c19">1 1 </span><span class="c3">convolution and three </span><span class="c19">3 3 </span><span class="c3">convolutions with </span><span class="c1">rates </span><span class="c19">= (6</span><span class="c1">, </span><span class="c19">12</span><span class="c1">, </span><span class="c19">18) </span><span class="c3">when </span><span class="c26">output stride </span><span class="c19">= 16 </span><span class="c3">(all with 256 filters and batch normaliza- tion), and (b) the image-level features, as shown in Fig. </span><span class="c15"><a class="c5" href="#h.tyjcwt">5</a></span><span class="c3">. Note that the rates are doubled when </span><span class="c26">output stride </span><span class="c19">= 8</span><span class="c3">. The resulting features from all the branches are then concate- nated and pass through another </span><span class="c19">1 1 </span><span class="c3">convolution (also with 256 filters and batch normalization) before the final </span><span class="c19">1 1 </span><span class="c3">convolution which generates the final logits.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 11.33px; height: 24.07px;"><img alt="" src="images/image11.png" style="width: 11.33px; height: 24.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 11.33px; height: 24.07px;"><img alt="" src="images/image11.png" style="width: 11.33px; height: 24.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 11.33px; height: 24.07px;"><img alt="" src="images/image11.png" style="width: 11.33px; height: 24.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 11.33px; height: 24.07px;"><img alt="" src="images/image11.png" style="width: 11.33px; height: 24.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 11.33px; height: 24.07px;"><img alt="" src="images/image11.png" style="width: 11.33px; height: 24.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 10.60px; height: 21.80px;"><img alt="" src="images/image4.png" style="width: 10.60px; height: 21.80px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 10.60px; height: 21.80px;"><img alt="" src="images/image4.png" style="width: 10.60px; height: 21.80px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c33"><span class="c13"></span></p><ol class="c52 lst-kix_list_4-0" start="4"><li class="c401 li-bullet-0"><h1 id="h.2et92p0" style="display:inline"><span class="c196 c55 c76">Experimental Evaluation</span></h1></li></ol><p class="c93"><span class="c3">We adapt the ImageNet-pretrained [</span><span class="c8"><a class="c5" href="#h.haapch">72</a></span><span class="c3">] ResNet [</span><span class="c8"><a class="c5" href="#h.46r0co2">32</a></span><span class="c3">] to the semantic segmentation by applying atrous convolution to extract dense features. Recall that </span><span class="c26">output stride </span><span class="c3">is defined as the ratio of input image spatial resolution to final out-</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c570"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 626.67px; height: 154.13px;"><img alt="" src="images/image51.png" style="width: 626.67px; height: 154.13px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 17.20px; height: 17.20px;"><img alt="" src="images/image23.png" style="width: 17.20px; height: 17.20px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 24.53px; height: 24.53px;"><img alt="" src="images/image22.png" style="width: 24.53px; height: 24.53px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 50.07px; height: 66.13px;"><img alt="" src="images/image3.jpg" style="width: 50.07px; height: 66.13px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 13.73px; height: 13.87px;"><img alt="" src="images/image6.png" style="width: 13.73px; height: 13.87px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c289" id="h.tyjcwt"><span class="c9">Figure 5. Parallel modules with atrous convolution (ASPP), augmented with image-level features.</span></p><p class="c74"><span class="c49 c428 c572"></span></p><p class="c501"><span class="c3">put resolution. For example, when </span><span class="c26">output stride </span><span class="c19">= 8</span><span class="c3">, the last two blocks (block3 and block4 in our notation) in the original ResNet contains atrous convolution with </span><span class="c1">rate </span><span class="c19">= 2 </span><span class="c3">and </span><span class="c1">rate </span><span class="c19">= 4 </span><span class="c3">respectively. Our implementation is built on TensorFlow [</span><span class="c8"><a class="c5" href="#h.3j2qqm3">1</a></span><span class="c3">].</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c309"><span class="c3">We evaluate the proposed models on the PASCAL VOC 2012 semantic segmentation benchmark [</span><span class="c8"><a class="c5" href="#h.2grqrue">20</a></span><span class="c3">] which con- tains 20 foreground object classes and one background class. The original dataset contains </span><span class="c19">1</span><span class="c1">, </span><span class="c19">464 </span><span class="c3">(</span><span class="c26">train</span><span class="c3">), </span><span class="c19">1</span><span class="c1">, </span><span class="c19">449 </span><span class="c3">(</span><span class="c26">val</span><span class="c3">), and </span><span class="c19">1</span><span class="c1">, </span><span class="c19">456 </span><span class="c3">(</span><span class="c26">test</span><span class="c3">) pixel-level labeled images for training, valida- tion, and testing, respectively. The dataset is augmented by the extra annotations provided by [</span><span class="c8"><a class="c5" href="#h.nmf14n">29</a></span><span class="c3">], resulting in </span><span class="c19">10</span><span class="c1">, </span><span class="c19">582 </span><span class="c3">(</span><span class="c26">trainaug</span><span class="c3">) training images. The performance is measured in terms of pixel intersection-over-union (IOU) averaged across the 21 classes.</span></p><ol class="c52 lst-kix_list_4-1" start="4"><li class="c312 li-bullet-7"><h2 id="h.3dy6vkm" style="display:inline"><span class="c166 c55">Training Protocol</span></h2></li></ol><p class="c80"><span class="c3">In this subsection, we discuss details of our training pro- tocol.</span></p><p class="c146"><span class="c30">Learning rate policy: </span><span class="c144">Similar to [</span><span class="c124"><a class="c5" href="#h.1x0gk37">58</a></span><span class="c144">, </span><span class="c124"><a class="c5" href="#h.1pxezwc">11</a></span><span class="c3">], we employ a</span></p><p class="c561"><span class="c3">&ldquo;poly&rdquo; learning rate policy where the initial learning rate is multiplied by </span><span class="c19">(1 </span><span class="c42 c144 c97 c378 c515">&minus;</span><span class="c42 c97 c291">&nbsp;</span><span class="c64 c42 c333">iter</span><span class="c64 c42 c211">&nbsp;</span><span class="c19">)</span><span class="c48 c42 c473">power</span><span class="c1">&nbsp;</span><span class="c3">with </span><span class="c1">power </span><span class="c19">= 0</span><span class="c1">.</span><span class="c19">9</span><span class="c3">.</span></p><p class="c287"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 2.80px; height: 0.53px;"><img alt="" src="images/image48.png" style="width: 2.80px; height: 0.53px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><hr style="page-break-before:always;display:none;"><p class="c25"><span class="c13"></span></p><p class="c25"><span class="c13"></span></p><p class="c283" id="h.1t3h5sf"><span class="c27">Table 1. Going deeper with atrous convolution when employing ResNet-50 with block7 and different </span><span class="c42 c27">output stride</span><span class="c27">. Adopting </span><span class="c42 c27">output stride </span><span class="c64 c27">= 8 </span><span class="c9">leads to better performance at the cost of more memory usage.</span></p><p class="c25"><span class="c13"></span></p><p class="c121"><span class="c3">convolution allows us to control </span><span class="c26">output stride </span><span class="c3">value at dif- ferent training stages without requiring learning extra model parameters. Also note that training with </span><span class="c26">output stride </span><span class="c19">= 16 </span><span class="c3">is several times faster than </span><span class="c26">output stride </span><span class="c19">= 8 </span><span class="c3">since the inter- mediate feature maps are spatially four times smaller, but at a sacrifice of accuracy since </span><span class="c26">output stride </span><span class="c19">= 16 </span><span class="c3">provides coarser feature maps.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 306.40px; height: 41.07px;"><img alt="" src="images/image35.png" style="width: 306.40px; height: 41.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c481"><span class="c0">Upsampling logits: </span><span class="c3">In our previous works [</span><span class="c8"><a class="c5" href="#h.3as4poj">10</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1pxezwc">11</a></span><span class="c3">], the target groundtruths are downsampled by 8 during training when </span><span class="c26">output stride </span><span class="c19">= 8</span><span class="c3">. We find it important to keep the groundtruths intact and instead upsample the final logits, since downsampling the groundtruths removes the fine anno- tations resulting in no back-propagation of details.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c547"><span class="c30">Data augmentation: </span><span class="c3">We apply data augmentation by</span></p><h3 class="c517"><span class="c0">Crop size:</span></h3><hr style="page-break-before:always;display:none;"><p class="c140"><span class="c91 c42 c97 c593">max iter</span></p><p class="c408"><span class="c3">Following the original training protocol [</span><span class="c8"><a class="c5" href="#h.3as4poj">10</a></span><span class="c3">,</span></p><hr style="page-break-before:always;display:none;"><p class="c198"><span class="c3">randomly scaling the input images (from 0.5 to 2.0) and</span></p><p class="c135"><span class="c8"><a class="c5" href="#h.1pxezwc">11</a></span><span class="c3">], patches are cropped from the image during training. For atrous convolution with large rates to be effective, large crop size is required; otherwise, the filter weights with large atrous rate are mostly applied to the padded zero region. We thus employ crop size to be 513 during both training and test on PASCAL VOC 2012 dataset.</span></p><p class="c260"><span class="c0">Batch normalization: </span><span class="c3">Our added modules on top of ResNet all include batch normalization parameters [</span><span class="c8"><a class="c5" href="#h.2zbgiuw">38</a></span><span class="c3">], which we found important to be trained as well. Since large batch size is required to train batch normalization parame- ters, we employ </span><span class="c26">output stride </span><span class="c19">= 16 </span><span class="c3">and compute the batch normalization statistics with a batch size of 16. The batch normalization parameters are trained with decay = 0.9997. After training on the </span><span class="c26">trainaug </span><span class="c3">set with 30K iterations and ini- tial learning rate = 0.007, we then freeze batch normalization parameters, employ </span><span class="c26">output stride </span><span class="c19">= 8</span><span class="c3">, and train on the offi- cial PASCAL VOC 2012 </span><span class="c26">trainval </span><span class="c3">set for another 30K itera- tions and smaller base learning rate = 0.001. Note that atrous</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><hr style="page-break-before:always;display:none;"><p class="c610"><span class="c3">randomly left-right flipping during training.</span></p><ol class="c52 lst-kix_list_4-1" start="5"><li class="c514 li-bullet-6"><h2 style="display:inline"><span class="c166 c55">Going Deeper with Atrous Convolution</span></h2></li></ol><p class="c156 c208"><span class="c3">We first experiment with building more blocks with atrous convolution in cascade.</span></p><p class="c67"><span class="c0">ResNet-50: </span><span class="c3">In Tab. </span><span class="c15"><a class="c5" href="#h.1t3h5sf">1</a></span><span class="c3">, we experiment with the effect of </span><span class="c26">output stride </span><span class="c3">when employing ResNet-50 with block7 (</span><span class="c26">i.e</span><span class="c3">., extra block5, block6, and block7). As shown in the table, in the case of </span><span class="c26">output stride </span><span class="c19">= 256 </span><span class="c3">(</span><span class="c26">i.e</span><span class="c3">., no atrous convolution at all), the performance is much worse than the others due to the severe signal decimation. When </span><span class="c26">output stride </span><span class="c3">gets larger and apply atrous convolution correspondingly, the performance improves from 20.29% to 75.18%, showing that atrous convolution is essential when building more blocks cascadedly for semantic segmentation.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c156 c192"><span class="c0">ResNet-50 vs. ResNet-101: </span><span class="c3">We replace ResNet-50 with deeper network ResNet-101 and change the number of cas- caded blocks. As shown in Tab. </span><span class="c15"><a class="c5" href="#h.4d34og8">2</a></span><span class="c3">, the performance improves</span></p><p class="c25"><span class="c3"></span></p><p class="c25"><span class="c3"></span></p><p class="c25"><span class="c3"></span></p><p class="c38"><span class="c49 c194"></span></p><p class="c186" id="h.4d34og8"><span class="c27">Table 2. Going deeper with atrous convolution when employ- ing ResNet-50 and ResNet-101 with different number of cas- caded blocks at </span><span class="c42 c27">output stride </span><span class="c64 c27">= 16</span><span class="c9">. Network structures &lsquo;block4&rsquo;, &lsquo;block5&rsquo;, &lsquo;block6&rsquo;, and &lsquo;block7&rsquo; add extra 0, 1, 2, 3 cascaded modules respectively. The performance is generally improved by adopting more cascaded blocks.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 289.39px; height: 3.52px;"><img alt="" src="images/image42.png" style="width: 289.39px; height: 3.52px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c230"><span class="c49 c194"></span></p><table class="c444"><tr class="c158"><td class="c589" colspan="1" rowspan="1"><p class="c602"><span class="c3">Multi-Grid</span></p></td><td class="c433" colspan="4" rowspan="1"><p class="c385"><span class="c3">block4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;block5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;block6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;block7</span></p></td></tr><tr class="c506"><td class="c389" colspan="1" rowspan="1"><p class="c624"><span class="c3">(1, 1, 1)</span></p></td><td class="c445" colspan="1" rowspan="1"><p class="c512"><span class="c3">68.39</span></p></td><td class="c256" colspan="1" rowspan="1"><p class="c604"><span class="c3">73.21</span></p></td><td class="c256" colspan="1" rowspan="1"><p class="c394"><span class="c3">75.34</span></p></td><td class="c256" colspan="1" rowspan="1"><p class="c394"><span class="c3">75.76</span></p></td></tr><tr class="c2"><td class="c417" colspan="1" rowspan="1"><p class="c477"><span class="c3">(1, 2, 1)</span></p></td><td class="c214" colspan="1" rowspan="1"><p class="c549"><span class="c3">70.23</span></p></td><td class="c90" colspan="1" rowspan="1"><p class="c603"><span class="c3">75.67</span></p></td><td class="c90" colspan="1" rowspan="1"><p class="c267"><span class="c3">76.09</span></p></td><td class="c90" colspan="1" rowspan="1"><p class="c267"><span class="c0">76.66</span></p></td></tr><tr class="c447"><td class="c417" colspan="1" rowspan="1"><p class="c529"><span class="c3">(1, 2, 3)</span></p></td><td class="c214" colspan="1" rowspan="1"><p class="c65"><span class="c3">73.14</span></p></td><td class="c90" colspan="1" rowspan="1"><p class="c583"><span class="c3">75.78</span></p></td><td class="c90" colspan="1" rowspan="1"><p class="c338"><span class="c3">75.96</span></p></td><td class="c90" colspan="1" rowspan="1"><p class="c338"><span class="c3">76.11</span></p></td></tr><tr class="c63"><td class="c417" colspan="1" rowspan="1"><p class="c294"><span class="c3">(1, 2, 4)</span></p></td><td class="c214" colspan="1" rowspan="1"><p class="c619"><span class="c3">73.45</span></p></td><td class="c90" colspan="1" rowspan="1"><p class="c545"><span class="c3">75.74</span></p></td><td class="c90" colspan="1" rowspan="1"><p class="c258"><span class="c3">75.85</span></p></td><td class="c90" colspan="1" rowspan="1"><p class="c258"><span class="c3">76.02</span></p></td></tr><tr class="c158"><td class="c449" colspan="1" rowspan="1"><p class="c126" id="h.2s8eyo1"><span class="c3">(2, 2, 2)</span></p></td><td class="c398" colspan="1" rowspan="1"><p class="c70"><span class="c3">71.45</span></p></td><td class="c87" colspan="1" rowspan="1"><p class="c635"><span class="c3">74.30</span></p></td><td class="c87" colspan="1" rowspan="1"><p class="c228"><span class="c3">74.70</span></p></td><td class="c87" colspan="1" rowspan="1"><p class="c228"><span class="c3">74.62</span></p></td></tr></table><p class="c494"><span class="c27">Table 3. Employing multi-grid method for ResNet-101 with dif- ferent number of cascaded blocks at </span><span class="c42 c27">output stride </span><span class="c64 c27">= 16</span><span class="c9">. The best model performance is shown in bold.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c25"><span class="c13"></span></p><p class="c391"><span class="c3">as more blocks are added, but the margin of improvement becomes smaller. Noticeably, employing block7 to ResNet- 50 decreases slightly the performance while it still improves the performance for ResNet-101.</span></p><p class="c451"><span class="c0">Multi-grid: </span><span class="c3">We apply the multi-grid method to ResNet- 101 with several cascadedly added blocks in Tab. </span><span class="c15"><a class="c5" href="#h.2s8eyo1">3</a></span><span class="c3">. The unit rates, </span><span class="c26">Multi Grid </span><span class="c19">= (</span><span class="c1">r</span><span class="c18">1</span><span class="c1">, r</span><span class="c18">2</span><span class="c1">, r</span><span class="c18">3</span><span class="c19">)</span><span class="c3">, are applied to block4 and all the other added blocks. As shown in the table, we observe that (a) applying multi-grid method is generally better than the vanilla version where </span><span class="c19">(</span><span class="c1">r</span><span class="c18">1</span><span class="c1">, r</span><span class="c18">2</span><span class="c1">, r</span><span class="c18">3</span><span class="c19">) = (1</span><span class="c1">, </span><span class="c19">1</span><span class="c1">, </span><span class="c19">1)</span><span class="c3">, (b) simply doubling the unit rates (</span><span class="c26">i.e</span><span class="c3">., </span><span class="c19">(</span><span class="c1">r</span><span class="c18">1</span><span class="c1">, r</span><span class="c18">2</span><span class="c1">, r</span><span class="c18">3</span><span class="c19">) = (2</span><span class="c1">, </span><span class="c19">2</span><span class="c1">, </span><span class="c19">2)</span><span class="c3">) is not effective, and (c) going deeper with multi-grid improves the performance. Our best model is the case where block7 and </span><span class="c19">(</span><span class="c1">r</span><span class="c18">1</span><span class="c1">, r</span><span class="c18">2</span><span class="c1">, r</span><span class="c18">3</span><span class="c19">) = (1</span><span class="c1">, </span><span class="c19">2</span><span class="c1">, </span><span class="c19">1) </span><span class="c3">are employed.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c497"><span class="c0">Inference strategy on val set: </span><span class="c3">The proposed model is trained with </span><span class="c26">output stride </span><span class="c19">= 16</span><span class="c3">, and then during inference we apply </span><span class="c26">output stride </span><span class="c19">= 8 </span><span class="c3">to get more detailed feature map. As shown in Tab. </span><span class="c15"><a class="c5" href="#h.17dp8vu">4</a></span><span class="c3">, interestingly, when evaluating our best cascaded model with </span><span class="c26">output stride </span><span class="c19">= 8</span><span class="c3">, the per- formance improves over evaluating with </span><span class="c26">output stride </span><span class="c19">= 16 </span><span class="c3">by </span><span class="c19">1</span><span class="c1">.</span><span class="c19">39%</span><span class="c3">. The performance is further improved by per- forming inference on multi-scale inputs (with </span><span class="c1">scales </span><span class="c19">= 0</span><span class="c1">.</span><span class="c19">5</span><span class="c1">, </span><span class="c19">0</span><span class="c1">.</span><span class="c19">75</span><span class="c1">, </span><span class="c19">1</span><span class="c1">.</span><span class="c19">0</span><span class="c1">, </span><span class="c19">1</span><span class="c1">.</span><span class="c19">25</span><span class="c1">, </span><span class="c19">1</span><span class="c1">.</span><span class="c19">5</span><span class="c1">, </span><span class="c19">1</span><span class="c1">.</span><span class="c19">75 </span><span class="c3">) and also left-right flipped images. In particular, we compute as the final result the average probabilities from each scale and flipped images.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 165.60px; height: 24.07px;"><img alt="" src="images/image39.png" style="width: 165.60px; height: 24.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image13.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ol class="c52 lst-kix_list_4-1" start="6"><li class="c496 li-bullet-6"><h2 style="display:inline"><span class="c166 c55">Atrous Spatial Pyramid Pooling</span></h2></li></ol><p class="c156 c204"><span class="c3">We then experiment with the Atrous Spatial Pyramid Pooling (ASPP) module with the main differences from [</span><span class="c8"><a class="c5" href="#h.1pxezwc">11</a></span><span class="c3">] being that batch normalization parameters [</span><span class="c8"><a class="c5" href="#h.2zbgiuw">38</a></span><span class="c3">] are fine-tuned and image-level features are included.</span></p><hr style="page-break-before:always;display:none;"><p class="c25"><span class="c13"></span></p><p class="c25"><span class="c13"></span></p><p class="c382" id="h.17dp8vu"><span class="c27">Table 4. Inference strategy on the </span><span class="c42 c27">val </span><span class="c27">set. </span><span class="c55 c27">MG</span><span class="c27">: Multi-grid. </span><span class="c55 c27">OS</span><span class="c27">: </span><span class="c42 c27">output stride</span><span class="c27">. </span><span class="c55 c27">MS</span><span class="c27">: Multi-scale inputs during test. </span><span class="c55 c27">Flip</span><span class="c9">: Adding left-right flipped inputs.</span></p><p class="c127"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 312.66px; height: 2.86px;"><img alt="" src="images/image52.png" style="width: 312.66px; height: 2.86px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><table class="c403"><tr class="c375"><td class="c105" colspan="1" rowspan="1"><p class="c226"><span class="c22">Multi-Grid</span></p><p class="c164"><span class="c22">(1, 1, 1) &nbsp; &nbsp;(1, 2, 1) &nbsp; &nbsp;(1, 2, 4)</span></p></td><td class="c576" colspan="1" rowspan="1"><p class="c340"><span class="c22">ASPP</span></p><p class="c173"><span class="c22">(6, 12, 18) &nbsp; &nbsp;(6, 12, 18, 24)</span></p></td><td class="c376" colspan="1" rowspan="1"><p class="c579"><span class="c22">Image</span></p><p class="c11"><span class="c22">Pooling</span></p></td><td class="c215" colspan="1" rowspan="1"><p class="c38"><span class="c49 c428 c531"></span></p><p class="c253"><span class="c22">mIOU</span></p></td></tr><tr class="c563"><td class="c141" colspan="1" rowspan="1"><p class="c623"><span class="c100 c97">C</span></p></td><td class="c141" colspan="1" rowspan="1"><p class="c25"><span class="c49 c155"></span></p></td><td class="c199" colspan="1" rowspan="1"><p class="c25"><span class="c49 c155"></span></p></td><td class="c86" colspan="1" rowspan="1"><p class="c197"><span class="c100 c97">C</span></p></td><td class="c213" colspan="1" rowspan="1"><p class="c25"><span class="c49 c155"></span></p></td><td class="c259" colspan="1" rowspan="1"><p class="c25"><span class="c49 c155"></span></p></td><td class="c219" colspan="1" rowspan="1"><p class="c88"><span class="c22">75.36</span></p></td></tr><tr class="c522"><td class="c141" colspan="1" rowspan="1"><p class="c25"><span class="c49 c155"></span></p></td><td class="c141" colspan="1" rowspan="1"><p class="c10"><span class="c100 c97">C</span></p></td><td class="c199" colspan="1" rowspan="1"><p class="c25"><span class="c49 c155"></span></p></td><td class="c86" colspan="1" rowspan="1"><p class="c119"><span class="c97 c100">C</span></p></td><td class="c213" colspan="1" rowspan="1"><p class="c25"><span class="c49 c155"></span></p></td><td class="c259" colspan="1" rowspan="1"><p class="c25"><span class="c49 c155"></span></p></td><td class="c219" colspan="1" rowspan="1"><p class="c557"><span class="c22">75.93</span></p></td></tr><tr class="c522"><td class="c141" colspan="1" rowspan="1"><p class="c25"><span class="c49 c155"></span></p></td><td class="c141" colspan="1" rowspan="1"><p class="c25"><span class="c49 c155"></span></p></td><td class="c199" colspan="1" rowspan="1"><p class="c510"><span class="c100 c97">C</span></p></td><td class="c86" colspan="1" rowspan="1"><p class="c119"><span class="c100 c97">C</span></p></td><td class="c213" colspan="1" rowspan="1"><p class="c25"><span class="c49 c155"></span></p></td><td class="c259" colspan="1" rowspan="1"><p class="c25"><span class="c49 c155"></span></p></td><td class="c219" colspan="1" rowspan="1"><p class="c557"><span class="c22">76.58</span></p></td></tr><tr class="c482"><td class="c107" colspan="1" rowspan="1"><p class="c25"><span class="c9"></span></p></td><td class="c107" colspan="1" rowspan="1"><p class="c25"><span class="c9"></span></p></td><td class="c504" colspan="1" rowspan="1"><p class="c524" id="h.3rdcrjn"><span class="c100 c97">C C</span></p></td><td class="c182" colspan="1" rowspan="1"><p class="c230"><span class="c49 c428 c500"></span></p><p class="c271"><span class="c100 c97">C</span></p></td><td class="c125" colspan="1" rowspan="1"><p class="c231"><span class="c100 c97">C</span></p></td><td class="c475" colspan="1" rowspan="1"><p class="c230"><span class="c49 c500 c428"></span></p><p class="c118"><span class="c100 c97">C</span></p></td><td class="c404" colspan="1" rowspan="1"><p class="c523"><span class="c22">76.46</span></p><p class="c268"><span class="c22">77.21</span></p></td></tr></table><p class="c535"><span class="c27">Table 5. Atrous Spatial Pyramid Pooling with multi-grid method and image-level features at </span><span class="c42 c27">output stride </span><span class="c27 c64">= 16</span><span class="c9">.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c419"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 313.68px; height: 3.12px;"><img alt="" src="images/image49.png" style="width: 313.68px; height: 3.12px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><table class="c437"><tr class="c241"><td class="c210" colspan="1" rowspan="1"><p class="c200"><span class="c39">Method</span></p></td><td class="c538" colspan="1" rowspan="1"><p class="c175"><span class="c39">OS=16</span></p></td><td class="c341" colspan="1" rowspan="1"><p class="c264"><span class="c39">OS=8</span></p></td><td class="c165" colspan="1" rowspan="1"><p class="c264"><span class="c39">MS</span></p></td><td class="c607" colspan="1" rowspan="1"><p class="c250"><span class="c39">Flip</span></p></td><td class="c508" colspan="1" rowspan="1"><p class="c250"><span class="c39">COCO</span></p></td><td class="c485" colspan="1" rowspan="1"><p class="c264"><span class="c39">mIOU</span></p></td></tr><tr class="c255"><td class="c586" colspan="1" rowspan="1"><p class="c527"><span class="c39">MG(1, 2, 4) +</span></p></td><td class="c300" colspan="1" rowspan="1"><p class="c12"><span class="c50">C</span></p></td><td class="c270" colspan="1" rowspan="1"><p class="c25"><span class="c49 c99"></span></p></td><td class="c373" colspan="1" rowspan="1"><p class="c25"><span class="c49 c99"></span></p></td><td class="c397" colspan="1" rowspan="1"><p class="c25"><span class="c49 c99"></span></p></td><td class="c516" colspan="1" rowspan="1"><p class="c25"><span class="c49 c99"></span></p></td><td class="c320" colspan="1" rowspan="1"><p class="c24"><span class="c39">77.21</span></p></td></tr><tr class="c14"><td class="c322" colspan="1" rowspan="1"><p class="c484"><span class="c39">ASPP(6, 12, 18) +</span></p></td><td class="c288" colspan="1" rowspan="1"><p class="c25"><span class="c49 c99"></span></p></td><td class="c143" colspan="1" rowspan="1"><p class="c292"><span class="c50">C</span></p></td><td class="c82" colspan="1" rowspan="1"><p class="c25"><span class="c49 c99"></span></p></td><td class="c79" colspan="1" rowspan="1"><p class="c25"><span class="c49 c99"></span></p></td><td class="c286" colspan="1" rowspan="1"><p class="c25"><span class="c49 c99"></span></p></td><td class="c195" colspan="1" rowspan="1"><p class="c599"><span class="c39">78.51</span></p></td></tr><tr class="c290"><td class="c322" colspan="1" rowspan="1"><p class="c222"><span class="c39">Image Pooling</span></p></td><td class="c288" colspan="1" rowspan="1"><p class="c25"><span class="c49 c99"></span></p></td><td class="c143" colspan="1" rowspan="1"><p class="c556"><span class="c50">C</span></p></td><td class="c82" colspan="1" rowspan="1"><p class="c357"><span class="c50">C</span></p></td><td class="c79" colspan="1" rowspan="1"><p class="c25"><span class="c49 c99"></span></p></td><td class="c286" colspan="1" rowspan="1"><p class="c25"><span class="c49 c99"></span></p></td><td class="c195" colspan="1" rowspan="1"><p class="c414"><span class="c39">79.45</span></p></td></tr><tr class="c14"><td class="c322" colspan="1" rowspan="1"><p class="c25"><span class="c49 c99"></span></p></td><td class="c288" colspan="1" rowspan="1"><p class="c25"><span class="c49 c99"></span></p></td><td class="c143" colspan="1" rowspan="1"><p class="c45"><span class="c50">C</span></p></td><td class="c82" colspan="1" rowspan="1"><p class="c617"><span class="c50">C</span></p></td><td class="c79" colspan="1" rowspan="1"><p class="c442"><span class="c50">C</span></p></td><td class="c286" colspan="1" rowspan="1"><p class="c25"><span class="c49 c99"></span></p></td><td class="c195" colspan="1" rowspan="1"><p class="c189"><span class="c39">79.77</span></p></td></tr><tr class="c319"><td class="c533" colspan="1" rowspan="1"><p class="c25"><span class="c39"></span></p></td><td class="c262" colspan="1" rowspan="1"><p class="c25"><span class="c39"></span></p></td><td class="c546" colspan="1" rowspan="1"><p class="c153" id="h.26in1rg"><span class="c50">C</span></p></td><td class="c310" colspan="1" rowspan="1"><p class="c159"><span class="c50">C</span></p></td><td class="c459" colspan="1" rowspan="1"><p class="c43"><span class="c50">C</span></p></td><td class="c244" colspan="1" rowspan="1"><p class="c412"><span class="c50">C</span></p></td><td class="c243" colspan="1" rowspan="1"><p class="c536"><span class="c39">82.70</span></p></td></tr></table><p class="c598"><span class="c27">Table 6. Inference strategy on the </span><span class="c42 c27">val </span><span class="c27">set: </span><span class="c55 c27">MG</span><span class="c27">: Multi-grid. </span><span class="c27 c55">ASPP</span><span class="c27">: Atrous spatial pyramid pooling. </span><span class="c55 c27">OS</span><span class="c27">: </span><span class="c42 c27">output stride</span><span class="c27">. </span><span class="c55 c27">MS</span><span class="c27">: Multi- scale inputs during test. </span><span class="c55 c27">Flip</span><span class="c27">: Adding left-right flipped inputs. </span><span class="c55 c27">COCO</span><span class="c9">: Model pretrained on MS-COCO.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c25"><span class="c13"></span></p><p class="c302"><span class="c0">ASPP: </span><span class="c3">In Tab. </span><span class="c15"><a class="c5" href="#h.3rdcrjn">5</a></span><span class="c3">, we experiment with the effect of in- corporating multi-grid in block4 and image-level features to the improved ASPP module. We first fix </span><span class="c1">ASPP </span><span class="c19">= (6</span><span class="c1">, </span><span class="c19">12</span><span class="c1">, </span><span class="c19">18) </span><span class="c3">(</span><span class="c26">i.e</span><span class="c3">., employ </span><span class="c1">rates </span><span class="c19">= (6</span><span class="c1">, </span><span class="c19">12</span><span class="c1">, </span><span class="c19">18) </span><span class="c3">for the three parallel </span><span class="c19">3 3 </span><span class="c3">convolution branches), and vary the multi- grid value. &nbsp; Employing </span><span class="c26">Multi Grid </span><span class="c19">= &nbsp; (1</span><span class="c1">, </span><span class="c19">2</span><span class="c1">, </span><span class="c19">1) </span><span class="c3">is better than </span><span class="c26">Multi Grid </span><span class="c19">= &nbsp; (1</span><span class="c1">, </span><span class="c19">1</span><span class="c1">, </span><span class="c19">1)</span><span class="c3">, while further improvement is attained by adopting </span><span class="c26">Multi Grid </span><span class="c19">= (1</span><span class="c1">, </span><span class="c19">2</span><span class="c1">, </span><span class="c19">4) </span><span class="c3">in the con- text of </span><span class="c1">ASPP </span><span class="c19">= (6</span><span class="c1">, </span><span class="c19">12</span><span class="c1">, </span><span class="c19">18) </span><span class="c3">(</span><span class="c26">cf </span><span class="c3">., the &lsquo;block4&rsquo; column in Tab. </span><span class="c15"><a class="c5" href="#h.2s8eyo1">3</a></span><span class="c3">). If we additionally employ another parallel branch with </span><span class="c1">rate </span><span class="c19">= 24 </span><span class="c3">for longer range context, the performance drops slightly by 0.12%. On the other hand, augmenting the ASPP module with image-level feature is effective, reaching the final performance of 77.21%.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 11.33px; height: 24.07px;"><img alt="" src="images/image11.png" style="width: 11.33px; height: 24.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c35"><span class="c0">Inference strategy on val set: </span><span class="c3">Similarly, we apply </span><span class="c26">output stride </span><span class="c19">= 8 </span><span class="c3">during inference once the model is trained. As shown in Tab. </span><span class="c15"><a class="c5" href="#h.26in1rg">6</a></span><span class="c3">, employing </span><span class="c26">output stride </span><span class="c19">= 8 </span><span class="c3">brings 1.3% improvement over using </span><span class="c26">output stride </span><span class="c19">= 16</span><span class="c3">, adopting multi-scale inputs and adding left-right flipped images fur- ther improve the performance by 0.94% and 0.32%, respec- tively. The best model with ASPP attains the performance of 79.77%, better than the best model with cascaded atrous convolution modules (79.35%), and thus is selected as our final model for test set evaluation.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c426"><span class="c30">Comparison with DeepLabv2: </span><span class="c3">Both our best cascaded</span></p><p class="c252"><span class="c3">model (in Tab. </span><span class="c15"><a class="c5" href="#h.17dp8vu">4</a></span><span class="c3">) and ASPP model (in Tab. </span><span class="c15"><a class="c5" href="#h.26in1rg">6</a></span><span class="c3">) (in both cases without DenseCRF post-processing or MS-COCO pre-training) already outperform DeepLabv2 (77.69% with DenseCRF and pretrained on MS-COCO in Tab. 4 of [</span><span class="c8"><a class="c5" href="#h.1pxezwc">11</a></span><span class="c3">]) on the PASCAL VOC 2012 </span><span class="c26">val </span><span class="c3">set. The improvement mainly comes from including and fine-tuning batch normalization parameters [</span><span class="c8"><a class="c5" href="#h.2zbgiuw">38</a></span><span class="c3">] in the proposed models and having a better way to encode multi-scale context.</span></p><p class="c156 c424"><span class="c0">Appendix: </span><span class="c3">We show more experimental results, such as the effect of hyper parameters and Cityscapes [</span><span class="c8"><a class="c5" href="#h.3o7alnk">14</a></span><span class="c3">] results, in the appendix.</span></p><p class="c424 c156"><span class="c0">Qualitative results: </span><span class="c3">We provide qualitative visual results of our best ASPP model in Fig. </span><span class="c15"><a class="c5" href="#h.lnxbz9">6</a></span><span class="c3">. As shown in the figure, our model is able to segment objects very well without any DenseCRF post-processing.</span></p><p class="c424 c156"><span class="c0">Failure mode: </span><span class="c3">As shown in the bottom row of Fig. </span><span class="c15"><a class="c5" href="#h.lnxbz9">6</a></span><span class="c3">, our model has difficulty in segmenting (a) sofa </span><span class="c26">vs</span><span class="c3">. chair, (b) dining table and chair, and (c) rare view of objects.</span></p><p class="c117"><span class="c0">Pretrained on COCO: </span><span class="c3">For comparison with other state- of-art models, we further pretrain our best ASPP model on MS-COCO dataset [</span><span class="c8"><a class="c5" href="#h.3hv69ve">57</a></span><span class="c3">]. &nbsp; From the MS-COCO </span><span class="c26">train- val minus minival </span><span class="c3">set, we only select the images that have annotation regions larger than 1000 pixels and contain the classes defined in PASCAL VOC 2012, resulting in about 60K images for training. Besides, the MS-COCO classes not defined in PASCAL VOC 2012 are all treated as back- ground class. After pretraining on MS-COCO dataset, our proposed model attains performance of 82.7% on </span><span class="c26">val </span><span class="c3">set when using </span><span class="c26">output stride </span><span class="c19">= 8</span><span class="c3">, multi-scale inputs and adding left-right flipped images during inference. We adopt smaller initial learning rate = 0.0001 and same training protocol as in Sec. </span><span class="c15"><a class="c5" href="#h.3dy6vkm">4.1 </a></span><span class="c3">when fine-tuning on PASCAL VOC 2012 dataset.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c474"><span class="c0">Test set result and an effective bootstrapping method: </span><span class="c3">We notice that PASCAL VOC 2012 dataset provides higher quality of annotations than the augmented dataset [</span><span class="c8"><a class="c5" href="#h.nmf14n">29</a></span><span class="c3">], es- pecially for the bicycle class. We thus further fine-tune our model on the official PASCAL VOC 2012 </span><span class="c26">trainval </span><span class="c3">set be- fore evaluating on the test set. Specifically, our model is trained with </span><span class="c26">output stride </span><span class="c19">= 8 </span><span class="c3">(so that annotation details are kept) and the batch normalization parameters are frozen (see Sec. </span><span class="c15"><a class="c5" href="#h.3dy6vkm">4.1 </a></span><span class="c3">for details). Besides, instead of performing pixel hard example mining as [</span><span class="c8"><a class="c5" href="#h.meukdy">85</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.3mzq4wv">70</a></span><span class="c3">], we resort to bootstrapping on hard images. In particular, we duplicate the images that contain hard classes (namely bicycle, chair, table, potted- plant, and sofa) in the training set. As shown in Fig. </span><span class="c15"><a class="c5" href="#h.1ksv4uv">7</a></span><span class="c3">, the simple bootstrapping method is effective for segmenting the bicycle class. In the end, our &lsquo;DeepLabv3&rsquo; achieves the per- formance of 85.7% on the test set without any DenseCRF post-processing, as shown in Tab. </span><span class="c15"><a class="c5" href="#h.2ce457m">7</a></span><span class="c3">.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c345"><span class="c0">Model pretrained on JFT-300M: </span><span class="c3">Motivated by the re- cent work of [</span><span class="c8"><a class="c5" href="#h.1tuee74">79</a></span><span class="c3">], we further employ the ResNet-101 model which has been pretraind on both ImageNet and the JFT- 300M dataset [</span><span class="c8"><a class="c5" href="#h.111kx3o">34</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.2p2csry">13</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1tuee74">79</a></span><span class="c3">], resulting in a performance of</span></p><hr style="page-break-before:always;display:none;"><p class="c74"><span class="c49 c428 c468"></span></p><p class="c115"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 276.07px; height: 310.00px;"><img alt="" src="images/image26.png" style="width: 276.07px; height: 310.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c191"><span class="c27">Table 7. Performance on PASCAL VOC 2012 </span><span class="c42 c27">test </span><span class="c9">set.</span></p><p class="c74"><span class="c49 c318"></span></p><p class="c123"><span class="c3">86.9% on PASCAL VOC 2012 test set.</span></p><p class="c74"><span class="c3"></span></p><ol class="c52 lst-kix_list_4-0" start="5"><li class="c411 li-bullet-0"><h1 style="display:inline"><span class="c196 c55 c76">Conclusion</span></h1></li></ol><p class="c120"><span class="c3">Our proposed model &ldquo;DeepLabv3&rdquo; employs atrous con- volution with upsampled filters to extract dense feature maps and to capture long range context. Specifically, to encode multi-scale information, our proposed cascaded module grad- ually doubles the atrous rates while our proposed atrous spa- tial pyramid pooling module augmented with image-level features probes the features with filters at multiple sampling rates and effective field-of-views. Our experimental results show that the proposed model significantly improves over previous DeepLab versions and achieves comparable perfor- mance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.</span></p><p class="c230"><span class="c149 c97"></span></p><p class="c183"><span class="c0">Acknowledgments </span><span class="c3">We would like to acknowledge valu- able discussions with Zbigniew Wojna, the help from Chen Sun and Andrew Howard, and the support from Google Mobile Vision team.</span></p><p class="c38"><span class="c4"></span></p><ol class="c52 lst-kix_list_2-0 start" start="1"><li class="c526 li-bullet-8"><h1 style="display:inline"><span class="c196 c55 c76">Effect of hyper-parameters</span></h1></li></ol><p class="c156 c554"><span class="c3">In this section, we follow the same training protocol as in the main paper and experiment with the effect of some hyper-parameters.</span></p><p class="c443"><span class="c0">New training protocol: </span><span class="c3">As mentioned in the main paper, we change the training protocol in [</span><span class="c8"><a class="c5" href="#h.3as4poj">10</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1pxezwc">11</a></span><span class="c3">] with three main differences: (1) larger crop size, (2) upsampling logits during training, and (3) fine-tuning batch normalization. Here, we quantitatively measure the effect of the changes. As shown</span></p><p class="c33"><span class="c49 c298"></span></p><p class="c167"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 617.07px; height: 792.61px;"><img alt="" src="images/image54.jpg" style="width: 617.07px; height: 792.61px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><table class="c190"><tr class="c319"><td class="c131" colspan="1" rowspan="1"><p class="c543"><span class="c4">Method</span></p></td><td class="c95" colspan="1" rowspan="1"><p class="c325"><span class="c4">OS=16</span></p></td><td class="c205" colspan="1" rowspan="1"><p class="c448"><span class="c4">OS=8</span></p></td><td class="c534" colspan="1" rowspan="1"><p class="c448"><span class="c4">MS</span></p></td><td class="c528" colspan="1" rowspan="1"><p class="c325"><span class="c4">Flip</span></p></td><td class="c395" colspan="1" rowspan="1"><p class="c325"><span class="c4">mIOU</span></p></td></tr><tr class="c158"><td class="c552" colspan="1" rowspan="1"><p class="c390"><span class="c3">Network</span></p></td><td class="c553" colspan="4" rowspan="1"><p class="c454"><span class="c3">block4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;block5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;block6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;block7</span></p></td></tr><tr class="c334"><td class="c218" colspan="1" rowspan="1"><p class="c116"><span class="c3">ResNet-50</span></p></td><td class="c464" colspan="1" rowspan="1"><p class="c311"><span class="c3">64.81</span></p></td><td class="c239" colspan="1" rowspan="1"><p class="c342"><span class="c3">72.14</span></p></td><td class="c201" colspan="1" rowspan="1"><p class="c113"><span class="c3">74.29</span></p></td><td class="c587" colspan="1" rowspan="1"><p class="c311"><span class="c3">73.88</span></p></td></tr><tr class="c158"><td class="c280" colspan="1" rowspan="1"><p class="c416"><span class="c3">ResNet-101</span></p></td><td class="c209" colspan="1" rowspan="1"><p class="c102"><span class="c3">68.39</span></p></td><td class="c295" colspan="1" rowspan="1"><p class="c339"><span class="c3">73.21</span></p></td><td class="c78" colspan="1" rowspan="1"><p class="c17"><span class="c3">75.34</span></p></td><td class="c577" colspan="1" rowspan="1"><p class="c102"><span class="c3">75.76</span></p></td></tr><tr class="c229"><td class="c630" colspan="1" rowspan="1"><p class="c597"><span class="c4">block7 +</span></p></td><td class="c349" colspan="1" rowspan="1"><p class="c393"><span class="c148 c97">C</span></p></td><td class="c360" colspan="1" rowspan="1"><p class="c25"><span class="c22"></span></p></td><td class="c331" colspan="1" rowspan="1"><p class="c25"><span class="c22"></span></p></td><td class="c227" colspan="1" rowspan="1"><p class="c25"><span class="c22"></span></p></td><td class="c145" colspan="1" rowspan="1"><p class="c161"><span class="c4">76.66</span></p></td></tr><tr class="c251"><td class="c337" colspan="1" rowspan="1"><p class="c565"><span class="c4">MG(1, 2, 1)</span></p></td><td class="c220" colspan="1" rowspan="1"><p class="c25"><span class="c39"></span></p></td><td class="c387" colspan="1" rowspan="1"><p class="c567"><span class="c148 c97">C</span></p></td><td class="c450" colspan="1" rowspan="1"><p class="c25"><span class="c39"></span></p></td><td class="c172" colspan="1" rowspan="1"><p class="c25"><span class="c39"></span></p></td><td class="c145" colspan="1" rowspan="1"><p class="c69"><span class="c4">78.05</span></p></td></tr><tr class="c317"><td class="c337" colspan="1" rowspan="1"><p class="c25"><span class="c39"></span></p></td><td class="c220" colspan="1" rowspan="1"><p class="c25"><span class="c39"></span></p></td><td class="c387" colspan="1" rowspan="1"><p class="c478"><span class="c148 c97">C</span></p></td><td class="c450" colspan="1" rowspan="1"><p class="c421"><span class="c148 c97">C</span></p></td><td class="c172" colspan="1" rowspan="1"><p class="c25"><span class="c39"></span></p></td><td class="c145" colspan="1" rowspan="1"><p class="c488"><span class="c4">78.93</span></p></td></tr><tr class="c315"><td class="c280" colspan="1" rowspan="1"><p class="c25"><span class="c9"></span></p></td><td class="c209" colspan="1" rowspan="1"><p class="c25"><span class="c9"></span></p></td><td class="c295" colspan="1" rowspan="1"><p class="c324"><span class="c148 c97">C</span></p></td><td class="c78" colspan="1" rowspan="1"><p class="c505"><span class="c97 c148">C</span></p></td><td class="c368" colspan="1" rowspan="1"><p class="c308"><span class="c148 c97">C</span></p></td><td class="c513" colspan="1" rowspan="1"><p class="c365"><span class="c4">79.35</span></p></td></tr></table><p class="c25 c240"><span class="c3"></span></p><p class="c257" id="h.lnxbz9"><span class="c27">Figure 6. Visualization results on the </span><span class="c42 c27">val </span><span class="c9">set when employing our best ASPP model. The last row shows a failure mode.</span></p><p class="c519"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 70.00px; height: 52.50px;"><img alt="" src="images/image55.jpg" style="width: 70.00px; height: 52.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 70.00px; height: 52.50px;"><img alt="" src="images/image56.png" style="width: 70.00px; height: 52.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 67.50px; height: 50.70px;"><img alt="" src="images/image57.png" style="width: 67.50px; height: 50.70px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 67.50px; height: 50.40px;"><img alt="" src="images/image58.png" style="width: 67.50px; height: 50.40px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c537 c489"><span class="c49 c221"></span></p><table class="c493"><tr class="c566"><td class="c284" colspan="1" rowspan="1"><p class="c207"><span class="c3">513</span></p></td><td class="c386" colspan="1" rowspan="1"><p class="c304"><span class="c62">C</span></p></td><td class="c432" colspan="1" rowspan="1"><p class="c348"><span class="c3">76.01</span></p></td></tr><tr class="c53"><td class="c66" colspan="1" rowspan="1"><p class="c329" id="h.35nkun2"><span class="c3">321</span></p></td><td class="c232" colspan="1" rowspan="1"><p class="c629"><span class="c62">C</span></p></td><td class="c301" colspan="1" rowspan="1"><p class="c438"><span class="c3">67.22</span></p></td></tr></table><p class="c344"><span class="c3"></span></p><p class="c25"><span class="c49 c99"></span></p><p class="c33"><span class="c49 c99"></span></p><ol class="c52 lst-kix_list_2-1 start" start="1"><li class="c174 li-bullet-9" id="h.1ksv4uv"><span class="c49 c155">Image&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(b) G.T.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(c) w/o bootstrapping&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(d) w/ bootstrapping</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 0.53px; height: 31.93px;"><img alt="" src="images/image9.png" style="width: 0.53px; height: 31.93px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 187.20px; height: 62.00px;"><img alt="" src="images/image36.png" style="width: 187.20px; height: 62.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 69.29px; height: 46.29px;"><img alt="" src="images/image44.jpg" style="width: 69.29px; height: 46.29px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 69.29px; height: 46.29px;"><img alt="" src="images/image43.png" style="width: 69.29px; height: 46.29px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 68.68px; height: 45.94px;"><img alt="" src="images/image12.png" style="width: 68.68px; height: 45.94px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 68.68px; height: 45.94px;"><img alt="" src="images/image53.png" style="width: 68.68px; height: 45.94px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ol><p class="c132"><span class="c9">Figure 7. Bootstrapping on hard images improves segmentation accuracy for rare and finely annotated classes such as bicycle.</span></p><p class="c25"><span class="c13"></span></p><p class="c33"><span class="c49 c276"></span></p><p class="c509"><span class="c3">in Tab. </span><span class="c15"><a class="c5" href="#h.35nkun2">8</a></span><span class="c3">, DeepLabv3 attains the performance of 77.21% on the PASCAL VOC 2012 </span><span class="c26">val </span><span class="c3">set [</span><span class="c8"><a class="c5" href="#h.2grqrue">20</a></span><span class="c3">] when adopting the new training protocol setting as in the main paper. When training DeepLabv3 without fine-tuning the batch normal- ization, the performance drops to 75.95%. If we do not upsample the logits during training (and instead downsam- ple the groundtruths), the performance decreases to 76.01%. Furthermore, if we employ smaller value of crop size (</span><span class="c26">i.e</span><span class="c3">.,</span></p><hr style="page-break-before:always;display:none;"><p class="c128"><span class="c27">Table 8. Effect of hyper-parameters during training on PASCAL VOC 2012 </span><span class="c42 c27">val </span><span class="c27">set at </span><span class="c42 c27">output stride=16</span><span class="c27">. </span><span class="c55 c27">UL</span><span class="c27">: Upsampling Logits. </span><span class="c55 c27">BN</span><span class="c9">: Fine-tuning batch normalization.</span></p><p class="c25"><span class="c13"></span></p><p class="c25"><span class="c13"></span></p><p class="c25"><span class="c13"></span></p><p class="c25"><span class="c13"></span></p><p class="c25"><span class="c13"></span></p><p class="c25"><span class="c13"></span></p><p class="c269" id="h.44sinio"><span class="c27">Table 9. Effect of batch size on PASCAL VOC 2012 </span><span class="c42 c27">val </span><span class="c27">set. We em- ploy </span><span class="c42 c27">output stride=16 </span><span class="c9">during both training and evaluation. Large batch size is required while training the model with fine-tuning the batch normalization parameters.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 119.13px; height: 96.07px;"><img alt="" src="images/image41.png" style="width: 119.13px; height: 96.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c41"><span class="c3">321 as in [</span><span class="c8"><a class="c5" href="#h.3as4poj">10</a></span><span class="c3">, </span><span class="c8"><a class="c5" href="#h.1pxezwc">11</a></span><span class="c3">]), the performance significantly decreases&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c144 c97 c206">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><p class="c305"><span class="c3">to 67.22%, demonstrating that boundary effect resulted from small crop size hurts the performance of DeepLabv3 which employs large atrous rates in the Atrous Spatial Pyramid</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 245.47px; height: 49.80px;"><img alt="" src="images/image2.png" style="width: 245.47px; height: 49.80px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c25"><span class="c3"></span></p><p class="c38"><span class="c49 c423"></span></p><p class="c608"><span class="c3">As shown in Tab. </span><span class="c15"><a class="c5" href="#h.44sinio">9</a></span><span class="c3">, employing small batch size is inefficient to train the model, while using larger batch size leads to better performance.</span></p><p class="c176"><span class="c0">Output stride: </span><span class="c3">The value of </span><span class="c26">output stride </span><span class="c3">determines the output feature map resolution and in turn affects the largest batch size we could use during training. In Tab. </span><span class="c15"><a class="c5" href="#h.2jxsxqh">10</a></span><span class="c3">, we quantitatively measure the effect of employing different </span><span class="c26">output stride </span><span class="c3">values during both training and evaluation on the PASCAL VOC 2012 </span><span class="c26">val </span><span class="c3">set. We first fix the evaluation </span><span class="c26">output stride </span><span class="c19">= 16</span><span class="c3">, vary the training </span><span class="c26">output stride </span><span class="c3">and fit the largest possible batch size for all the settings (we are able to fit batch size 6, 16, and 24 for training </span><span class="c26">output stride </span><span class="c3">equal to 8, 16, and 32, respectively). As shown in the top rows of Tab. </span><span class="c15"><a class="c5" href="#h.2jxsxqh">10</a></span><span class="c3">, employing training </span><span class="c26">output stride </span><span class="c19">= 8 </span><span class="c3">only attains the performance of 74.45% because we could not fit large batch size in this setting which degrades the performance while fine-tuning the batch normalization parameters. When employing training </span><span class="c26">output stride </span><span class="c19">= 32</span><span class="c3">, we could fit large batch size but we lose feature map details. On the other hand, employing training </span><span class="c26">output stride </span><span class="c19">= 16 </span><span class="c3">strikes the best trade- off and leads to the best performance. In the bottom rows of Tab. </span><span class="c15"><a class="c5" href="#h.2jxsxqh">10</a></span><span class="c3">, we increase the evaluation </span><span class="c26">output stride </span><span class="c19">= 8</span><span class="c3">. All settings improve the performance except the one where training </span><span class="c26">output stride </span><span class="c19">= 32</span><span class="c3">. We hypothesize that we lose too much feature map details during training, and thus the model could not recover the details even when employing</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><hr style="page-break-before:always;display:none;"><p class="c530" id="h.2jxsxqh"><span class="c27">Table 10. Effect of </span><span class="c42 c27">output stride </span><span class="c27">on PASCAL VOC 2012 </span><span class="c42 c27">val </span><span class="c27">set. Employing </span><span class="c42 c27">output stride=16 </span><span class="c27">during training leads to better perfor- mance for both eval </span><span class="c42 c27">output stride </span><span class="c64 c27">= 8 </span><span class="c27">and </span><span class="c64 c27">16</span><span class="c9">.</span></p><p class="c25"><span class="c13"></span></p><p class="c580"><span class="c42 c144">output stride </span><span class="c48">= 8 </span><span class="c3">during evaluation.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c230"><span class="c4"></span></p><ol class="c52 lst-kix_list_2-0" start="2"><li class="c36 li-bullet-10"><h1 style="display:inline"><span class="c196 c55 c76">Asynchronous training</span></h1></li></ol><p class="c156 c495"><span class="c3">In this section, we experiment DeepLabv3 with Tensor- Flow asynchronous training [</span><span class="c8"><a class="c5" href="#h.3j2qqm3">1</a></span><span class="c3">]. We measure the effect of training the model with multiple replicas on PASCAL VOC 2012 semantic segmentation dataset. Our baseline employs simply one replica and requires training time 3.65 days with a K80 GPU. As shown in Tab. </span><span class="c15"><a class="c5" href="#h.z337ya">11</a></span><span class="c3">, we found that the perfor- mance of using multiple replicas does not drop compared to the baseline. However, training time with 32 replicas is significantly reduced to 2.74 hours.</span></p><p class="c230"><span class="c4"></span></p><ol class="c52 lst-kix_list_2-0" start="3"><li class="c297 li-bullet-11"><h1 style="display:inline"><span class="c196 c55 c76">DeepLabv3 on Cityscapes dataset</span></h1></li></ol><p class="c495 c156"><span class="c3">Cityscapes [</span><span class="c8"><a class="c5" href="#h.3o7alnk">14</a></span><span class="c3">] is a large-scale dataset containing high quality pixel-level annotations of 5000 images (2975, 500, and 1525 for the training, validation, and test sets respec- tively) and about 20000 coarsely annotated images. Follow- ing the evaluation protocol [</span><span class="c8"><a class="c5" href="#h.3o7alnk">14</a></span><span class="c3">], 19 semantic labels are used for evaluation without considering the void label.</span></p><p class="c25"><span class="c3"></span></p><p class="c25"><span class="c3"></span></p><p class="c25"><span class="c3"></span></p><p class="c25"><span class="c3"></span></p><p class="c25"><span class="c3"></span></p><p class="c25"><span class="c3"></span></p><p class="c25"><span class="c3"></span></p><p class="c230"><span class="c13"></span></p><p class="c237" id="h.z337ya"><span class="c27">Table 11. Evaluation performance on PASCAL VOC 2012 </span><span class="c42 c27">val </span><span class="c9">set when adopting asynchronous training.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 0.53px; height: 255.07px;"><img alt="" src="images/image24.png" style="width: 0.53px; height: 255.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c402"><span class="c49 c194"></span></p><p class="c279"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 225.00px; height: 93.87px;"><img alt="" src="images/image28.png" style="width: 225.00px; height: 93.87px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 4.00px; height: 0.53px;"><img alt="" src="images/image7.png" style="width: 4.00px; height: 0.53px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c97 c303">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 4.00px; height: 0.53px;"><img alt="" src="images/image7.png" style="width: 4.00px; height: 0.53px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c458"><span class="c27">Table 12. DeepLabv3 on the Cityscapes </span><span class="c42 c27">val </span><span class="c9">set when trained with</span></p><p class="c352"><span class="c27">only </span><span class="c42 c27">train fine </span><span class="c27">set. </span><span class="c55 c27">OS</span><span class="c27">: </span><span class="c42 c27">output stride</span><span class="c27">. </span><span class="c55 c27">MS</span><span class="c27">: Multi-scale inputs during inference. </span><span class="c55 c27">Flip</span><span class="c9">: Adding left-right flipped inputs.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image13.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c25"><span class="c13"></span></p><p class="c248"><span class="c3">We first evaluate the proposed DeepLabv3 model on the validation set when training with only 2975 images (</span><span class="c26">i.e</span><span class="c3">., </span><span class="c26">train fine </span><span class="c3">set). We adopt the same training protocol as before except that we employ 90K training iterations, crop size equal to 769, and running inference on the whole image, instead of on the overlapped regions as in [</span><span class="c8"><a class="c5" href="#h.1pxezwc">11</a></span><span class="c3">]. As shown in Tab. </span><span class="c15"><a class="c5" href="#h.rjefff">12</a></span><span class="c3">, DeepLabv3 attains the performance of 77.23% when evaluating at </span><span class="c26">output stride </span><span class="c19">= 16</span><span class="c3">. Evaluating the model at </span><span class="c26">output stride </span><span class="c19">= 8 </span><span class="c3">improves the performance to 77.82%. When we employ multi-scale inputs (we could fit </span><span class="c1">scales </span><span class="c19">= 0</span><span class="c1">.</span><span class="c19">75</span><span class="c1">, </span><span class="c19">1</span><span class="c1">, </span><span class="c19">1</span><span class="c1">.</span><span class="c19">25 </span><span class="c3">on a K40 GPU) and add left-right flipped inputs, the model achieves 79.30%.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 80.00px; height: 24.07px;"><img alt="" src="images/image38.png" style="width: 80.00px; height: 24.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c156 c425"><span class="c3">In order to compete with other state-of-art models, we further train DeepLabv3 on the </span><span class="c26">trainval coarse </span><span class="c3">set (</span><span class="c26">i.e</span><span class="c3">., the 3475 finely annotated images and the extra 20000 coarsely annotated images). We adopt more scales and finer </span><span class="c26">output stride </span><span class="c3">during inference. In particular, we perform in- ference with </span><span class="c1">scales </span><span class="c19">= 0</span><span class="c1">.</span><span class="c19">75</span><span class="c1">, </span><span class="c19">1</span><span class="c1">, </span><span class="c19">1</span><span class="c1">.</span><span class="c19">25</span><span class="c1">, </span><span class="c19">1</span><span class="c1">.</span><span class="c19">5</span><span class="c1">, </span><span class="c19">1</span><span class="c1">.</span><span class="c19">75</span><span class="c1">, </span><span class="c19">2 </span><span class="c3">and eval- uation </span><span class="c26">output stride </span><span class="c19">= 4 </span><span class="c3">with CPUs, which contributes extra 0.8% and 0.1% respectively on the validation set compared to using only three scales and </span><span class="c26">output stride </span><span class="c19">= 8</span><span class="c3">. In the end, as shown in Tab. </span><span class="c15"><a class="c5" href="#h.1y810tw">13</a></span><span class="c3">, our proposed DeepLabv3 achieves the performance of 81.3% on the test set. Some results on </span><span class="c26">val </span><span class="c3">set are visualized in Fig. </span><span class="c15"><a class="c5" href="#h.147n2zr">8</a></span><span class="c3">.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 144.93px; height: 24.07px;"><img alt="" src="images/image25.png" style="width: 144.93px; height: 24.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c38"><span class="c49 c194"></span></p><h1 class="c261"><span class="c55 c76 c196">References</span></h1><ol class="c52 lst-kix_list_1-0 start" start="1"><li class="c130 li-bullet-12" id="h.3j2qqm3"><span class="c9">M. Abadi, A. Agarwal, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. </span><span class="c49 c42 c27">arXiv:1603.04467</span><span class="c9">, 2016.</span></li><li class="c392 li-bullet-13"><span class="c9">A. Adams, J. Baek, and M. A. Davis. Fast high-dimensional filtering using the permutohedral lattice. In </span><span class="c49 c42 c27">Eurographics</span><span class="c9">, 2010.</span></li></ol><hr style="page-break-before:always;display:none;"><p class="c489 c541"><span class="c39"></span></p><p class="c503"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 0.53px; height: 16.00px;"><img alt="" src="images/image20.png" style="width: 0.53px; height: 16.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c33"><span class="c149 c97"></span></p><p class="c615" id="h.1y810tw"><span class="c27">Table 13. Performance on Cityscapes </span><span class="c42 c27">test </span><span class="c27">set. &nbsp; </span><span class="c55 c27">Coarse</span><span class="c27">: &nbsp; Use </span><span class="c42 c27">train extra </span><span class="c9">set (coarse annotations) as well. Only a few top models with known references are listed in this table.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c38"><span class="c49 c428 c511"></span></p><ol class="c52 lst-kix_list_1-0" start="3"><li class="c436 li-bullet-13" id="h.4i7ojhp"><span class="c9">V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. </span><span class="c49 c42 c27">arXiv:1511.00561</span><span class="c9">, 2015.</span></li><li class="c584 li-bullet-14" id="h.2xcytpi"><span class="c9">A. Brandt. Multi-level adaptive solutions to boundary-value problems. </span><span class="c49 c42 c27">Mathematics of computation</span><span class="c9">, 31(138):333&ndash;390, 1977.</span></li><li class="c626 li-bullet-15" id="h.1ci93xb"><span class="c9">W. L. Briggs, V. E. Henson, and S. F. McCormick. </span><span class="c49 c42 c27">A multigrid tutorial</span><span class="c9">. SIAM, 2000.</span></li><li class="c446 li-bullet-16" id="h.3whwml4"><span class="c9">W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki. Scene labeling with lstm recurrent neural networks. In </span><span class="c49 c42 c27">CVPR</span><span class="c9">, 2015.</span></li><li class="c353 li-bullet-17" id="h.2bn6wsx"><span class="c9">H. Caesar, J. Uijlings, and V. Ferrari. COCO-Stuff: Thing and stuff classes in context. </span><span class="c49 c42 c27">arXiv:1612.03716</span><span class="c9">, 2016.</span></li><li class="c446 li-bullet-18" id="h.qsh70q"><span class="c9">S. Chandra and I. Kokkinos. Fast, exact and multi-scale in- ference for semantic image segmentation with deep Gaussian CRFs. </span><span class="c49 c42 c27">arXiv:1603.08358</span><span class="c9">, 2016.</span></li><li class="c366 li-bullet-19" id="h.3as4poj"><span class="c9">L.-C. Chen, J. T. Barron, G. Papandreou, K. Murphy, and A. L. Yuille. Semantic image segmentation with task-specific edge detection using cnns and a discriminatively trained domain transform. In </span><span class="c49 c42 c27">CVPR</span><span class="c9">, 2016.</span></li><li class="c46 li-bullet-20" id="h.1pxezwc"><span class="c9">L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Semantic image segmentation with deep convolutional nets and fully connected crfs. In </span><span class="c49 c42 c27">ICLR</span><span class="c9">, 2015.</span></li><li class="c354 li-bullet-20" id="h.49x2ik5"><span class="c9">L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. </span><span class="c49 c42 c27">arXiv:1606.00915</span><span class="c9">, 2016.</span></li><li class="c363 li-bullet-21" id="h.2p2csry"><span class="c9">L.-C. Chen, Y. Yang, J. Wang, W. Xu, and A. L. Yuille. At- tention to scale: Scale-aware semantic image segmentation. In </span><span class="c49 c42 c27">CVPR</span><span class="c9">, 2016.</span></li><li class="c273 li-bullet-22"><span class="c9">F. Chollet. Xception: Deep learning with depthwise separable convolutions. </span><span class="c49 c42 c27">arXiv:1610.02357</span><span class="c9">, 2016.</span></li></ol><p class="c336"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 622.18px; height: 798.33px;"><img alt="" src="images/image47.jpg" style="width: 622.18px; height: 798.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><table class="c190"><tr class="c16"><td class="c20" colspan="1" rowspan="1"><p class="c236"><span class="c3">Pooling (ASPP) module.</span></p></td><td class="c92" colspan="1" rowspan="1"><p class="c245"><span class="c9">32</span></p></td><td class="c299" colspan="1" rowspan="1"><p class="c532"><span class="c9">16</span></p></td><td class="c622" colspan="1" rowspan="1"><p class="c285"><span class="c9">75.90</span></p></td></tr><tr class="c430"><td class="c20" colspan="1" rowspan="1"><p class="c178"><span class="c0">Varying batch size: </span><span class="c3">Since it is important to train</span></p></td><td class="c453" colspan="1" rowspan="1"><p class="c180"><span class="c9">8</span></p></td><td class="c356" colspan="1" rowspan="1"><p class="c606"><span class="c9">8</span></p></td><td class="c129" colspan="1" rowspan="1"><p class="c634"><span class="c9">75.62</span></p></td></tr><tr class="c317"><td class="c20" colspan="1" rowspan="1"><p class="c275"><span class="c3">DeepLabv3 with fine-tuning the batch normalization, we</span></p></td><td class="c184" colspan="1" rowspan="1"><p class="c85"><span class="c9">16</span></p></td><td class="c380" colspan="1" rowspan="1"><p class="c246"><span class="c9">8</span></p></td><td class="c460" colspan="1" rowspan="1"><p class="c136"><span class="c9">78.51</span></p></td></tr><tr class="c274"><td class="c594" colspan="1" rowspan="1"><p class="c68"><span class="c3">further experiment with the effect of different batch sizes.</span></p></td><td class="c498" colspan="1" rowspan="1"><p class="c21"><span class="c9">32</span></p></td><td class="c185" colspan="1" rowspan="1"><p class="c332"><span class="c9">8</span></p></td><td class="c480" colspan="1" rowspan="1"><p class="c355"><span class="c9">75.75</span></p></td></tr><tr class="c158"><td class="c628" colspan="1" rowspan="1"><p class="c169"><span class="c3">num replicas</span></p></td><td class="c249" colspan="1" rowspan="1"><p class="c618"><span class="c3">mIOU&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;relative</span></p></td><td class="c367" colspan="1" rowspan="1"><p class="c439"><span class="c3">training time</span></p></td><td class="c575" colspan="1" rowspan="1"><p class="c25"><span class="c9"></span></p></td><td class="c616" colspan="1" rowspan="1"><p class="c152"><span class="c0">Method</span></p></td><td class="c104" colspan="1" rowspan="1"><p class="c551"><span class="c0">Coarse</span></p></td><td class="c350" colspan="1" rowspan="1"><p class="c98"><span class="c0">mIOU</span></p></td></tr><tr class="c334"><td class="c247" colspan="1" rowspan="1"><p class="c328"><span class="c3">1</span></p></td><td class="c138" colspan="1" rowspan="1"><p class="c168"><span class="c3">77.21</span></p></td><td class="c520" colspan="1" rowspan="1"><p class="c266"><span class="c3">1.00x</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c25"><span class="c9"></span></p></td><td class="c590" colspan="1" rowspan="1"><p class="c347"><span class="c3">DeepLabv2-CRF [</span><span class="c8"><a class="c5" href="#h.1pxezwc">11</a></span><span class="c3">]</span></p></td><td class="c47" colspan="1" rowspan="1"><p class="c25"><span class="c9"></span></p></td><td class="c574" colspan="1" rowspan="1"><p class="c263"><span class="c3">70.4</span></p></td></tr><tr class="c63"><td class="c170" colspan="1" rowspan="1"><p class="c179"><span class="c3">2</span></p></td><td class="c413" colspan="1" rowspan="1"><p class="c57"><span class="c3">77.15</span></p></td><td class="c112" colspan="1" rowspan="1"><p class="c242"><span class="c3">0.50x</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c25"><span class="c39"></span></p></td><td class="c160" colspan="1" rowspan="1"><p class="c83"><span class="c3">Deep Layer Cascade [</span><span class="c8"><a class="c5" href="#h.1jlao46">52</a></span><span class="c3">]</span></p></td><td class="c60" colspan="1" rowspan="1"><p class="c25"><span class="c39"></span></p></td><td class="c364" colspan="1" rowspan="1"><p class="c72"><span class="c3">71.1</span></p></td></tr><tr class="c63"><td class="c170" colspan="1" rowspan="1"><p class="c179"><span class="c3">4</span></p></td><td class="c413" colspan="1" rowspan="1"><p class="c57"><span class="c3">76.79</span></p></td><td class="c112" colspan="1" rowspan="1"><p class="c242"><span class="c3">0.25x</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c25"><span class="c39"></span></p></td><td class="c160" colspan="1" rowspan="1"><p class="c83"><span class="c3">ML-CRNN [</span><span class="c8"><a class="c5" href="#h.vx1227">21</a></span><span class="c3">]</span></p></td><td class="c60" colspan="1" rowspan="1"><p class="c25"><span class="c39"></span></p></td><td class="c364" colspan="1" rowspan="1"><p class="c72"><span class="c3">71.2</span></p></td></tr><tr class="c63"><td class="c170" colspan="1" rowspan="1"><p class="c179"><span class="c3">8</span></p></td><td class="c413" colspan="1" rowspan="1"><p class="c57"><span class="c3">77.02</span></p></td><td class="c112" colspan="1" rowspan="1"><p class="c242"><span class="c3">0.13x</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c25"><span class="c39"></span></p></td><td class="c160" colspan="1" rowspan="1"><p class="c83"><span class="c3">Adelaide context [</span><span class="c8"><a class="c5" href="#h.2iq8gzs">55</a></span><span class="c3">]</span></p></td><td class="c60" colspan="1" rowspan="1"><p class="c25"><span class="c39"></span></p></td><td class="c364" colspan="1" rowspan="1"><p class="c72"><span class="c3">71.6</span></p></td></tr><tr class="c233"><td class="c170" colspan="1" rowspan="1"><p class="c571"><span class="c3">16</span></p></td><td class="c413" colspan="1" rowspan="1"><p class="c605"><span class="c3">77.18</span></p></td><td class="c112" colspan="1" rowspan="1"><p class="c139"><span class="c3">0.06x</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c25"><span class="c39"></span></p></td><td class="c160" colspan="1" rowspan="1"><p class="c313"><span class="c3">FRRN [</span><span class="c8"><a class="c5" href="#h.3mzq4wv">70</a></span><span class="c3">]</span></p></td><td class="c60" colspan="1" rowspan="1"><p class="c25"><span class="c39"></span></p></td><td class="c364" colspan="1" rowspan="1"><p class="c306"><span class="c3">71.8</span></p></td></tr><tr class="c452"><td class="c405" colspan="1" rowspan="1"><p class="c59"><span class="c3">32</span></p></td><td class="c321" colspan="1" rowspan="1"><p class="c346"><span class="c3">76.69</span></p></td><td class="c299" colspan="1" rowspan="1"><p class="c491"><span class="c3">0.03x</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c25"><span class="c9"></span></p></td><td class="c160" colspan="1" rowspan="1"><p class="c406"><span class="c3">LRR-4x [</span><span class="c8"><a class="c5" href="#h.2u6wntf">25</a></span><span class="c3">]</span></p></td><td class="c60" colspan="1" rowspan="1"><p class="c466"><span class="c62">C</span></p></td><td class="c364" colspan="1" rowspan="1"><p class="c410"><span class="c3">71.8</span></p></td></tr><tr class="c609"><td class="c588" colspan="2" rowspan="1"><p class="c177 c281"><span class="c3">RefineNet [</span><span class="c8"><a class="c5" href="#h.2iq8gzs">54</a></span><span class="c3">]</span></p><p class="c177 c419"><span class="c3">FoveaNet [</span><span class="c8"><a class="c5" href="#h.34g0dwd">51</a></span><span class="c3">]</span></p><p class="c177 c381"><span class="c3">Ladder DenseNet [</span><span class="c8"><a class="c5" href="#h.2r0uhxc">46</a></span><span class="c3">]</span></p></td><td class="c555" colspan="1" rowspan="1"><p class="c281 c487"><span class="c3">73.6</span></p><p class="c216"><span class="c3">74.1</span></p><p class="c37"><span class="c3">74.3</span></p></td></tr><tr class="c63"><td class="c20" colspan="1" rowspan="1"><p class="c177 c479"><span class="c3">PEARL [</span><span class="c8"><a class="c5" href="#h.sqyw64">42</a></span><span class="c3">]</span></p></td><td class="c184" colspan="1" rowspan="1"><p class="c25"><span class="c39"></span></p></td><td class="c112" colspan="1" rowspan="1"><p class="c560"><span class="c3">75.4</span></p></td></tr><tr class="c63"><td class="c20" colspan="1" rowspan="1"><p class="c73"><span class="c3">Global-Local-Refinement [</span><span class="c8"><a class="c5" href="#h.4iylrwe">93</a></span><span class="c3">]</span></p></td><td class="c184" colspan="1" rowspan="1"><p class="c25"><span class="c39"></span></p></td><td class="c112" colspan="1" rowspan="1"><p class="c476"><span class="c3">77.3</span></p></td></tr><tr class="c233"><td class="c20" colspan="1" rowspan="1"><p class="c177 c327"><span class="c3">SAC multiple [</span><span class="c8"><a class="c5" href="#h.2y3w247">94</a></span><span class="c3">]</span></p></td><td class="c184" colspan="1" rowspan="1"><p class="c25"><span class="c39"></span></p></td><td class="c112" colspan="1" rowspan="1"><p class="c434"><span class="c3">78.1</span></p></td></tr><tr class="c58"><td class="c20" colspan="1" rowspan="1"><p class="c127 c177"><span class="c3">SegModel [</span><span class="c8"><a class="c5" href="#h.40ew0vw">75</a></span><span class="c3">]</span></p></td><td class="c184" colspan="1" rowspan="1"><p class="c254"><span class="c62">C</span></p></td><td class="c112" colspan="1" rowspan="1"><p class="c225"><span class="c3">79.2</span></p></td></tr><tr class="c63"><td class="c20" colspan="1" rowspan="1"><p class="c202 c177"><span class="c3">TuSimple Coarse [</span><span class="c8"><a class="c5" href="#h.279ka65">84</a></span><span class="c3">]</span></p></td><td class="c184" colspan="1" rowspan="1"><p class="c383"><span class="c62">C</span></p></td><td class="c112" colspan="1" rowspan="1"><p class="c181"><span class="c3">80.1</span></p></td></tr><tr class="c63"><td class="c20" colspan="1" rowspan="1"><p class="c177 c202"><span class="c3">Netwarp [</span><span class="c8"><a class="c5" href="#h.4f1mdlm">24</a></span><span class="c3">]</span></p></td><td class="c184" colspan="1" rowspan="1"><p class="c383"><span class="c62">C</span></p></td><td class="c112" colspan="1" rowspan="1"><p class="c181"><span class="c3">80.5</span></p></td></tr><tr class="c63"><td class="c20" colspan="1" rowspan="1"><p class="c202 c177"><span class="c3">ResNet-38 [</span><span class="c8"><a class="c5" href="#h.36ei31r">86</a></span><span class="c3">]</span></p></td><td class="c184" colspan="1" rowspan="1"><p class="c383"><span class="c62">C</span></p></td><td class="c112" colspan="1" rowspan="1"><p class="c181"><span class="c3">80.6</span></p></td></tr><tr class="c53"><td class="c518" colspan="1" rowspan="1"><p class="c202 c177"><span class="c3">PSPNet [</span><span class="c8"><a class="c5" href="#h.1d96cc0">95</a></span><span class="c3">]</span></p></td><td class="c92" colspan="1" rowspan="1"><p class="c383"><span class="c62">C</span></p></td><td class="c299" colspan="1" rowspan="1"><p class="c181"><span class="c3">81.2</span></p></td></tr><tr class="c420"><td class="c621" colspan="2" rowspan="1"><p class="c177 c542"><span class="c3">DeepLabv3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c62">C</span></p></td><td class="c407" colspan="1" rowspan="1"><p class="c431"><span class="c3">81.3</span></p></td></tr></table><p class="c336 c489"><span class="c3"></span></p><p class="c44" id="h.147n2zr"><span class="c27">Figure 8. Visualization results on Cityscapes </span><span class="c42 c27">val </span><span class="c27">set when training with only </span><span class="c42 c27">train fine </span><span class="c9">set.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1.33px; height: 1.33px;"><img alt="" src="images/image1.png" style="width: 1.33px; height: 1.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ol class="c52 lst-kix_list_1-0" start="14"><li class="c203 li-bullet-21" id="h.3o7alnk"><span class="c9">M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,</span></li></ol><p class="c369" id="h.23ckvvd"><span class="c27">R. Benenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset for semantic urban scene understanding. In </span><span class="c42 c27">CVPR</span><span class="c9">, 2016.</span></p><ol class="c52 lst-kix_list_1-0" start="15"><li class="c441 li-bullet-23" id="h.ihv636"><span class="c9">J. Dai, K. He, and J. Sun. Convolutional feature masking for joint object and stuff segmentation. </span><span class="c49 c42 c27">arXiv:1412.1283</span><span class="c9">, 2014.</span></li><li class="c7 li-bullet-24" id="h.32hioqz"><span class="c9">J. Dai, K. He, and J. Sun. Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmenta- tion. In </span><span class="c49 c42 c27">ICCV</span><span class="c9">, 2015.</span></li><li class="c384 li-bullet-21" id="h.1hmsyys"><span class="c9">J. Dai, Y. Li, K. He, and J. Sun. R-fcn: Object detection via region-based fully convolutional networks. </span><span class="c49 c42 c27">arXiv:1605.06409</span><span class="c9">, 2016.</span></li><li class="c7 li-bullet-25" id="h.41mghml"><span class="c9">J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei. Deformable convolutional networks. </span><span class="c49 c42 c27">arXiv:1703.06211</span><span class="c9">, 2017.</span></li><li class="c77 li-bullet-26" id="h.2grqrue"><span class="c9">D. Eigen and R. Fergus. Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. </span><span class="c49 c42 c27">arXiv:1411.4734</span><span class="c9">, 2014.</span></li><li class="c388 li-bullet-27" id="h.vx1227"><span class="c9">M. Everingham, S. M. A. Eslami, L. V. Gool, C. K. I. Williams, J. Winn, and A. Zisserma. The pascal visual object classes challenge a retrospective. </span><span class="c49 c42 c27">IJCV</span><span class="c9">, 2014.</span></li><li class="c7 li-bullet-28" id="h.3fwokq0"><span class="c9">H. Fan, X. Mei, D. Prokhorov, and H. Ling. Multi-level contextual rnns with attention model for scene labeling. </span><span class="c49 c42 c27">arXiv:1607.02537</span><span class="c9">, 2016.</span></li><li class="c77 li-bullet-29" id="h.1v1yuxt"><span class="c9">C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning hierarchical features for scene labeling. </span><span class="c49 c42 c27">PAMI</span><span class="c9">, 2013.</span></li><li class="c7 li-bullet-22" id="h.4f1mdlm"><span class="c9">J. Fu, J. Liu, Y. Wang, and H. Lu. Stacked deconvolutional network for semantic segmentation. </span><span class="c49 c42 c27">arXiv:1708.04943</span><span class="c9">, 2017.</span></li><li class="c578 li-bullet-21" id="h.2u6wntf"><span class="c9">R. Gadde, V. Jampani, and P. V. Gehler. Semantic video cnns through representation warping. In </span><span class="c49 c42 c27">ICCV</span><span class="c9">, 2017.</span></li><li class="c171 li-bullet-23" id="h.19c6y18"><span class="c9">G. Ghiasi and C. C. Fowlkes. Laplacian reconstruction and refinement for semantic segmentation. </span><span class="c49 c42 c27">arXiv:1605.02264</span><span class="c9">, 2016.</span></li><li class="c84 li-bullet-30"><span class="c9">A. Giusti, D. Ciresan, J. Masci, L. Gambardella, and</span></li></ol><p class="c521" id="h.3tbugp1"><span class="c27">J. Schmidhuber. Fast image scanning with deep max-pooling convolutional neural networks. In </span><span class="c42 c27">ICIP</span><span class="c9">, 2013.</span></p><ol class="c52 lst-kix_list_1-0" start="27"><li class="c7 li-bullet-31" id="h.28h4qwu"><span class="c9">S. Gould, R. Fulton, and D. Koller. Decomposing a scene into geometric and semantically consistent regions. In </span><span class="c49 c42 c27">ICCV</span><span class="c9">. IEEE, 2009.</span></li><li class="c384 li-bullet-24" id="h.nmf14n"><span class="c9">K. Grauman and T. Darrell. The pyramid match kernel: Dis- criminative classification with sets of image features. In </span><span class="c49 c42 c27">ICCV</span><span class="c9">, 2005.</span></li><li class="c7 li-bullet-32" id="h.37m2jsg"><span class="c9">B. Hariharan, P. Arbela&acute;ez, L. Bourdev, S. Maji, and J. Malik. Semantic contours from inverse detectors. In </span><span class="c49 c42 c27">ICCV</span><span class="c9">, 2011.</span></li><li class="c7 li-bullet-21" id="h.1mrcu09"><span class="c9">B. Hariharan, P. Arbela&acute;ez, R. Girshick, and J. Malik. Hyper- columns for object segmentation and fine-grained localization. In </span><span class="c49 c42 c27">CVPR</span><span class="c9">, 2015.</span></li><li class="c613 li-bullet-33" id="h.46r0co2"><span class="c9">K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In </span><span class="c49 c42 c27">ECCV</span><span class="c9">, 2014.</span></li><li class="c396 li-bullet-34" id="h.2lwamvv"><span class="c9">K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. </span><span class="c49 c42 c27">arXiv:1512.03385</span><span class="c9">, 2015.</span></li><li class="c351 li-bullet-31" id="h.111kx3o"><span class="c9">X. He, R. S. Zemel, and M. Carreira-Perpindn. Multiscale conditional random fields for image labeling. In </span><span class="c49 c42 c27">CVPR</span><span class="c9">, 2004.</span></li><li class="c396 li-bullet-31"><span class="c9">G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. In </span><span class="c49 c42 c27">NIPS</span><span class="c9">, 2014.</span></li></ol><hr style="page-break-before:always;display:none;"><ol class="c52 lst-kix_list_1-0" start="35"><li class="c203 li-bullet-31" id="h.3l18frh"><span class="c9">S. Hochreiter and J. Schmidhuber. Long short-term memory.</span></li></ol><p class="c470" id="h.206ipza"><span class="c42 c27">Neural computation</span><span class="c9">, 9(8):1735&ndash;1780, 1997.</span></p><ol class="c52 lst-kix_list_1-0" start="36"><li class="c582 li-bullet-25"><span class="c9">M. Holschneider, R. Kronland-Martinet, J. Morlet, and</span></li></ol><p class="c358" id="h.4k668n3"><span class="c27">P. Tchamitchian. A real-time algorithm for signal analysis with the help of the wavelet transform. In </span><span class="c42 c27">Wavelets: Time- Frequency Methods and Phase Space</span><span class="c9">, pages 289&ndash;297. 1989.</span></p><ol class="c52 lst-kix_list_1-0" start="37"><li class="c212 li-bullet-24"><span class="c9">J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi,</span></li></ol><ol class="c52 lst-kix_list_1-1 start" start="1"><li class="c335" id="h.2zbgiuw"><span class="c9">Fischer, Z. Wojna, Y. Song, S. Guadarrama, and K. Murphy. Speed/accuracy trade-offs for modern convolutional object detectors. In </span><span class="c49 c42 c27">CVPR</span><span class="c9">, 2017.</span></li></ol><ol class="c52 lst-kix_list_1-0" start="38"><li class="c28 li-bullet-35" id="h.1egqt2p"><span class="c9">S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. </span><span class="c49 c42 c27">arXiv:1502.03167</span><span class="c9">, 2015.</span></li><li class="c370 li-bullet-36" id="h.3ygebqi"><span class="c9">M. A. Islam, M. Rochan, N. D. Bruce, and Y. Wang. Gated feedback refinement network for dense image labeling. In </span><span class="c49 c42 c27">CVPR</span><span class="c9">, 2017.</span></li><li class="c28 li-bullet-34" id="h.2dlolyb"><span class="c9">S. D. Jain, B. Xiong, and K. Grauman. Fusionseg: Learn- ing to combine motion and appearance for fully automatic segmention of generic objects in videos. In </span><span class="c49 c42 c27">CVPR</span><span class="c9">, 2017.</span></li><li class="c379 li-bullet-37" id="h.sqyw64"><span class="c9">V. Jampani, M. Kiefel, and P. V. Gehler. Learning sparse high dimensional filters: Image filtering, dense crfs and bilateral neural networks. In </span><span class="c49 c42 c27">CVPR</span><span class="c9">, 2016.</span></li><li class="c212 li-bullet-25"><span class="c9">X. Jin, X. Li, H. Xiao, X. Shen, Z. Lin, J. Yang, Y. Chen,</span></li></ol><p class="c81" id="h.3cqmetx"><span class="c27">J. Dong, L. Liu, Z. Jie, J. Feng, and S. Yan. Video scene parsing with predictive feature learning. In </span><span class="c42 c27">ICCV</span><span class="c9">, 2017.</span></p><ol class="c52 lst-kix_list_1-0" start="43"><li class="c564 li-bullet-25" id="h.1rvwp1q"><span class="c9">P. Kohli, P. H. Torr, et al. Robust higher order potentials for enforcing label consistency. </span><span class="c49 c42 c27">IJCV</span><span class="c9">, 82(3):302&ndash;324, 2009.</span></li><li class="c150 li-bullet-35" id="h.4bvk7pj"><span class="c9">S. Kong and C. Fowlkes. Recurrent scene parsing with per- spective understanding in the loop. </span><span class="c49 c42 c27">arXiv:1705.07238</span><span class="c9">, 2017.</span></li><li class="c150 li-bullet-26" id="h.2r0uhxc"><span class="c9">P. Kra&uml;henbu&uml; hl and V. Koltun. &nbsp; Efficient inference in fully connected crfs with gaussian edge potentials. In </span><span class="c49 c42 c27">NIPS</span><span class="c9">, 2011.</span></li><li class="c343 li-bullet-38" id="h.1664s55"><span class="c9">I. Kres&#711;o, S. S&#711;egvic&acute;, and J. Krapac. &nbsp;Ladder-style densenets for semantic segmentation of large natural images. In </span><span class="c49 c42 c27">ICCV CVRSUAD workshop</span><span class="c9">, 2017.</span></li><li class="c595 li-bullet-39" id="h.3q5sasy"><span class="c9">A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In </span><span class="c49 c42 c27">NIPS</span><span class="c9">, 2012.</span></li><li class="c637 li-bullet-40" id="h.25b2l0r"><span class="c9">L. Ladicky, C. Russell, P. Kohli, and P. H. Torr. Associative hierarchical crfs for object class image segmentation. In </span><span class="c49 c42 c27">ICCV</span><span class="c9">, 2009.</span></li><li class="c28 li-bullet-34" id="h.kgcv8k"><span class="c9">S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of fea- tures: Spatial pyramid matching for recognizing natural scene categories. In </span><span class="c49 c42 c27">CVPR</span><span class="c9">, 2006.</span></li><li class="c28 li-bullet-35" id="h.34g0dwd"><span class="c9">Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. </span><span class="c49 c42 c27">Neural computa- tion</span><span class="c9">, 1(4):541&ndash;551, 1989.</span></li><li class="c212 li-bullet-24"><span class="c9">X. Li, Z. Jie, W. Wang, C. Liu, J. Yang, X. Shen, Z. Lin,</span></li></ol><p class="c330" id="h.1jlao46"><span class="c27">Q. Chen, S. Yan, and J. Feng. Foveanet: Perspective-aware urban scene parsing. </span><span class="c42 c27">arXiv:1708.02421</span><span class="c9">, 2017.</span></p><ol class="c52 lst-kix_list_1-0" start="52"><li class="c379 li-bullet-31" id="h.43ky6rz"><span class="c9">X. Li, Z. Liu, P. Luo, C. C. Loy, and X. Tang. Not all pixels are equal: Difficulty-aware semantic segmentation via deep layer cascade. </span><span class="c49 c42 c27">arXiv:1704.01344</span><span class="c9">, 2017.</span></li><li class="c28 li-bullet-29"><span class="c9">X. Liang, X. Shen, D. Xiang, J. Feng, L. Lin, and S. Yan. Semantic object parsing with local-global long short-term memory. </span><span class="c49 c42 c27">arXiv:1511.04510</span><span class="c9">, 2015.</span></li><li class="c106 li-bullet-31" id="h.2iq8gzs"><span class="c9">G. Lin, A. Milan, C. Shen, and I. Reid. Refinenet: Multi- path refinement networks with identity mappings for high- resolution semantic segmentation. </span><span class="c49 c42 c27">arXiv:1611.06612</span><span class="c9">, 2016.</span></li><li class="c108 li-bullet-34" id="h.xvir7l"><span class="c9">G. Lin, C. Shen, I. Reid, et al. Efficient piecewise train- ing of deep structured models for semantic segmentation. </span><span class="c49 c42 c27">arXiv:1504.01013</span><span class="c9">, 2015.</span></li><li class="c483 li-bullet-41"><span class="c9">T.-Y. Lin, P. Dolla&acute;r, R. Girshick, K. He, B. Hariharan, and</span></li></ol><p class="c400"><span class="c9">S. Belongie. Feature pyramid networks for object detection.</span></p><p class="c147" id="h.3hv69ve"><span class="c42 c27">arXiv:1612.03144</span><span class="c9">, 2016.</span></p><ol class="c52 lst-kix_list_1-0" start="57"><li class="c427 li-bullet-42" id="h.1x0gk37"><span class="c9">T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra- manan, P. Dolla&acute;r, and C. L. Zitnick. Microsoft COCO: Com- mon objects in context. In </span><span class="c49 c42 c27">ECCV</span><span class="c9">, 2014.</span></li><li class="c467 li-bullet-33" id="h.4h042r0"><span class="c9">W. Liu, A. Rabinovich, and A. C. Berg. Parsenet: Looking wider to see better. </span><span class="c49 c42 c27">arXiv:1506.04579</span><span class="c9">, 2015.</span></li><li class="c122 li-bullet-31" id="h.2w5ecyt"><span class="c9">Z. Liu, X. Li, P. Luo, C. C. Loy, and X. Tang. Semantic image segmentation via deep parsing network. In </span><span class="c49 c42 c27">ICCV</span><span class="c9">, 2015.</span></li><li class="c122 li-bullet-31" id="h.1baon6m"><span class="c9">J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In </span><span class="c49 c42 c27">CVPR</span><span class="c9">, 2015.</span></li><li class="c122 li-bullet-23" id="h.3vac5uf"><span class="c9">P. Luo, G. Wang, L. Lin, and X. Wang. Deep dual learning for semantic image segmentation. In </span><span class="c49 c42 c27">ICCV</span><span class="c9">, 2017.</span></li><li class="c639 li-bullet-43" id="h.2afmg28"><span class="c9">M. Mostajabi, P. Yadollahpour, and G. Shakhnarovich. Feed- forward semantic segmentation with zoom-out features. In </span><span class="c49 c42 c27">CVPR</span><span class="c9">, 2015.</span></li><li class="c457 li-bullet-44"><span class="c9">R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler,</span></li></ol><p class="c502" id="h.pkwqa1"><span class="c27">R. Urtasun, and A. Yuille. The role of context for object detection and semantic segmentation in the wild. In </span><span class="c42 c27">CVPR</span><span class="c9">, 2014.</span></p><ol class="c52 lst-kix_list_1-0" start="64"><li class="c592 li-bullet-33" id="h.39kk8xu"><span class="c9">H. Noh, S. Hong, and B. Han. Learning deconvolution net- work for semantic segmentation. In </span><span class="c49 c42 c27">ICCV</span><span class="c9">, 2015.</span></li><li class="c638 li-bullet-45" id="h.1opuj5n"><span class="c9">G. Papandreou, L.-C. Chen, K. Murphy, and A. L. Yuille. Weakly- and semi-supervised learning of a dcnn for semantic image segmentation. In </span><span class="c49 c42 c27">ICCV</span><span class="c9">, 2015.</span></li><li class="c544 li-bullet-35" id="h.48pi1tg"><span class="c9">G. Papandreou, I. Kokkinos, and P.-A. Savalle. Modeling local and global deformations in deep learning: Epitomic convolution, multiple instance learning, and sliding window detection. In </span><span class="c49 c42 c27">CVPR</span><span class="c9">, 2015.</span></li><li class="c326 li-bullet-29" id="h.2nusc19"><span class="c9">G. Papandreou and P. Maragos. Multigrid geometric active contour models. </span><span class="c49 c42 c27">TIP</span><span class="c9">, 16(1):229&ndash;240, 2007.</span></li><li class="c108 li-bullet-24" id="h.1302m92"><span class="c9">C. Peng, X. Zhang, G. Yu, G. Luo, and J. Sun. Large kernel matters&ndash;improve semantic segmentation by global convolu- tional network. </span><span class="c49 c42 c27">arXiv:1703.02719</span><span class="c9">, 2017.</span></li><li class="c122 li-bullet-30" id="h.3mzq4wv"><span class="c9">P. Pinheiro and R. Collobert. Recurrent convolutional neural networks for scene labeling. In </span><span class="c49 c42 c27">ICML</span><span class="c9">, 2014.</span></li><li class="c316 li-bullet-37" id="h.2250f4o"><span class="c9">T. Pohlen, A. Hermans, M. Mathias, and B. Leibe. Full- resolution residual networks for semantic segmentation in street scenes. </span><span class="c49 c42 c27">arXiv:1611.08323</span><span class="c9">, 2016.</span></li><li class="c627 li-bullet-34" id="h.haapch"><span class="c9">O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In </span><span class="c49 c42 c27">MICCAI</span><span class="c9">, 2015.</span></li><li class="c457 li-bullet-23"><span class="c9">O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,</span></li></ol><p class="c539" id="h.319y80a"><span class="c27">Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. </span><span class="c42 c27">IJCV</span><span class="c9">, 2015.</span></p><ol class="c52 lst-kix_list_1-0" start="73"><li class="c108 li-bullet-31" id="h.1gf8i83"><span class="c9">A. G. Schwing and R. Urtasun. Fully connected deep struc- tured networks. </span><span class="c49 c42 c27">arXiv:1503.02351</span><span class="c9">, 2015.</span></li><li class="c483 li-bullet-46"><span class="c9">P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and</span></li></ol><p class="c525"><span class="c9">Y. LeCun. Overfeat: Integrated recognition, localization and</span></p><hr style="page-break-before:always;display:none;"><p class="c71" id="h.40ew0vw"><span class="c27">detection using convolutional networks. </span><span class="c42 c27">arXiv:1312.6229</span><span class="c9">, 2013.</span></p><ol class="c52 lst-kix_list_1-0" start="75"><li class="c75 li-bullet-47" id="h.2fk6b3p"><span class="c9">F. Shen, R. Gan, S. Yan, and G. Zeng. Semantic segmentation via structured patch prediction, context crf and guidance crf. In </span><span class="c49 c42 c27">CVPR</span><span class="c9">, 2017.</span></li><li class="c461 li-bullet-48" id="h.upglbi"><span class="c9">J. Shotton, J. Winn, C. Rother, and A. Criminisi. Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context. </span><span class="c49 c42 c27">IJCV</span><span class="c9">, 2009.</span></li><li class="c75 li-bullet-49" id="h.3ep43zb"><span class="c9">A. Shrivastava, R. Sukthankar, J. Malik, and A. Gupta. Be- yond skip connections: Top-down modulation for object de- tection. </span><span class="c49 c42 c27">arXiv:1612.06851</span><span class="c9">, 2016.</span></li><li class="c187 li-bullet-21" id="h.1tuee74"><span class="c9">K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In </span><span class="c49 c42 c27">ICLR</span><span class="c9">, 2015.</span></li><li class="c596 li-bullet-50" id="h.4du1wux"><span class="c9">C. Sun, A. Shrivastava, S. Singh, and A. Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In </span><span class="c49 c42 c27">ICCV</span><span class="c9">, 2017.</span></li><li class="c614 li-bullet-35" id="h.2szc72q"><span class="c9">H. Sun, D. Xie, and S. Pu. Mixed context networks for semantic segmentation. </span><span class="c49 c42 c27">arXiv:1610.05854</span><span class="c9">, 2016.</span></li><li class="c187 li-bullet-51" id="h.184mhaj"><span class="c9">D. Terzopoulos. Image analysis using multigrid relaxation methods. </span><span class="c49 c42 c27">TPAMI</span><span class="c9">, (2):129&ndash;139, 1986.</span></li><li class="c110 li-bullet-31" id="h.3s49zyc"><span class="c9">R. Vemulapalli, O. Tuzel, M.-Y. Liu, and R. Chellappa. Gaus- sian conditional random field network for semantic segmenta- tion. In </span><span class="c49 c42 c27">CVPR</span><span class="c9">, 2016.</span></li><li class="c465 li-bullet-52" id="h.279ka65"><span class="c9">G. Wang, P. Luo, L. Lin, and X. Wang. Learning object inter- actions and descriptions for semantic image segmentation. In </span><span class="c49 c42 c27">CVPR</span><span class="c9">, 2017.</span></li><li class="c409 li-bullet-29"><span class="c9">P. Wang, P. Chen, Y. Yuan, D. Liu, Z. Huang, X. Hou, and</span></li></ol><p class="c636" id="h.meukdy"><span class="c27">G. Cottrell. Understanding convolution for semantic segmen- tation. </span><span class="c42 c27">arXiv:1702.08502</span><span class="c9">, 2017.</span></p><ol class="c52 lst-kix_list_1-0" start="85"><li class="c110 li-bullet-31" id="h.36ei31r"><span class="c9">Z. Wu, C. Shen, and A. van den Hengel. Bridging category-level and instance-level semantic image segmen- tation. </span><span class="c49 c42 c27">arXiv:1605.06885</span><span class="c9">, 2016.</span></li><li class="c110 li-bullet-34" id="h.1ljsd9k"><span class="c9">Z. Wu, C. Shen, and A. van den Hengel. Wider or deeper: Revisiting the resnet model for visual recognition. </span><span class="c49 c42 c27">arXiv:1611.10080</span><span class="c9">, 2016.</span></li><li class="c110 li-bullet-21" id="h.45jfvxd"><span class="c9">F. Xia, P. Wang, L.-C. Chen, and A. L. Yuille. Zoom better to see clearer: Huamn part segmentation with auto zoom net. </span><span class="c49 c42 c27">arXiv:1511.06881</span><span class="c9">, 2015.</span></li><li class="c471 li-bullet-31" id="h.2koq656"><span class="c9">Z. Yan, H. Zhang, Y. Jia, T. Breuel, and Y. Yu. Combining the best of convolutional layers and recurrent layers: A hybrid network for semantic segmentation. </span><span class="c49 c42 c27">arXiv:1603.04871</span><span class="c9">, 2016.</span></li><li class="c568 li-bullet-53" id="h.zu0gcz"><span class="c9">J. Yao, S. Fidler, and R. Urtasun. Describing the scene as a whole: Joint object detection, scene classification and seman- tic segmentation. In </span><span class="c49 c42 c27">CVPR</span><span class="c9">, 2012.</span></li><li class="c234 li-bullet-31" id="h.3jtnz0s"><span class="c9">F. Yu and V. Koltun. Multi-scale context aggregation by dilated convolutions. In </span><span class="c49 c42 c27">ICLR</span><span class="c9">, 2016.</span></li><li class="c457 li-bullet-25"><span class="c9">S. Zagoruyko and N. Komodakis. Wide residual networks.</span></li></ol><p class="c188" id="h.1yyy98l"><span class="c42 c27">arXiv:1605.07146</span><span class="c9">, 2016.</span></p><ol class="c52 lst-kix_list_1-0" start="92"><li class="c377 li-bullet-54" id="h.4iylrwe"><span class="c9">M. D. Zeiler, G. W. Taylor, and R. Fergus. Adaptive deconvo- lutional networks for mid and high level feature learning. In </span><span class="c49 c42 c27">ICCV</span><span class="c9">, 2011.</span></li><li class="c187 li-bullet-24" id="h.2y3w247"><span class="c9">R. Zhang, S. Tang, M. Lin, J. Li, and S. Yan. Global-residual and local-boundary refinement networks for rectifying scene parsing predictions. </span><span class="c49 c42 c27">IJCAI</span><span class="c9">, 2017.</span></li><li class="c187 li-bullet-32"><span class="c9">R. Zhang, S. Tang, Y. Zhang, J. Li, and S. Yan. Scale-adaptive convolutions for scene parsing. In </span><span class="c49 c42 c27">ICCV</span><span class="c9">, 2017.</span></li><li class="c371 li-bullet-21" id="h.1d96cc0"><span class="c9">H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene parsing network. </span><span class="c42 c27 c49">arXiv:1612.01105</span><span class="c9">, 2016.</span></li><li class="c282 li-bullet-21"><span class="c9">S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,</span></li></ol><p class="c217" id="h.3x8tuzt"><span class="c27">Z. Su, D. Du, C. Huang, and P. Torr. Conditional random fields as recurrent neural networks. In </span><span class="c27 c42">ICCV</span><span class="c9">, 2015.</span></p><ol class="c52 lst-kix_list_1-0" start="97"><li class="c486 li-bullet-30"><span class="c9">B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Tor- ralba. Scene parsing through ade20k dataset. In </span><span class="c49 c42 c27">CVPR</span><span class="c9">, 2017.</span></li></ol></body></html>